[{"title":"你写的代码是否无出安放-上","date":"2021-09-23T09:37:00.000Z","path":"2021/09/23/你写的代码是否无处安放-上/","text":"其实一直想聊一下这个话题，主要想表达什么呢？我见过公司里一些开发新人，他们总是抱着一个MVC三层架构死不放，只会将代码粗颗粒度的摆放在这三层中的某一层。因为大概率下是不会放错的，毕竟MVC的年龄可能比他们本身都大，总不会被否定吧？！ 但其实在实际开发过程中，还是存在很多“到底放那一层更合适”的抉择的，如果能停下来多思考一下，对开发人员来说一定是意义的。而且就我个人的经验而言，多思考这个问题也是非常有价值的~ 我知道，谈这种偏理论学术的话题，需要自身有深厚的编码功底，至少阅码百万？或编码10年？。。呃，要不还是算了我一介莽夫，怕会是谈错。不过，怕错怎么能进步呢？不是俗话说拉出来溜溜么？（主要是我的博客阅读量很少，哪怕说错应该不会有啥太大的影响） 那么从哪里开始呢？之前听过一个ThoughtWorks的分享，分享者抛出一个观点，乍听起来觉得很震惊，但细品起来非常的有道理，那我们就先从“前后端分离”这个点开始吧。 前后端分离，写web项目的开发者一定非常的熟悉，毕竟现在不管是面向B端系统，还是面向C端系统，这种分离应该都已经是事实标准了，总不可能错吧？但八叉（上面提到的分享者）却偏说：在很长一段时间里，他对前后端分离这种做法持否定态度更多一些！！ 纳尼？被雷到了？别急，且听一下他的解释，大概如下：如果不考虑业务领域边界，仅仅从技术层面完成前后端分离，那这种分离一定会导致的是开发团队内部沟通成本增加，以及无法很好的完成端到端交付等相关问题。 我在听完这个解释后，细想一下还真的是没错。要想尽可能的表达我对这句话的认同感，需要随我一起回到过去~~ Long Long ago. Web世界连css都是新鲜事物，那时候所有的数据+样式（html+css）+交互（几乎无交互可言）都是一股脑的在后端服务器上计算好，然后再丢给浏览器渲染的。 随着web前端技术的不断推陈出新，当然也得益于Firefox和Chrome浏览器的不断发展，富客户端的概念慢慢成为了一个诱惑的选项，大家走到了一个技术路口！ 前后端分离出现了，前几年（可能前十几年？）推广这个概念的时候，还炒过一段时间的“全栈程序员”的概念。它们可能没有什么直接关系，但就我的经历而言，还是想放在一起讲。只因为当时我已经在负责一个项目了，需求确定后，首先要确定的就是技术选型。而那时候团队里没有人会angular，ember或react这样的前端js框架，甚至都未曾听过这些陌生的名字。但如果不借助它们的能力，靠jQuery一个js库就去做全站的前后端分离，大家一定会觉得你疯了。。 所以当时我就成了团队里第一个吃螃蟹的人，最终将项目的前端部分用angular重构了一遍（当然是和组员一起哦）。之后的其他项目我又尝试过react，vue等几个前端框架，那段时间的自己，是需要解决项目里遇到的任何端（这里特指前端和后端）的问题的，也是那个时候我就开始思考：团队中需要一个角色，他必须是前后端都懂的，不需要精通，但要在理论和思维层面上则必须是有经验的。现在回想起来，假如当时的我把前端技术调研的工作交给同事去做，那我之后在做很多技术决策的时候一定是片面的，很可能会直接导致项目失败（虽然好像之前的项目最终也都终止了~~娃哈哈）。 只是那个时候我只是认知到了表象，即：需要“全栈思维”才能驾驭前后端分离。但这也只是前后端分离在技术层面对团队提出的一个小小的要求。当然随着框架的成熟和普及，这个要求慢慢的变的不再那么重要，毕竟经历过无数项目洗礼后，这些框架和使用它们的开发者也已经做了很多理论和思想的验证。哪怕你不思考，很多以前需要做的决策，也慢慢都成了业界的 SOP，被做成了框架的使用规范和最佳实践，根本就不会给你犯错的机会~~ 那是不是说前后端分离就零成本了？答案就要回到八叉的观点了。确实，回想我参与过的前后端分离项目，的确存在着一个隐形的成本，尤其是在一些处理一些复杂业务的时候，没错，那就是沟通成本。你可能觉得沟通问题没什么难解决的，开会就完事儿了~~呃，确实，如果你很容易把所有相关人员都喊到会议室（需要参会的人数越多，发生个别同事请假，或不在状态的概率就会增加），并且他们的默契程度非常高，并且都有出色的表达能力，也有必要的同理心和责任感。那么恭喜你，你所在的团队真的是非常的出色，但我没有你那么走运，我所见过的团队，几乎没有符合这种条件的。。。 之前看过不少相关的文章，沟通，技术人员永远的痛。。尤其是当团队本身就存在前端工程师和后端工程师的独立岗位时，有时候调和他们之间的冲突，和调和不同团队，甚至不同公司之间的冲突没有什么两样，因为它们的共同点就是：立场不同！ 科技公司的组织架构，也是经历过好几次迭代了。每个时代背景下出现的各种合适的组织架构都是服务于特定业务的，从来都不是由技术本身的要求决定的。前后端分离也不能例外，我们不能应为它的诸多优点（我非常认可它的所有优点，只是它对我们来说太熟悉了，不想重复描述了），就觉得它是银弹，对吧？那我们要怎么做才能最大化前后端分离的收益呢？ 八叉也提到了这点，只要能做到敏捷开发中提到的端到端交付能力，就意味着团队在使用前后端分离的同时，也考虑到了业务领域的上下文。这样做能最大程度的减少不合适的领域知识扩散（当然这只是其中一点好处），将参与开发的人员降低到合理的数量，在这个业务领域内的所有知识，他们都需要共享，这是无法避免的，也是非常有价值的。 听着怎么像在夸微服务和 DDD 呢？没错啦，其实这些架构诞生都是为了解决存在的问题的。新的思想不断提供更好的解决方案，推动着整个软件开发行业的发展。这个世界就是这样运作的，不管是哪个领域都是如此。对吗？不过其实它们三个是在不同的层面解决问题的，完全可以相互配合来一起发力。 白霍了这么多，我的目的只是想从前后端分离的角度，引出一个思考：我的这个逻辑到底要放在前端还是后端来实现好呢？ 我们排除那些完全不需要纠结的逻辑，例如“将数据存储到DB”这种的，再白痴也知道是要在后端处理的，对吧！！？？请在你的编码人生中，找一下，看是否存在那么一个业务点，它让你纠结过的，如果有，真心希望你能留言分享一下，最好还能把你做的决定以及理由也分享出来~~感激不尽！ 不管你以前是基于什么理由来做决策的，我都希望你以后在为某个逻辑思考这个问题时，能增加几个维度： 该决策是否会造成不必要的沟通成本？ 该决策是否会造成性能问题？ 该决策是否会泄露商业秘密？ 该是否存在安全隐患？ 需求是否经常会发生变更，变更后该决策会造成哪些部署成本？ 这里只是笼统列了一下思考点，希望对你有帮助，有机会我们再展开聊一下~~","tags":[{"name":"前后端分离","slug":"前后端分离","permalink":"https://blog.kazaff.me/tags/前后端分离/"}]},{"title":"面对无休止的需求讨论会议的反思1","date":"2021-09-18T09:37:00.000Z","path":"2021/09/18/面对无休止的需求讨论会议的反思1/","text":"不知道大家参与，甚至负责过的复杂业务需求分析都是怎么顺利完成交付的？有没有碰到最近发生在我身边的这种情况呢？翻来覆去，没完没了的需求分析会议，感觉真的就这样会一直无休无止的持续下去！ 你可能会有如下几个疑问： 业务真的那么复杂？没有任何业务专家能够讲清楚？ 模型真的那么创新？毫无竞品参考？ 内容真的那么庞大？上天入地，跨行跨业？ 干系人真的那么多？几百上千？沟通不完？ 好像都没有。而且不光是没有，而是完全不是这种量级的系统。就是一个带有一点特殊功能的B2C电商系统，仅此而已。而所谓的“一点特殊”，就真的是一点了，我以一个10年开发经验的老兵的视角这么分析的，而不是以一个天真烂漫的新人产品经理为了“糊弄”开发人员才这么形容的。。 OK，既然如此，那怎么就能把一个Web领域平均开发经验在5年左右，将近10个人的技术团队为难成这样？ 立项阶段该项目一开始投入的人力组成大概如下： 项目经理 1名（中层管理） 产品经理 2名（领导层） 负责公司现有电商系统的开发人员 1名 这个阶段的核心事务，是围绕着现有电商系统的已有功能，以及上线运营以来积累的一些新需求的。在过程中，参与人讨论重心在于：确定哪些功能应该保留，商定哪些功能应该升级，规划哪些功能应该新增。 这个阶段起初是比较顺利的，大家对已有系统的问题现状的认知，以及重构的目标保持一致，很容易就能达成意见一致。不过由于该阶段并没有有效产出物，大家这种所谓沟通良好的感觉都保存在各自的脑中。换句话说，可能每个人脑中的新需求的画面都各不相同，但又整体一致。 这其实在项目立项初期，也是可以接受的，我认为只要大的方向是毫无争议的，就足够能确立项目的价值，以及大致规模了。 需求分析阶段一开始投入到这个项目的人，并不是太多，大概包含： 产品经理 2名 项目经理 1名 高级工程师 2名 其实我在犹豫到底应不应该称那两位领导为产品经理角色？他们确实拥有要做哪些的决定权，但他们并不会事无巨细的画出详细的业务流程，甚至一张原型草图都不曾提供过。 只能由项目经理和那两位工程师根据会议口头讨论内容，结合常规电商系统的功能，以及现有系统的一些特殊功能，拼组出一份功能清单，对于一些复杂的用例也会提供相关的草图。 在这个阶段进行中，公司还提出了一些非功能性的需求，例如：要考虑产品化，要能通过简单的操作来剪裁个别功能，等一系列新的想法和挑战。 可以看出随着沟通的深入和广泛，需求点正在悄无声息的增加，但还没有到爆炸的程度，我觉得这种情况也属于正常现象。 不过这里我认为确实存在一个问题，参与的项目经理和工程师在日常的沟通讨论中，不自觉地在思考如何实现，甚至是设计数据库。现在看来是真的无语啊，天真的以为需求会简单的经过几轮讨论就固定下来。倒不是说不能提前思考，只是太早这么做，不光会浪费时间，也会影响士气。试想你为一个灵光乍现的一个需求点花了一整夜的时间进行技术方案的构想，结果第二天上班领导说先不用做了，你心情会如何？ 设计阶段设计阶段问题就开始不停的出现了，此时该项目的参与者已经多了起来： 产品经理 2名 项目经理 1名 需求分析人员 2名 设计文档编写人员 5名 一开始出现的问题，应该就是沟通问题。因为新进项目组的人员，之前对项目几乎可以算是完全不了解。而此时已经积累了大量的内容需要大家消化，再加上每个人的经验和能力都不一样，此时就出现了一些尴尬的情况，例如针对一个功能，重复反复的多次会议讨论。 你可能觉得这很正常，确实，但我无法表达出那种惊恐，当你发现针对某项需求点，几周下来不停的讨论，依然会不断的出现新的问题和逻辑缺失。。往往是正在讲解的过程中突然就爆出一个新的问题~~我就是这个时候开始觉得不对劲的。 我承认每个人的思维都是有死角的，我自己就是如此，即便是我自认为已经想的是否周全了，但还是会在和别人讨论的时候脑子里突然蹦出新的问题。。 但我无法接受的是，这种情况在一个需求点上周而复始的发生N次。。而且是在参与者都已经尽力的情况下发生的。。在我写这篇文章的时候我依然想知道到底是因为什么？ 开发阶段你应该能猜想的到，在设计阶段问题如此之多的前提下，在开发阶段会发生多么惨烈的事儿。。。 到底发生了什么如果非要逼我说一下可能的理由的话，我只能不自信的列出下面的问题点了： 项目管理经验不足 沟通技巧不足 会议技巧不足 文档编写能力不足 技术能力不足 卧槽，这几乎是一个团队可能发生的所有问题了吧。。。可是它们怎么能隐藏的如此完美，在之前的项目中就完全没有爆发呢？难道是因为其他项目的要求不高？或者期望不高？肯定都不是啊！！ 而且当这些问题都存在的时候，往往是恶性循环。沟通技巧不足往往意味着表达能力和理解能力有问题，这会严重影响一些稍微复杂点的需求的分析效率。再加上会议技巧，就已经足够让团队陷入毫无意义的会议中了。 而项目管理经验不足，会导致人员安排上达不到最佳配置，从而进一步导致任务排期出现错误。直接导致团队交付压力变大，进而让沟通变的越来越没有耐心，让文档变的越来越粗糙。由于没有完善的书面文档，项目增加人手变的非常困难，最终让项目不停的延期，却又无可奈何。 那要怎么破局呢？假如今天有个天降猛男，脚踏七彩祥云，他会如何指点江山呢？ To Be Continue…","tags":[{"name":"需求分析","slug":"需求分析","permalink":"https://blog.kazaff.me/tags/需求分析/"},{"name":"会议","slug":"会议","permalink":"https://blog.kazaff.me/tags/会议/"}]},{"title":"Vscode突然不智能了怎么办","date":"2021-09-13T09:37:00.000Z","path":"2021/09/13/vscode突然不智能了怎么办/","text":"自从被人推荐开始使用微软的VSCODE，基本上已经不再使用其它任何同类工具了。。。可见我对它的满意度有多高~~但今天它却给我的热情狠狠的来了一击，对，没错，它突然罢工了。。。 表现为，不识别任何语言，虽然代码着色还在，但是却不提供任何自动提示，以及快速跳转等高级功能。没错，它现在和彩色的记事本没差别~~网上也没有搜到我这种情况，怎么就突然病了呢？我今天甚至都没有做什么新的操作。。找了一圈也没有看到有价值的信息，只能使用终极必杀技了：卸载，重装！！！ 首先剧透一下，这招确实管用，但问题是你必须要卸载干净，否则重装也还是没鸟用（这足以证明是设置问题，而不是软件bug）。那卸载它，到底要怎么才算彻底呢？你必须要： 卸载vscode工具，可以通过任何一种常规方式卸载：如在控制面板里，或点击vscode安装文件夹里的卸载图标； 要手动删除 %APPDATA%\\Code； 要手动删除 %USERPROFILE%\\.vscode。 这样你应该能通过重新安装得到一个崭新的vscode了，新到完全没有任何插件，是的，都删了。。我不确定是否可以不执行上述第三步也能解决问题，但你知道的，我已经没有办法测试了。。 祝你好运~~","tags":[{"name":"vscode","slug":"vscode","permalink":"https://blog.kazaff.me/tags/vscode/"},{"name":"卸载","slug":"卸载","permalink":"https://blog.kazaff.me/tags/卸载/"}]},{"title":"这只猴子有点油","date":"2021-04-04T09:37:00.000Z","path":"2021/04/04/这只猴子有点油/","text":"久仰大名Tampermonkey（油猴），之前一直没有机会用在工作中，所以一直都只是知道有这么个强大的东西。前段时间在和我们的客户讨论需求时，根据对方的描述，我觉得必须要油猴出马了。有了动机，接下来就是学习，油猴的开发文档内容非常精炼，基本上30分钟就能通读完毕（上次让我有这种感叹的，应该是Vuejs）。而之所以内容篇幅很小，我猜应该是因为“大道至简”吧，工作越久就越发能体会到这个概念的伟大，概括了事物的本质：简约（有点扯远）。当你了解完油猴的自我价值定位后，你应该就能多少体会我的意思了：当我们解决问题时，视图抓住事物的本质（第一性原理）时会发现解决方案往往出其不意的简约（不是简单）。 那么请允许我说一下我是怎么理解油猴的：提供一种扩展目标网站的能力。可能是界面上的扩展，也可能是功能上的扩展，总之油猴只是提供最基本的接口，剩下的交给用户来发挥。正因为油猴没有做更多的假设，所以才会这么纯粹，不仅仅文档内容精炼，接口也好不多余。 基本的用法，网上前辈们已经写了不少，我在最后的参考资料里会列出我觉得不错的文章链接，方便有需要的童鞋查阅。那么接下来，我主要写一些我实战后的心得体会吧。 安全当用户使用你发布的油猴脚本时，最关注的应该就是它是否安全了，毕竟油猴太强大，能做太多事，不加思考的随意使用第三方提供的脚本，可能会给自己带来灭顶之灾（毫不为过）。而油猴自身也明白这一点，所以它要求脚本明示自己会使用哪些油猴提供的能力，此外也做了沙盒环境来阻断脚本直接去修改源网站系统的Js变量。但这对于粗心的用户 + 恶意开发者这种组合来说，对于安全提升似乎并没有什么太大的帮助。所以，衷心提醒广大童鞋，一定要当心噢！！！ 那我们转过来说一下，当油猴的一些安全设定和我们的目标冲突的时候，该怎么办呢？对我的业务场景来说，无法修改源网站系统中的JS变量，这对我造成了困扰。不过转念一想，既然油猴允许我们操作DOM，那我们只需要修改DOM追加一段脚本即可，在脚本里我们可以做任何想做的事儿（如: 覆盖window变量）。 额外的一点也需要提一下，如果你也需要获取浏览器端的cookie，尤其是设置为httponly类型的cookie，那传统的JS是绝对无法做到的，而油猴文档里其实有这方面的接口能力：GM_cookie，但由于这个对安全的影响实在是太大了，稳定版的油猴是不允许使用这个能力的，官方建议使用Beta版本。所以你懂了吧，如果必须要用，就需要你的用户安装正确版本的油猴哦~~ 共享数据如果我们需要跨域名的数据共享，那么使用油猴的GM_*Value相关接口即可，它的命名空间我个人的理解应该是基于脚本的，并不是访问网站的域名。这样我们就可以做到同一个脚本插件下不同的页面灵活的传递数据了。配合GM_addValueChangeListener监听目标变量的状态改变事件，可以实现更多炫酷的想法哦。 复杂交互假如只是不爽源网站的界面，那借助油猴你可以大刀阔斧的进行一番装修，直到满意为止~~或者仅仅是想提供一个额外的按钮，点击后做点什么羞羞的事儿，那也不是什么难事儿~~但如果你想要的是，“接管”用户的操作，让网站自动的进行一系列的操作（例如：在电商系统里，自动的完成将一系列的目标商品批量加入购物车），那你可就要停下来思考一下了。 没错，问题在于：浏览器刷新。你写的油猴脚本载入并执行，是在每次浏览器加载页面后的指定事件发生时触发的。假如前面提到的这一系列的操作，会触发浏览器重新载入页面，那你的油猴脚本其实也会一并重新加载（即执行上下文重置）。那每次刷新，脚本怎么知道要如何“继续”执行合适的操作呢？ 这个时候，我们就还是要借助GM_*Value相关接口了，思路是这样的：把这一系列的操作，分解成一段一段的，每一段的最后一步一定要确保触发页面加载。然后，我们只需要将拆分后得到的全部的可执行命令保存在GM_VALUE里，另一方面在脚本里监控目标变量是否有内容，有的话就优先开始按顺序取出指令进行执行即可（执行完记得删除哦）。 大概的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142 function createStep()&#123; //用来生成可执行命令集合。 let steps = [&#123;cmd:'clearCart'&#125;, &#123;cmd:'searchSKU', data: 'kazaff'&#125;, &#123;cmd:'keyinQTY', data: 7&#125;, &#123;cmd: \"pageRefresh\"&#125;, &#123;cmd:'over'&#125; ]; GM_setValue('autoRun', steps); &#125; // 脚本加载完毕后会自动触发 (function autoRun()&#123; let steps = GM_getValue('autoRun', null); if(steps == null || steps.length == 0)&#123; return; &#125; let currentStep = steps.shift(); GM_setValue('autoRun', steps); switch(currentStep.cmd)&#123; case \"clearCart\": clearCart();break; case \"searchSKU\": searchSKU(currentStep.data);break; case \"keyinQTY\": keyinQTY(currentStep.data);break; case \"pageRefresh\": window.location.reload();break; case \"over\": GM_deleteValue('autoRun'); alert(\"购物车导入完毕\"); &#125;&#125;()); // 在页面上植入触发按钮 jQuery('目标DOM').prepend('&lt;button id=\"autoRun\"&gt;自动加满购物车&lt;/button&gt;'); jQuery('#autoRun').click(function()&#123; createStep(); &#125;); 参考资料油猴脚本开发入门GM_cookie - HttpOnly Cookie #465","tags":[{"name":"tampermonkey","slug":"tampermonkey","permalink":"https://blog.kazaff.me/tags/tampermonkey/"},{"name":"油猴","slug":"油猴","permalink":"https://blog.kazaff.me/tags/油猴/"}]},{"title":"Centos7下安装puppeteer","date":"2021-03-22T09:37:00.000Z","path":"2021/03/22/centos7下安装puppeteer/","text":"刚做了个服务迁移，结果发现比预期想的事儿要多啊，不开心。。迁移后发现程序报错，提示缺少一大堆库文件。。。记忆里之前也解决过这个问题，搜了一下博客，竟然发现没有记录下来，那好吧，这次就补一下该内容。 其实一般碰到这种缺少库文件的问题时，没什么悬念，安装对应版本即可。所以我淡定的按照提示缺少的库名，进行了一轮安装： 1yum install -y at-spi2-atk libXcursor libXdamage cups-libs libXScrnSaver libXrandr atk pango gtk3 由于我是在服务器环境下运行的，所以肯定开启的headless模式，所以官方提示需要预装的一些界面库我就不需要了。有需要的同鞋可以参考下面这个列表： 1234567891011121314151617181920alsa-lib.x86_64atk.x86_64cups-libs.x86_64gtk3.x86_64ipa-gothic-fontslibXcomposite.x86_64libXcursor.x86_64libXdamage.x86_64libXext.x86_64libXi.x86_64libXrandr.x86_64libXScrnSaver.x86_64libXtst.x86_64pango.x86_64xorg-x11-fonts-100dpixorg-x11-fonts-75dpixorg-x11-fonts-cyrillicxorg-x11-fonts-miscxorg-x11-fonts-Type1xorg-x11-utils 打完收工~~ 参考资料Chrome headless doesn’t launch on UNIX","tags":[{"name":"centos","slug":"centos","permalink":"https://blog.kazaff.me/tags/centos/"},{"name":"puppeteer","slug":"puppeteer","permalink":"https://blog.kazaff.me/tags/puppeteer/"}]},{"title":"Mail-listener下的IMAP ID怎么搞？","date":"2021-03-18T09:37:00.000Z","path":"2021/03/18/mail-listener下的IMAP ID怎么搞/","text":"今天发现一直使用的nodejs的一个IMAP库，无法完成国内163邮箱的登录授权，会提示： Unsafe Login. Please contact kefu@188.com for help 太讨厌了。然后一顿乱搜，原来是163邮箱在登录认证的时候，需要客户端携带特定的身份参数。文章后面的参考资料里提供了官方的解释以及另一个博主的分享。 不过可惜，这些资料里提供的demo都不是基于nodejs的。所以还是只能靠自己来解决~~经过进行源码阅读，发现其实nodejs的底层imap库也是提供了对应的方法的，只是奇怪的是无法直接使用。相关代码如下： 123456789101112131415161718192021let &#123; MailListener &#125; = require(\"mail-listener5\");let mailWatcher = new MailListener(&#123; username: \"eyusei@163.com\", password: \"MSZOYXXOTHNTEERI\", host: \"imap.163.com\", port: 993, tls: true, //debug: console.log, tlsOptions: &#123;rejectUnauthorized: false&#125;, mailbox: \"INBOX\", searchFilter: [\"UNSEEN\"], markSeen: true, fetchUnreadOnStart: true, mailParserOptions: &#123;streamAttachments: false&#125;, attachments: false, attachmentOptions: &#123;directory: \"./attachments/\"&#125;&#125;);mailWatcher.start();mailWatcher.imap.id(&#123;\"name\": \"kazaffClient\", \"version\": \"1.0\"&#125;); 运行后会直接报错： Server does not support ID 进行debug后发现，原来在调用imap.id方法时，直接就被拒绝了： 1234Connection.prototype.id = function(identification, cb) &#123; if (!this.serverSupports('ID')) throw new Error('Server does not support ID'); ...... 完整代码看这里。 尝试将385~386两行注释掉后，就完全ok了。分析后果感觉是因为163邮件服务那边要求身份认证的时机和imap库认为的发送认证时机不匹配导致的。直接注释掉这个校验，由于163邮件服务器肯定支持ID校验，所以不会有任何问题。 唯一不开心的问题是直接修改源码导致后期升级麻烦。不过至少可以先跑起来了~~ 不知道有朋友知道更好的办法没有？请教了~ 参考资料解决网易163邮箱Unsafe Login.错误imap连接提示Unsafe Login，被阻止的收信行为","tags":[{"name":"163邮箱","slug":"163邮箱","permalink":"https://blog.kazaff.me/tags/163邮箱/"},{"name":"unsafe_login","slug":"unsafe-login","permalink":"https://blog.kazaff.me/tags/unsafe-login/"},{"name":"mail-listener","slug":"mail-listener","permalink":"https://blog.kazaff.me/tags/mail-listener/"}]},{"title":"Puppeteer如何下载pdf文件","date":"2021-01-19T09:37:00.000Z","path":"2021/01/19/puppeteer如何下载pdf文件/","text":"相信大家伙儿都清楚，访问一些常见的文件格式，浏览器是会“自作主张”的提供预览功能的，但有时候我们想直接下载到文件。尤其是在用puppeteer这种方式进行操作时，我们如何绕过浏览器的默认行为呢？ 拿PDF文件来举个例子吧，因为chrome默认会使用内置的pdf viewer来直接预览文件，而浏览器进入到预览模式后其实上下文发生了切换，导致无法像操作普通页面那样来操作页面上的元素。此时，我们就需要必须要找到办法直接下载文件了。 目前要下载文件，必须保证程序不是在headless模式下，换句话说即必须要有可视化窗口。接下来，我们就要动手绕过浏览器默认行为了。思路很简单，浏览器之所以会进行预览，是因为请求的响应头中，包含了浏览器适配预览的内容类型（content-type）标识。所以我们的目标就是要想办法覆盖这个响应头设置即可。 查阅了一下文章后，发现chrome确实提供了这种接口： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859...... const browser = await puppeteer.launch(&#123; headless: false, timeout: 60000, &#125;); const page = await browser.newPage(); const client = await page.target().createCDPSession(); // 激活 chrome devtools模式 await client.send('Fetch.enable', &#123; // 开启request domain的控制开关 patterns: [ &#123; urlPattern: '*', // 设置视图控制的request url模式 requestStage: 'Response', // 设置 目标请求的控制阶段， Response表示为拿到服务器响应内容，并在交给浏览器流程之前的阶段 &#125; ] &#125;); // 所有匹配url模式的请求都会触发这个事件回调，再没有手动触发Fetch.fulfillRequest，continueRequest, failRequest等事件之前，请求处于暂停状态 await client.on('Fetch.requestPaused', async (reqEvent)=&gt;&#123; const &#123;requestId&#125; = reqEvent; // 获取浏览器为当前请求分配的编号 // 检查默认的responseHeader中是否包含目标内容 let responseHeaders = reqEvent.responseHeaders || []; let contentType = ''; for(let elements of responseHeaders)&#123; if(elements.name.toLowerCase() == 'content-type')&#123; contentType = elements.value; &#125; &#125; if(contentType.endsWith('pdf'))&#123; responseHeaders.push(&#123; // 直接覆盖默认的格式为附件类型，避免触发浏览器的默认预览模式 name: 'content-disposition', value: 'attachment', &#125;); const responseObj = await client.send('Fetch.getResponseBody', &#123;requestId&#125;); // 获取原本请求的响应内容 await client.send('Fetch.fulfillRequest', &#123; // 覆盖默认的响应数据属性 requestId, responseCode: 200, responseHeaders, body: responseObj.body, &#125;); &#125;else&#123; await client.send('Fetch.continueRequest', &#123;requestId&#125;); // 直接“放行”，不进行任何属性覆盖 &#125; &#125;); try&#123; await client.send('Page.setDownloadBehavior', &#123;behavior: 'allow', downloadPath: \"这里填写想要保存附件的位置，默认会下载到系统的downloads文件夹\"&#125;); await page.goto('这里填写pdf目标链接'); &#125;catch(err)&#123; console.log(err); // 目前会抛出一个异常，忽略即可。 &#125; await client.send('Fetch.disable'); // 关闭 request domain的控制开关 await client.detach(); // 关闭cdpSession...... 这样处理后，chrome就不会做任何“多余”的操作，直接老老实实的把文件下载到本地了。 不过需要注意的是，下面参考资料中，把这个方案用到的一些方法表示为：实验性（Experimental），已弃用（Deprecated）。不排除未来的新版本chrome不再支持这些接口了。 希望这篇文章帮到你了，回见~~ 参考资料stack overflowchrome devtools protocolCDPSession","tags":[{"name":"puppeteer","slug":"puppeteer","permalink":"https://blog.kazaff.me/tags/puppeteer/"},{"name":"chrome","slug":"chrome","permalink":"https://blog.kazaff.me/tags/chrome/"},{"name":"devtools","slug":"devtools","permalink":"https://blog.kazaff.me/tags/devtools/"},{"name":"pdf","slug":"pdf","permalink":"https://blog.kazaff.me/tags/pdf/"}]},{"title":"重新认识Innodb","date":"2020-12-25T09:37:00.000Z","path":"2020/12/25/重新认识Innodb/","text":"打从第一天上班开始，就在与Mysql打交道，当时也啃过几本评分很高的讲mysql的书。记得曾经看过书中讲的眼花缭乱的复杂sql后也跃跃欲试的想要在项目中大展拳脚，但转眼就被各种慢查询和数据不一致问题虐的体无完肤。经历过各种尴尬，看过不少前辈的分享后，发现其实常见的互联网项目，大量的逻辑可以很好的在应用代码这一侧就完成，不仅能保证效率，有利于维护，而且未来的优化空间对一般开发人员来说也比较大，不至于面对数据库这个黑匣子一脸懵逼。所以从业一段时间后，自己接受的项目中就已经很少见什么子查询，各种join，更直接就禁用了存储过程等等。 我相信，不少人和我一样，停留在这个阶段。我之所以用“停留”这个词，就是想表达：这种程度还远远不够。我们可以让mysql这样的DB只用于持久化，但这并不代表就可以完全无视它的基础法则。如果你觉得你的sql足够简单，就可以完全不在意mysql的“复杂玩法”，那你就只能等着挨打了~~ 写这么多，想必应该看得出，刚刚被打的，正是在下吧~ 最近在帮同事排查问题的时候，发现自己在mysql方面有一些基础知识缺失了，导致从问题表象上看，各种反直觉。所以就决定花时间先把基础知识“复习”一下。 接下来我们就直奔主题，把这次复习到的内容全部总结出来（现学现卖），有兴趣的童鞋一定要认真看本文最后的“参考资料”，那里面会有大量的干货和实例。 Innodb通常情况下我们选择使用mysql这样的传统db，多半离不开ACID，而要获取这样的保证，就肯定要基于事务，而mysql下支持事务的引擎就是Innodb，所以咱们本篇内容就只围绕Innodb来讲。 一般常识下，大家都知道Innodb支持行锁，有更好的并发性能，但其实这个理解中存在着很大的认知缺陷。首先，Innodb的行锁是基于索引的，假如你的where条件无法使用索引的前提下，就会产生“锁表”（也有文章提到说是临键值锁，只是区间覆盖全表）。所以可见索引的重要性，可不仅仅是针对查询提速哦。 其次，一般情况下我们并不是“生硬”的直接去inert/update/delete，而是根据db中现有的数据值基础上进行一些数据变更（直白点讲说，就是使用select…for update），如果在事务中没有明确的申请查询锁（共享锁/排它锁）话，根据不同的事务隔离设置，会有截然不同的逻辑，如 快照读。 事务隔离咱们先来捋顺这个概念，mysql的事务隔离有四种： 读未提交 读已提交（推荐） 可重复读（默认） 串行化 不同的事务隔离设置主要是针对业务对“脏读，不可重复读，幻读”的接受度来考量的。当然，通常我们的项目肯定是不接受“脏读”的（即提前读到其他事务未commit的数据变更），但是否一定要求“可重复读”，这一点就要大家好好根据自身的业务需要好好思考了。 何为“可重复读”，主要就是指在事务中多次读取（未明确申请任何锁的前提下）相同的数据范围，得到的数据集是相同的，不会发生变化。mysql主要是依靠“快照读”来做到的，这里一定要注意一些细节：快照版本和事务开启的时刻无关，仅和该事务中第一次读操作的时刻有关，即从第一次读操作发生的时刻开始，其它任何事务的任何晚于这个时刻的提交，本事务都无法“感知”。这个特点给mysql在保证可重复读的前提下带来的很高的并发性能（避免了读写锁阻塞）。 各种锁如果你在事务中明确申请锁，那你的读取操作就不会在是快照读了，而是会尝试加锁获取当前数据。 接下来我们来看看，常见的锁： 共享锁/排它锁 间隙锁 记录锁（行锁） 临键锁（间隙锁+记录锁） 自增锁 共享锁其实就是读锁（如 select…lock share mode），解决读读并发问题。排它锁就是写锁（如 select…for update, update, delete），排斥一切，包括自己。其实还有“意向共享锁/意向排它锁”，它们是基于表的，存在的价值就是提速锁判断。具体大家可以自己阅读相关文献。 根据sql的where条件的范围，我们可以一次性获取 N 条数据的 读或写锁。这些锁，可能是记录锁（如 where id=1），也可能是临键锁（如 where id &gt; 1），即 记录锁 + 间隙锁。 记录锁很容易理解，就是我们一直挂嘴边的行锁。那什么是间隙锁呢？它主要是来解决幻读的，假如事务A锁定一个数据范围（如 where id between 1 and 10），事务B尝试insert一条记录（如 id = 5，假如之前不存在id=5的记录）时，由于事务A在这个范围上加了间隙锁，事务B只能等待。当然，事务B更不可能update/delete任何记录，因为也都被事务A加上了记录锁。这里要额外叮嘱一下：此时事务B如果进行select id=5 for update的时候，是不会阻塞的，而是会直接返回记录不存在。但随后可能的insert肯定会阻塞。 另外要注意哦，前面我们提到当使用的where条件没有命中索引的时候，mysql会进行全表范围的间隙锁哦~~ 最后咱们在来看一下自增锁，这个主要是来对那些业务需要保证多次插入得到连续的自增值的场景，默认mysql是没有开启的。但若开启后，任何一个事务开启后，相当于会在整个表上增加一个自增锁，用来阻塞其他事务的insert操作。根据这个定义，存在一个有意思的事儿：假设我们有一张新空表test，保持默认设置（即未开启自增锁），然后先开启A事务，然后插入一条数据，不提交，再开启B事务，也插入一条数据，此时提交B事务，你觉得B事务插入的记录的自增字段的值会是多少？答案是：2。值会基于事务A未提交的那条记录的自增值+1，即便是在事务B提交前，事务A回滚也如此。 有点意思吧。 orderby + limit 下的任性最后，我们来看一个更“冷”的知识。在我们处理分页的时候，如果你的orderby时，刚巧碰到了多条记录该排序字段的值是相同的。你猜mysql会如何做？随机排序，吃惊么？其实一般场景下对这种处理并不敏感，甚至无视。 但加上limit设置的话，好死不死刚好随机发生在limit条件上，对应的分页结果就会表现的随机性。这还不是最狗血的，假如你的业务比较冷门，要基于当前表中的记录按照指定排序条件获取期望的特定数据值时（limit 0,1），恐怕这种随机性会让你抓狂。 我们的某个遗留系统中，就碰到这种问题，明明开了事务，加了锁，但按照创建时间倒序，想要获取最新的一条记录的订单号，在基于该订单号+1的逻辑创建新的订单号时，在压力测试（会发生同一秒创建多笔订单）的时候还是会出现订单号重复的bug。 我们先不管业务订单号必须连续的合理性，以及sql语句的严谨性之外，单单这个mysql的随机返回结果的特性，都让排查问题的时候基本抓瞎。 总结了解了上面的基础知识后，相信在使用innodb的时候，尤其是处理一些“异常”的时候，会有思路一些吧。强烈建议大家把下面分享的文献读一下，很开阔视野。 参考资料挖坑，InnoDB的七种锁4种事务的隔离级别，InnoDB如何巧妙实现？InnoDB的快照读，到底和什么相关？数据库索引，到底是什么做的？58到家MySQL军规升级版Mysql order by与limit混用陷阱","tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.kazaff.me/tags/mysql/"},{"name":"innodb","slug":"innodb","permalink":"https://blog.kazaff.me/tags/innodb/"},{"name":"锁","slug":"锁","permalink":"https://blog.kazaff.me/tags/锁/"},{"name":"事务","slug":"事务","permalink":"https://blog.kazaff.me/tags/事务/"},{"name":"快照读","slug":"快照读","permalink":"https://blog.kazaff.me/tags/快照读/"}]},{"title":"Github更换认证方式后sourcetree的设置","date":"2020-11-28T09:37:12.000Z","path":"2020/11/28/github更换认证方式后sourcetree的设置/","text":"个人非常喜欢sourcetree这款git客户端工具，比较喜欢它的设计理念，即将所有功能都集成在同一个UI中，而不是散落在不同的菜单或文件夹里。不过之前一直用github的https协议来作为项目的git地址，sourcetree似乎对https协议的项目默认使用的认证授权方式就是账号密码。但是随着github近期调整了认证方式，不再允许第三方工具基于账号密码来访问和管理项目了，所以我在sourcetree无法推送更新到github的代码仓库了。。。。这就尴尬了。 一开始认为github推荐的OAuth方案，sourcetree应该也提供了对接吧。可不巧的是并没有，这就尴尬了。不过我们还可以使用ssh协议+证书的方式来打通sourcetree和github，说干就干！！ 首先，打开你的sourcetree，再顶部主菜单中选择 “Tools -&gt; Create or Import SSH Keys”，在弹出的窗口中点击“Generate”，根据提示再窗口中央的空白区域不停的摩擦鼠标直至生成完毕，然后我们把创建的“public key”的字符串拷贝好，并点击“Save private key”将私钥文件保存在文件中。 接下来，登录你的github的web后台，点击右上角你的头像，选择 “setting -&gt; SSH and GPG keys”，在跳转后的页面，左上角点击“New SSH key”，将上一步拷贝的公钥字符串黏贴在页面的“Key”输入框中，“Title”填写一个你觉得合适的名字即可。 最后，再回到sourcetree的界面， 点击 “Tools -&gt; Options” 打开sourcetree的选项菜单，然后在 “General” 选项卡中找到“SSH Client Configuration”设置块，其中的 “SSH Key”项我们就设置成前面我们私钥文件即可。 最后，在github中获取clone链接的时候，记得切换成SSH协议，用该协议的项目地址作为sourcetree使用的仓库地址，就完成了所有的配置哦。 祝福好运~~","tags":[{"name":"github","slug":"github","permalink":"https://blog.kazaff.me/tags/github/"},{"name":"sourcetree","slug":"sourcetree","permalink":"https://blog.kazaff.me/tags/sourcetree/"}]},{"title":"DDD读书摘录","date":"2020-11-20T09:37:12.000Z","path":"2020/11/20/DDD学习摘录/","text":"最近在复习 领域驱动设计 相关的知识，主要是因为越发的觉得DDD思想和方法论能帮助自己甄别好坏以及辅助设计。很早之前就读过DDD知识领域相关的“圣经”，但是初读时候并没有悟道太多，打算花时间再把能搜集到的资料整理一下看是否能沉淀更多价值出来。索性全部记录在这篇博文中，方便自己日后复习和查证。目前没有想好要以如何结构来编排内容，暂时都一股脑的记录下来，以后有好的归纳想法后再调整吧，如果觉得阅读起来很乱，不妨试试用关键词检索的方式来抓取内容。 领域建模的步骤 在事件风暴中梳理业务过程中的用户操作、事件以及外部依赖关系等，根据这些要素梳理出领域实体等领域对象。 根据领域实体之间的业务关联性，将业务紧密相关的实体进行组合形成聚合，同时确定聚合中的聚合根、值对象和实体。在这个图里，聚合之间的边界是第一层边界，它们在同一个微服务实例中运行，这个边界是逻辑边界，所以用虚线表示。 根据业务及语义边界等因素，将一个或者多个聚合划定在一个限界上下文内，形成领域模型。在这个图里，限界上下文之间的边界是第二层边界，这一层边界可能就是未来微服务的边界，不同限界上下文内的领域逻辑被隔离在不同的微服务实例中运行，物理上相互隔离，所以是物理边界，边界之间用实线来表示。 DDD与微服务的关系DDD是一种架构设计方法，微服务是一种架构风格。 DDD主要关注：从业务领域视角划分领域边界，构建通用语言进行高效沟通，通过业务抽象，建立领域模型，维持业务和代码的逻辑一致性。 微服务主要关注：运行时的进程间通信、容错和故障隔离，实现去中心化数据管理和去中心化服务治理，关注微服务的独立开发、测试、构建和部署。 领域，子域，核心域，通用域，支撑域领域：在研究和解决业务问题时，DDD会按照一定的规则将业务领域进行细分，当领域细分到一定的程度后，DDD会将问题范围限定在特定的边界内，在这个边界内建立领域模型，进而用代码实现该领域模型，解决相应的业务问题。简言之，DDD的领域就是这个边界内要解决的业务问题域。 子域: 领域可以进一步划分为子领域。我们把划分出来的多个子领域称为子域，每个子域对应一个更小的问题域或更小的业务范围。 核心域：决定产品和公司核心竞争力的子域是核心域，它是业务成功的主要因素和公司的核心竞争力。 通用域：没有太多个性化的诉求，同时被多个子域使用的通用功能子域是通用域。 支撑域：既不包含决定产品和公司核心竞争力的功能，也不包含通用功能的子域，但这些功能子域又是必须的，那它们就是支撑域。 举个实际的例子，我们将桃树细分为了根、茎、叶、花、果实和种子等六个子域，那桃树是否有核心域？有的话，到底哪个是核心域呢？ 不同的人对桃树的理解是不同的。如果这棵桃树生长在公园里，在园丁的眼里，他喜欢的是“人面桃花相映红”的阳春三月，这时花就是桃树的核心域。但如果这棵桃树生长在果园里，对果农来说，他则是希望在丰收的季节收获硕果累累的桃子，这时果实就是桃树的核心域。 限界上下文限界上下文：用来封装通用语言和领域对象，提供上下文环境，保证在领域之内的一些术语、业务相关对象等（通用语言）有一个确切的含义，没有二义性。这个边界定义了模型的适用范围，使团队所有成员能够明确地知道什么应该在模型中实现，什么不应该在模型中实现。领域边界就是通过限界上下文来定义的，将限界上下文内的领域模型映射到微服务，就完成了从问题域到软件的解决方案。。理想情况下，子域和限界上下文是重合的，但子域可能会包含多个限界上下文。 限界上下文所界定的边界，究竟是逻辑边界，还是物理边界？这并没有定论，需得依据不同场景而做出不同的决策。 逻辑边界：所有的限界上下文都部署在同一个进程中，因此不能针对某一个限界上下文进行水平伸缩。编写代码时，我们需要谨守这条无形的逻辑边界，时刻注意不要逾界，并确定限界上下文各自对外公开的接口，避免对具体的实现产生依赖。 物理边界：每个限界上下文就变成了一个个细粒度的微服务。可以保证边界内的服务、基础设施乃至于存储资源、中间件等其他外部资源的完整性，最终形成自治的服务。限界上下文之间仅仅通过限定的方式以限定的通信协议和数据格式进行通信，除此之外，彼此没有任何共享。但是运维与监控的复杂度也随之而剧增。 聚合，聚合根，实体，值对象实体：有ID标识，通过ID判断相等性，ID在聚合内唯一即可。对这些对象而言，重要的不是其属性，而是其延续性和标识，对象的延续性和标识会跨越甚至超出软件的生命周期。我们把这样的对象称为实体。在DDD里，这些实体类通常采用充血模型，与这个实体相关的所有业务逻辑都在实体类的方法中实现，跨多个实体的领域逻辑则在领域服务中实现。 值对象：无ID，不可变，无生命周期，用完即扔。值对象只是若干个属性的集合，只有数据初始化操作和有限的不涉及修改数据的行为，基本不包含业务逻辑。值对象的属性集虽然在物理上独立出来了，但在逻辑上它仍然是实体属性的一部分，用于描述实体的特征。在值对象中也有部分共享的标准类型的值对象，它们有自己的限界上下文，有自己的持久化对象，可以建立共享的数据类微服务，比如数据字典。 聚合：由业务和逻辑紧密关联的实体和值对象组合而成的，聚合是数据修改和持久化的基本单元，每一个聚合对应一个仓储，实现数据的持久化。聚合有一个聚合根和上下文边界，这个边界根据业务单一职责和高内聚原则，定义了聚合内部应该包含哪些实体和值对象，而聚合之间的边界是松耦合的。聚合内实体以充血模型实现个体业务能力，以及业务逻辑的高内聚。跨多个实体的业务逻辑通过领域服务来实现，跨多个聚合的业务逻辑通过应用服务来实现。 聚合根：聚合根的主要目的是为了避免由于复杂数据模型缺少统一的业务规则控制，而导致聚合、实体之间数据不一致性的问题。首先它作为实体本身，拥有实体的属性和业务行为，实现自身的业务逻辑。其次它作为聚合的管理者，在聚合内部负责协调实体和值对象按照固定的业务规则协同完成共同的业务逻辑。最后在聚合之间，它还是聚合对外的接口人，以聚合根ID关联的方式接受外部任务和请求，在上下文内实现聚合之间的业务协同。也就是说，聚合之间通过聚合根ID关联引用，如果需要访问其它聚合的实体，就要先访问聚合根，再导航到聚合内部实体，外部对象不能直接访问聚合内实体。 怎么设计聚合 第5步：多个聚合根据业务语义和上下文一起划分到同一个限界上下文内。 需要说明一下：投保人和被保人的数据，是通过关联客户ID从客户聚合中获取的，在投保聚合里它们是投保单的值对象，这些值对象的数据是客户的冗余数据，即使未来客户聚合的数据发生了变更，也不会影响投保单的值对象数据。从图中我们还可以看出实体之间的引用关系，比如在投保聚合里投保单聚合根引用了报价单实体，报价单实体则引用了报价规则子实体。 判断一个实体是否是聚合根，可以结合分析： 是否有独立的生命周期？是否有全局唯一ID？ 是否可以创建或修改其它对象？ 是否有专门的模块来管这个实体？ 聚合的一些设计原则 在一致性边界内建模真正的不变条件。聚合内有一套不变的业务规则，各实体和值对象按照统一的业务规则运行，实现对象数据的一致性，边界之外的任何东西都与该聚合无关，这就是聚合能实现业务高内聚的原因。 设计小聚合。如果聚合设计得过大，聚合会因为包含过多的实体，导致实体之间的管理过于复杂，高频操作时会出现并发冲突或者数据库锁，最终导致系统可用性变差。 通过唯一标识引用其它聚合。聚合之间是通过关联外部聚合根ID的方式引用，而不是直接对象引用的方式。外部聚合的对象放在聚合边界内管理，容易导致聚合的边界不清晰，也会增加聚合之间的耦合度。 在边界之外使用最终一致性。在一次事务中，最多只能更改一个聚合的状态。如果一次业务操作涉及多个聚合状态的更改，应采用领域事件的方式异步修改相关的聚合，实现聚合之间的解耦。 通过应用层实现跨聚合的服务调用。应避免跨聚合的领域服务调用和跨聚合的数据库表关联。 领域事件领域事件驱动设计可以切断领域模型之间的强依赖关系，事件发布完成后，发布方不必关心后续订阅方事件处理是否成功，这样可以实现领域模型的解耦，维护领域模型的独立性和数据的一致性。在领域模型映射到微服务系统架构时，领域事件可以解耦微服务，微服务之间的数据不必要求强一致性，而是基于事件的最终一致性。 应用服务，领域服务应用服务：位于应用层。应用服务会对多个领域服务或外部应用服务进行封装、编排和组合，对外提供粗粒度的服务，是一段独立的业务逻辑。 领域服务：位于领域层。领域服务封装核心的业务逻辑，实现需要多个实体协作的核心领域逻辑。它对多个实体或方法的业务逻辑进行组合或编排，或者在严格分层架构中对实体方法进行封装，以领域服务的方式供应用层调用。所以如果有的实体方法需要被前端应用调用，我们会将它封装成领域服务，然后再封装为应用服务。为隐藏领域层的业务逻辑实现，所有领域方法和服务等均须通过领域服务对外暴露。为实现微服务内聚合之间的解耦，原则上禁止领域服务进行跨聚合的编排和跨聚合的数据相互关联。 DDD分层 DDD分层架构有一个重要的原则：每层只能与位于其下方的层发生耦合。 数据视图数据视图应用服务通过数据传输对象（DTO）完成外部数据交换。领域层通过领域对象（DO）作为领域实体和值对象的数据和行为载体。基础层利用持久化对象（PO）完成数据库的交换。 DTO 与 VO 通过 Restful 协议实现 JSON 格式和对象转换。 前端应用与应用层之间 DTO 与 DO 的转换发生在用户接口层。如微服务内应用服务需调用外部微服务的应用服务，则 DTO 的组装和 DTO 与 DO 的转换发生在应用层。 领域层 DO 与 PO 的转换发生在基础层。 DDD中的读操作在DDD的写操作中，我们需要严格地按照 “ 应用服务 -&gt; 聚合根 -&gt; 资源库 ” 的结构进行编码，而在读操作中，采用与写操作相同的结构有时不但得不到好处，反而使整个过程变得冗繁。常见的3种读操作的方式： 基于领域模型的读操作 读操作完全束缚于聚合根的边界划分； 导致的结果是Repository上处理了太多的查询逻辑，变得越来越复杂，也逐渐偏离了Repository本应该承担的职责 基于数据模型的读操作 由于读操作和写操作共享了数据库，而此时的数据库主要是对应于聚合根的结构创建的，因此读操作依然会受到写操作的数据模型的牵制 CQRS 复杂度高(与“基于数据模型的读操作”不同的是，在CQRS中写操作和读操作使用了不同的数据库，数据从写模型数据库同步到读模型数据库，通常通过领域事件的形式同步变更信息) 数据一致性 受 数据同步策略影响 参考文献 《DDD实战课》 限界上下文的边界 领域驱动设计(DDD)编码实践 基于DDD的微服务设计和开发实战 细数软件架构中的解耦","tags":[{"name":"领域建模","slug":"领域建模","permalink":"https://blog.kazaff.me/tags/领域建模/"},{"name":"微服务","slug":"微服务","permalink":"https://blog.kazaff.me/tags/微服务/"},{"name":"读写分离","slug":"读写分离","permalink":"https://blog.kazaff.me/tags/读写分离/"}]},{"title":"蝉","date":"2020-07-06T09:37:12.000Z","path":"2020/07/06/蝉/","text":"昨天做了一个非常刺激的梦，好像电影一样。我尝试把它写出来，并按照我希望的样子进行一点点加工，但90%的内容确实都是梦中的景象。这个故事，有一点恐怖，但留给我更多的是那种细思极恐的感觉，非常奇怪的是最近也没有看过惊悚片，可能大脑太渴望了，就自发的以梦的形式自己导演了一步。我不是很擅长写这种东西，但又非常想记录下它，所以可能叙事混乱，请见谅。 —————-正文分割线—————– 先介绍一下故事的主人公吧，“森林”是个普普通通的80后打工族，每天朝八晚五点半，和大学同学结婚，并有个快两岁的宝宝。毕业十几年，还保持联系的大学同学就2人，一个外号“老杂”，一个外号“明哥”，他们经常在一个只有我们仨的微信群里闲聊。不知道从什么时候开始，森林和老杂就习惯性的分享自己的梦，都是一些奇奇怪怪的梦，有时候做了一个非常屌的梦想在群里分享的时候，才发现早已记不清了。但即便如此，他们还是会偶尔聊到前一晚的梦。 这一天是2020年7月6号，森林前一晚做了一个吊炸天的梦，早上起来就迫不及待的想在微信群里分享。可是另外两人似乎没有啥兴趣听，但森林实在太想分享了，所以自顾自的开始不停的在群里发着消息。 “我昨儿做了个梦，你们想听么？超级牛逼” –森林 “。。讲啊，你已经忘了吧” –老杂 “牛逼，讲啊，不讲是老杂的几把” –明哥 “慌个毛线，这就开始~” –森林“我昨儿梦见我在一个酒店包间里，和家人聚会，其实这个地方前段时间确实去过，梦里好像就是那次，但人比较多” –森林“包间里有我爸妈，我姥姥，你们知道我姥姥已经过世了，对吧？ 还有我老婆孩子，还有我妹和她男朋友” –森林“我妹的男朋友小陈是南方人，梦里面我们好像点了一盘菜，是油炸金蝉吧，我以前根本就不会吃的东西” –森林“在小陈的推荐下，我决定尝一次鲜，但是我确实对这种东西很抵触，小心翼翼的放嘴里后并不敢嚼，喊了一会儿就吐了” –森林“梦里都能感受到兴奋的心跳和满嘴的腥咸，很逼真的感觉” –森林“我觉得去洗手间漱漱口，小陈和我妹起身要和我一起去，我们仨就出了包间，但是梦里洗手间和包间的距离好远，我们边走边闲聊” –森林“然后我开始和他们讲前一天晚上我做的梦，牛逼吧，梦中梦，屌不屌？” –森林“我说，梦里的事儿我记得是开始在我下班的路上，我和同事陈鑫挤上了回家的公交车，应该是傍晚吧，但天比较亮” –森林“车上人虽然多，但依然有座位，所以我们俩就找了个座儿坐下了，但是似乎公交车并没有急着开” –森林“我靠窗的位置，因为天气热所以车窗是半开的，突然从外面飞进来一只金蝉，落在我手臂上，我从小对昆虫就比较抵触，吓的我忍不住叫了一声” –森林“我的同事先是楞了一下，然后不知为何开始破口大骂，大意是这么热的天也不开空调，开着窗户都往里面飞虫子，不嫌脏么” –森林“我不想搞得动静太大，连忙示意他别骂了，嘘！” –森林“可突然我发现，整个车厢都非常安静，所有乘客都以趴姿坐在各自的座位上，司机也趴在方向盘上。” –森林“我突然觉得后背发凉，我们俩小心翼翼的往司机的位置挪动，然后整个梦中的世界就慢慢的变暗，最后彻底黑色了” –森林“然后我妹问我：“就完了？”，我说，然后镜头再次亮了起来，但是视角是在马路对面，一群路人看到路中间横着一辆公交车” –森林“公交车的玻璃全部都是黑色的，根本看不到里面，有胆子大的一些人就慢慢靠近，然后一点一点的拉开了司机那一侧的车门” –森林“车门开了一条缝后，大家看到驾驶位置上只有一身衣服，空瘪瘪的挂在方向盘上，人不见了，等门开的再大一点的时候” –森林“突然从后面车厢开始一阵阵震耳欲聋的金蝉的叫声，然后大量的金蝉就跟蝙蝠一样从开着的门缝中窜了出来，把路人吓跑了一大片” –森林“大概一刻钟后，人们进到公交车里，看到的只是每个座位上的衣服，人全部都不见了。 恐怖么？” –森林“小陈大笑，说你真能编啊，还梦呢，你是因为刚吃了金蝉，才编这个故事来吓我们吧” –森林“听到小陈这么说，我突然楞住了，站在原地一动不动，是啊，好巧啊，为什么我前一天就梦到了金蝉？” –森林“然后我的梦就醒了” –森林 “就这？就这就给你吹的那么牛逼么” –明哥 “还行吧，可能是你没有办法把梦里的感觉完全的描述出来” –老杂 “对，对，我确实讲不出来梦里的感受，反正梦里非常的恐怖” –森林 “嗯，没别的事儿我就出门了啊，今儿还要去赣州呢” –明哥 “写代码去” –老杂 “88” –森林 微信群恢复了平静，然后荧幕再次切换，一只蝉趴在键盘上，一只蝉趴在在ipad上，一只蝉趴在手机上，三个设备屏幕中，显示着一个微信群的窗口，最后一条消息是： “88”","tags":[{"name":"知了","slug":"知了","permalink":"https://blog.kazaff.me/tags/知了/"}]},{"title":"Mapping Set to Strict, Dynamic Introduction of [_Class] Within [_Doc] Is Not Allowed","date":"2020-06-26T09:37:12.000Z","path":"2020/06/26/mapping set to strict, dynamic introduction of [_class] within [_doc] is not allowed/","text":"这个异常非常的“有趣”，因为我GG了一圈，竟然一个相关的结果都没有。。。一开始以为自己碰到疑难杂症了，深呼吸一口打算去翻源码了。结果“非常不小心”被我在官方文档上以“_class”作为关键字检索到了相关的内容，好尴尬啊~ 官方在Mapping Rules这一章节中，提到了Type Hints的概念： Mapping uses type hints embedded in the document sent to the server to allow generic type mapping. Those type hints are represented as _class attributes within the document and are written for each aggregate root. 在我这个新手看来，应该是Spring-Data-Elasticsearch库对将Elasticsearch返回的结果自动封装成POJO时需要的一个标识。很自然也很直观，对吧~ 恶心就恶心在，如果你和我一样是初次使用，并且在Elasticsearch中将Index的Mapping设置成严格的静态类型，就会导致本文标题的这个异常。然后你就会开始发蒙，_class这个属性是什么时候产生的。。。 解决这个异常的方法很简单，在我们的Mapping中增加_class属性的定义即可： 12345678...\"mappings\": &#123; \"dynamic\": \"strict\", \"properties\": &#123; \"_class\" : &#123; \"type\" : \"keyword\" &#125;,.... 奇怪的是，全世界的开发者好像都没有碰到这个问题。。。是我确实太菜了吧可能。。","tags":[{"name":"elascticsearch7+","slug":"elascticsearch7","permalink":"https://blog.kazaff.me/tags/elascticsearch7/"},{"name":"spring-data-elasticsearch","slug":"spring-data-elasticsearch","permalink":"https://blog.kazaff.me/tags/spring-data-elasticsearch/"}]},{"title":"关于电商系统的搜索框","date":"2020-05-26T10:37:12.000Z","path":"2020/05/26/关于电商系统的搜索框/","text":"这将会是一篇概括类的文章，可能最终遗留的问题比解决的多得多~~不是很有自信推荐你看，但如果有空闲时间的话，不妨跟着我一起思考其中的相关问题！注意: 本文章对解决问题的思考篇幅多于实战，且许多问题的解决方案也停留在理论阶段，未必有足够合理的可实施性。之所以分享出来，仅仅在于怕遗忘或错过思考过程中的火花，若能给你带来好的灵感，纯属巧合，也是我的荣幸。如若能将你的高见分享于我，那就太棒了！ 需求背景是源于我们公司的一个电商网站的改版，功能参考的是JD平台，本次主题主要围绕着商品搜索方面。其实现在大一些的电商平台，搜索体验做的都非常的好。换句话说，用户对于一个好的搜索体验已经达成了基本的共识，包括但不限于下面几点： 输入自动完成 可以对拼写自动纠错 支持拼音缩写检索 精准度和召回率上有一个比较好的平衡 返回结果能有一定的随机性提供惊喜 这些功能，如果打算直接依托于传统的关系型数据库(如mysql)来实现的话，实现成本和性能都很难满足预期。而目前在这主题上深耕多年并建树的开源框架也屈指可数，我们这里选择影响力最大“之一”的Elasticsearch来作为主要解决方案。 Elasticsearch入门这篇文章不包含任何科普Elasticsearch的内容，强烈建议有兴趣的小伙伴精读《Elasticsearch in action》这本书，在这里我们就只列一下需要掌握的基本概念和术语： 文档，类型和索引的概念和关系 节点，分片和副本的概念和关系 Mapping的写法和作用 CURD的DSL语法，尤其是各种Search语法 分析器，分词器和分词过滤器的概念和用法 聚集和桶的概念和用法 文档间的关系：对象型，嵌套型，父子型，反规范化 优化segments的手段有哪些 除了上面列出的这些外，书中还有大量宝贵的内容，再次建议花时间阅读完整本书的内容~~ 环境搭建我们当然需要一个本地的测试环境来练手，对吧~~当然Elasticsearch本身就已经是开箱即用了，大家去官网下载压缩包并按照说明就可以简单运行起来。我这里还是推荐直接在docker中下载官方的镜像即可~~本篇文章的所有内容都是在Elasticsearch6.5.1版本上做的（因为很早前搭建ELK环境的时候就装好了，懒得升级版本了~）。 强烈建议同时安装Kibana，它提供的Dev Tools非常方便我们调试语句。 中文分词 和 词库Elasticsearch默认的分词器对中文不是很友好，不过社区早已填补了这个遗憾。GG一下就会发现很多文章都推荐使用iK中文分词，使用和配置也非常的简单。我在文章末尾的“参考文献”中贴出了我当时找到的资料地址，按照步骤很容易就安装成功了~ 如果你也是在docker里跑的本地环境的话，记得直接重启容器，安装的插件才会生效哦~ 当然每年都会诞生一批新的流行词语，这会对分词器造成很多困扰，不过iK的分词器支持扩展词库，而且同时提供静态词库和热更新方案，非常贴心了算是！我们可以从网上搜索到比较新的词库文件，然后直接以静态文件的方式配置在iK配置文件中，然后在我们的电商系统后台，提供一个功能来自定义词库，使用热更新来动态更新到iK中。 更新了词库文件还不够哦，因为那些已经索引起来的数据是无法自动使用新的分词结果的，所以你需要reindex一下。如果系统中有海量的数据的话，这个步骤可能会花很久，而且也可能会造成系统资源溢出问题，建议多了解一下这方面的资料，已经有很多相关的文献供参考了，无需担心~~ 至于前面提到的词库，网上不少人推荐去搜狗上下载，不过下载的文件并不是txt，所以我们需要在线转换一下~ 这部分的最后，我们还需要提到同义词这个问题。一个好的同义词库，可以让搜索的结果产生更多惊喜，例如用户搜索“番茄”，搜索结果里还会自然的包含“西红柿”。这种看似理所当然的逻辑，对于计算机来说其实是很麻烦的。还好Elasticsearch提供了synonym特性，我们可以优雅的解决这个问题~ 不过如果想让synonym也拥有iK扩展词库那样的热更新能力，就得靠自己动手了，最后的“参考文献”中我贴了一个前辈自己封装的版本，不过可能需要根据你使用的具体版本进行调整，但是他的项目还是很有指导意义的~ filter vs. query简单的说，并不是所有的场景都一味的使用query context的，如果我们是针对精确的值进行筛选，那我们使用filter context更加的合理。它们的差别是，filter性能更好，不仅仅有缓存，也省去了为结果进行相关性打分的环节。 但在我们的场景中，其实相关性打分是很重要的，这并不是说明filter对我们来说就没有意义了。例如用户在某个分类下进行检索，甚至设定了价格区间范围，商品原产地条件等等，在这些情况下filter就非常的合适了。当然，这些用在filter中的条件，也都不是经过分析器处理后的原始内容。 排序，分页搜索离不开分页，而Elasticsearch最基本的分页方法和SQL的用法没什么两样：from 和 size 就搞定了。但是如果是海量的数据条件下，它的性能也和传统的SQL分页一样存在性能问题，如Mysql的limit 100000, 10这种。Elasticsearch还是提供了对应的解决方案的：Scroll。这种我记得好像某些Nosql产品也有类似语法~~ 在一些特定业务场景下，也可以根据数据的特定字段进行辅助的分页，例如自增的id，或更新时间等线性数据类型，且被依赖的字段必须保证足够的唯一性。但是这种场景对于前台系统复杂的检索条件，不是很适配。为什么呢？这就涉及到另外一个话题：排序。 前面提到过，Elasticsearch默认是按照相关性得分进行排序的，这个得分是动态计算出来的，所以肯定不是线性的。即便是你的场景不需要依赖得分排序，但也很难固定的就使用id这种“毫无意义”的排序条件吧。一个好的排序，可能让用户得到更需要ta得到的结果！为何这么说呢？我们把商品列表页面比作一个大型商超的货架，卖家为了将一些利润更多的，或者急需要倾销的商品摆在更明显的地方，有助于达到销售目的。所以结果的排序，并不是单纯的基于相关性得分就能达到“双赢”的目的的。 这么做几乎是所有搜索系统的标配，直接将付钱的客户的页面放在置顶的位置，要脸点的系统会在对应的链接上打上“广告”的标签。不要脸的系统甚至不理会用户到底搜索的是什么~~那Elasticsearch要怎么做到这种设置呢？别慌，其实sort是支持脚本的，这样就可以任意的影响搜索结果的排序问题，建议了解一下function_score(“参考文献”中有对应的文章)。 除了影响排序权重，还有个有趣的想法就是提供一定程度的随机性，这样在海量数据的时候能给用户带来某种神秘的惊喜感。这个也是可以通过function_score提供的random_score来实现的，算是比较方便了。 除此之外，还有像是根据地理位置距离，或者特定想使用多个字段经过复杂的计算来得到得分的诉求，Elasticsearch都是支持的，查看官方文档即可。 工具是都齐活了，难点在于根据自己所在的业务场景和数据性质，组合各种搜索条件和对应的权重来最终达到一个较为理想的检索效果。而且随着数据的变化，还要不断的调整参数来持续优化结果。这是一个漫长，但有趣的旅行~ 页面上的搜索元素文章开头就提到了，我们对标的是JD的搜索功能，意味着和搜索密切相关的有几块： 上图中黄色和蓝色方块，对标filter语句条件，随着用户的选择，全部会增加到query中去。蓝色区域的可选项，可以通过第一波query的结果进行聚集得到；而黄色方块包含的参数应该是每个商品都拥有的属性，其中比较有意思的是“出版时间”，应该是搜索结果都是图书类型导致的判断。如果我们将搜索关键字换成综合性质的内容，如“龙珠”，由于结果中包含各种类型的商品，所以对应地方显示的则是更加通用的“新品”~ 红色方块对应的是检索结果，而有别于搜索结果的是绿色方块，为何这么讲呢？根据用户的输入，自动补全内容会给用户带来非常大的便利和鼓励。在这种刺激下用户输入更多内容的成本会非常低，而输入的线索越多，搜索结果就会越精准，所以这也是自动完成成为搜索必备特性的原因。 进一步思考就会发现，获取自动提示下来菜单中选项的语句和获取搜索结果的query语句应该是不同的，甚至所基于的索引和数据结构都可能是完全不同的。这又是为何呢？用户在搜索框中输入的，多数情况下都是关键词（这不是废话吗），而不是根据你的数据原始内容来输入的，这意味着要想返回足够理想的结果，需要对输入的关键词进行分词，纠错，甚至拼音转换等等。那处理后得到的token要直接和索引中的token做匹配查询？ 如果是这样，那你得到的查询结果集是一个一个具体的数据，而这些数据更适合出现在上图的红色方块中，而不是自动提示下拉菜单中，下拉菜单中的值来自哪里呢？一种说法是，这些提示内容来自于一段时间的所有用户的搜索输入值，将所有用户在搜索框中输入的关键词收集，每个词的权重就是被输入的次数，并且定期进行关联条数的统计（对应上图中黑色椭圆形区域）。但还有一个初始化的问题，毕竟刚上线的时候没有历史搜索数据给你来收集分析~~一个解决方案是，基于现有数据自身的标签来生成第一批统计数据。 Elasticsearch针对搜索框自动提示，也有专门的语法：suggester，性能更好。 不过我开始反思一个问题，假如我们的电商系统中商品很少，我们是否还需要这套完整的方案？在自动提示下拉菜单中直接针对商品数据title使用phrase_prefix，有何不可吗？我们也不需要像JD那样提醒对应的商品数量，毕竟我们的商品品类规模非常的小（差不多2K），且title相对保持很大的独特性。更多细节，可以在“参考文献”中《ELASTIC 搜索开发实战》中找到更详细的实现方法。 业务场景和数据模型前面提到的全部内容，都离不开对索引的mapping的设计，一个好的mapping有助于索引的查询性能，存储空间的优化，查询的精准，等等。我们并不需要把DB中的所有数据都存在Elasticsearch中，而是只需要将用于搜索和部分用于结果展示的字段存在Elasticsearch中一份即可。而用户点击特定结果进而查看其更详细信息时会直接从DB中获取全部信息，这样各司其职不仅有助于资源利用率，也让维护数据一致性能更容易一些，毕竟谁都不想因为一个与搜索不相关的属性值的变更就要维护多个数据源。 具体的说，就是可以像上面那个JD图一样，根据我们自身的系统的搜索页面需要展示的数据范围设计mapping，并在程序中编写当这些属性变更的时候顺带更新一下Elasticsearch中对应的文档即可，未必需要维护绝对的一致性，完全可以将更新事件丢入消息队列，异步的完成Elasticsearch的数据更新即可。 还有一个有趣的问题，我们一般都会依照业务场景建立数据模型，然后再将数据模型转换成满足关系型数据库范式要求的结构，这么一番操作后，我们的数据就会切成一块一块的，并且块之间会有不同的关系: 一对一，一对多，多对多。 理解和处理这种关系，在Elasticsearch中需要特别的留意，因为它会影响搜索结果的正确性，也会涉及到数据变更的成本。啰嗦一句，在Elasticsearch中存储和检索条件有关的字段，而不是全部。那些详情里才会展示出的内容，还是放在原处吧~~在这个前提下，如果还存在复杂的数据关系，再考虑如何优化这种关系为普通关系，实在不行了再搞嵌套类型，父子类型等等。这部分和业务场景很紧密，还是要看自己对Elasticsearch的掌握程度和对业务的理解程度了。 最后的第一步做完所有的预备工作，真的去线上环境部署，才是旅程真正的开始~~为何如此？原因很简单，数据是在不断进化的，不仅是在数量上，质量上也在不断的变化。新的流行词，同一件东西的新别名，彻底的新东西等等，这些都可以上升的文化层面。上面也有提到大体的思路，那就是不断的收集用户的行为日志，通过各种手段分析得到这种变化，再反馈到Elasticsearch的数据结构，分词算法，权重上。这是一个无尽的旅程，干什么着急结束呢？ 参考文献Elasticsearch实战 - JD购买链接（买本看看吧，不亏）Elasticsearch 索引设计实战指南ELASTIC 搜索开发实战(强烈推荐)Elasticsearch Suggester详解Elasticsearch 默认分词器和中分分词器之间的比较及使用方法修改IK分词器源码来基于mysql热更新词库同义词 - 官方文档基于mysql动态维护同义词的插件Elasticsearch Java API如何使用search template使用logstash同步MySQL数据到ES使用 Elasticsearch 做一个好用的日语搜索引擎及自动补全仿京东淘宝搜索框实战基于Elasticsearch的地理位置简单搜索Elasticsearch function_score使用排序 - 官方文档Elasticsearch 随机返回数据 API（但无法很好的结合分页）Elasticsearch 7.x Nested 嵌套类型查询父子文档join类型 - 官方文档","tags":[{"name":"自动完成","slug":"自动完成","permalink":"https://blog.kazaff.me/tags/自动完成/"},{"name":"自动提示","slug":"自动提示","permalink":"https://blog.kazaff.me/tags/自动提示/"},{"name":"拼音转换","slug":"拼音转换","permalink":"https://blog.kazaff.me/tags/拼音转换/"},{"name":"简繁互转","slug":"简繁互转","permalink":"https://blog.kazaff.me/tags/简繁互转/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://blog.kazaff.me/tags/elasticsearch/"},{"name":"分词","slug":"分词","permalink":"https://blog.kazaff.me/tags/分词/"},{"name":"模糊查询","slug":"模糊查询","permalink":"https://blog.kazaff.me/tags/模糊查询/"},{"name":"拼写纠错","slug":"拼写纠错","permalink":"https://blog.kazaff.me/tags/拼写纠错/"},{"name":"权重","slug":"权重","permalink":"https://blog.kazaff.me/tags/权重/"},{"name":"热更新","slug":"热更新","permalink":"https://blog.kazaff.me/tags/热更新/"},{"name":"词库","slug":"词库","permalink":"https://blog.kazaff.me/tags/词库/"}]},{"title":"关于积分的Db设计之二","date":"2020-04-23T10:37:12.000Z","path":"2020/04/23/关于积分的db设计之二/","text":"继续积分这个话题，我们接着之前的那篇文章往下说。上一篇给了一个db的设计方案，也简单讨论了一下它的精妙与不足。那么，是否还可以有更好一些的db设计呢？ 别着急，我们还是先来说业务。其实除了上一篇文章提到的那些规则外，我们的电商系统里，还有另外一个额外的小规则，由于比较另类，所以我放在这里才说，因为它也会对上面的那个db设计造成一定的问题。 我们的电商系统，在客户结算时并不提供一个输入框来给客户输入打算用使用的积分数量，而是只提供了一个开关选项，如果客户选择使用积分（默认），则意味着会尽可能使用积分来完成结算。这一点应该算是比较另类了，这意味着，合理的情况下会尽可能避免用户的积分到期，除非他很久才回归。也意味着客户只要选择使用积分，就可能会关联一大批的积分记录（请结合之前的points表来理解这句话）。这也是我对之前的db设计直觉上总感到不完美的切入点。 我假设的是，客户每次下单都积累积分（假设10次），之后他想要使用积分了，就可能会影响10条记录，并且在这10条记录上做写锁以及扣减逻辑，未来退款还要再逆向做一次。而且，其实每次计算客户当前可用积分，where条件也很难很好的利用索引（expire &gt; NOW很难有效的过滤掉足够多的记录）。 那么结合这些因素，我们试着改良一下前面的db设计： 12345678910111213141516171819CREATE TABLE points_status( id INT NOT NULL AUTO_INCREMENT COMMENT '主键' , user_id INT NOT NULL COMMENT '用户id' , points INT NOT NULL DEFAULT 0 COMMENT '可用积分总值' , points_status TEXT NOT NULL DEFAULT [] COMMENT '积分状态明细' , PRIMARY KEY (id))ENGINE=InnoDB CHARSET=utf8 COMMENT = '积分状态表 ';CREATE TABLE points_logs( id INT NOT NULL AUTO_INCREMENT COMMENT '主键' , user_id INT NOT NULL COMMENT '用户id' , order_id INT NOT NULL DEFAULT 0 COMMENT '订单id' , type CHAR(1) NOT NULL DEFAULT 0 COMMENT '操作类型' , points INT NOT NULL COMMENT '积分变更值' , expire INT NOT NULL DEFAULT 0 COMMENT '到期时间批次' , PRIMARY KEY (id))ENGINE=MyISAM CHARSET=utf8 COMMENT = '积分流水表 ';CREATE INDEX idx1 ON points_logs (user_id, type, expire, order_id); 这次拆分成了两张表，也比较符合习惯，一张数据聚合表（points_status），另一张数据流水表（points_logs）。 其中积分状态表中直接将当前用户可用的积分总值存入points字段，方便获取，而为了记录每一批次积分的到期时间，该表的points_status字段中存放的也是json结构的数据字符串： 1[&#123;\"expire\": \"精确到日期的时间戳\", \"val\": \"该批次的积分数值\"&#125;, ...] 这里注意一个细节，expire键中存放的是精确到日期的时间戳，这样同一天到期的多个积分会自动汇总到一起。我们姑且称为积分批次。而在表points_logs中，每一条积分的流水记录也都有相同颗粒度的到期批次字段expire，这样当出现退款时，可以从对应流水记录中的批次得到每个批次应该退还多少数值。 我们还是举个具体的例子吧~~依然假设目前就只有一个客户，他通过下单，已经挣到了200积分，那么在db中会保存对应的记录： points_status表 id user_id points points_status 1 1 200 [{“expire”:”2020-05-01”, “val”:100},{“expire”:”2020-06-01”, “val”:100}] points_logs id user_id points expire type order_id 1 1 +100 2020-05-01 earn 1 2 1 +100 2020-06-01 earn 2 每次客户挣得积分，业务代码操作时只需要锁定points_status表中对应user_id的那一条数据。至于积分流水表，以插入为主。每当业务需要更新客户的积分状态时，都可以“顺便”做一件事儿：将points_status.points_status字段中的过期值清理一下，清理出得过期积分也要插入到points_logs表中，如下面这样的记录： points_logs id user_id points expire type order_id 3 1 -100 1984-07-23 expire 注意：该记录的type为expire，我们可以暂定type字段供有三种类型值：earn(得取)，cost(消费)，expire(过期) 我们也可以在每天的凌晨执行一个定时任务，用来把系统中所有客户的过期积分都处理一遍。当然如果系统的客户数据量巨大，也可以根据客户的活跃度分批次进行处理。即便是不处理，在客户决定使用积分的时候，也优先从points_status.points_status字段中动态计算积分的有效性，听起来好像也没有彻底解决实时计算的复杂度，但至少不会造成并发锁的冲突。 而且如果考虑到定时任务，那我们则可以认为points_status.points值就是精确的。即便是前面提到的数据量大而选择分批执行，我们也可以通过增加一个字段来存储是否精确，以此来最大程度减少频繁实时计算带来的性能问题。 我们再来看看之前提到的业务指标是否得到了满足： 积分存在有效期，过期作废 用户可以查看当前可用积分总数 允许用户看到累积过期的积分总数 结算时，若用户选择使用积分，需要优先使用快要到期的积分部分 退款时，若用户之前有使用积分，需要按照积分的有效期，优先退还到期时间晚的积分部分 第1、2，4点，直接在points_status表单条记录中就可以得到答案；第3点，可以从points_logs表中按照type==expire的条件拿到总数；第5点，需要借助points_logs中type==cost &amp;&amp; order_id==退款订单id的条件得到需要退款的积分数值和对应的批次，再去points_status表中进行具体查找修改即可（若在对应的记录中已经找不到对应的积分批次了，则说明该批次积分已经过期了，此时不需要其它操作，直接将需要退还的积分直接以type=expire类型插入积分流水表即可）。 这样的设计，解决了一部分问题。可是，还有更好一些的设计方案吗？ 我相信答案一定是肯定的，请留下你的看法，谢谢~~","tags":[{"name":"积分设计","slug":"积分设计","permalink":"https://blog.kazaff.me/tags/积分设计/"},{"name":"有效期","slug":"有效期","permalink":"https://blog.kazaff.me/tags/有效期/"}]},{"title":"关于积分的Db设计之一","date":"2020-04-23T09:37:12.000Z","path":"2020/04/23/关于积分的db设计之一/","text":"今天来讲一个业务场景及其对应的解决方案：用户积分！肯定不陌生，毕竟现在很多电商系统都提供积分功能。简单说一下我手里的电商项目中积分的相关规则： 下单会根据订单金额，奖励对应比例的积分 订单结算时，用户可以使用积分来按照比例抵扣现金消费 积分存在有效期，过期作废 用户可以查看自身的积分流水 用户可以查看当前可用积分总数 极其常规的业务逻辑，对吧~~这里延伸几点规则，算是基于上述规则的合理推导： 结算时，若用户选择使用积分，需要优先使用快要到期的积分部分 退款时，若用户之前有使用积分，需要按照积分的有效期，优先退还到期时间晚的积分部分 有积分快过期时给用户足够的提醒 允许用户看到累积过期的积分总数 那么接下来，如果要你来设计满足上面这些需求点的db结构，你会怎么做呢？经过我们开发组的一轮商讨，给出了下面的一个表设计方案： 12345678910CREATE TABLE points( id INT NOT NULL AUTO_INCREMENT COMMENT '主键' , user_id INT NOT NULL COMMENT '用户id' , total_point INT NOT NULL DEFAULT 0 COMMENT '积分总值' , expire INT NOT NULL COMMENT '到期时间戳' , cost_point INT NOT NULL DEFAULT 0 COMMENT '消耗积分值' , used_detail TEXT NOT NULL DEFAULT [] COMMENT '使用占比明细' , order_id INT NOT NULL DEFAULT 0 COMMENT '订单id' , PRIMARY KEY (id)) ENGINE=InnoDB CHARSET=utf8; 单看表结构，无法直观的理解到这种设计的用意，我下面拿一个实际场景来举例吧~~ 假设目前就只有一个客户，他通过下单，已经挣到了200积分，那么在db中会保存对应的记录： id user_id total_point expire cost_point order_id used_detail 1 1 +100 2020/3/30 0 1 [] 2 1 +100 2020/4/30 0 2 [] 注意：我把expire字段写成容易阅读的时间格式了，实际存储的应该是unix时间戳 目前应该不需要额外的解释，除了cost_point，used_detail外，其它字段的含义都是很直观的。 接下来，我们的客户又要开始shopping了，不过这次他结账的时候想要使用积分来抵扣现金了，那么db中会如何记录呢？ id user_id total_point expire cost_point order_id used_detail 1 1 +100 2020/3/30 100 1 [] 2 1 +100 2020/4/30 50 2 [] 3 1 -150 0 0 3 [{id:2, val:-50},{id: 1, val:-100}] 看到了么？这个时候，used_detail里保存了这次使用积分的细节，以json结构字符串保存在表中。这个json结构也比较明了吧： id: 指向对应的积分记录 val: 标识消耗了对应积分记录中的多少积分 同时，对应积分记录的cost_point字段也会出现对应的值。 这么设计是否能很好的满足我们的业务需要呢？我们来拿前面提到的业务指标来核查一下是否都满足。其中一些简单的业务规则咱们就不讨论了，我们就拿其中比较重要的一些来检查： 积分存在有效期，过期作废 用户可以查看当前可用积分总数 允许用户看到累积过期的积分总数 结算时，若用户选择使用积分，需要优先使用快要到期的积分部分 退款时，若用户之前有使用积分，需要按照积分的有效期，优先退还到期时间晚的积分部分 第1点，靠expire字段可以很容易保证，只需要检查一下expire和当前时间对比一下，就可知道哪些积分已经过期了；第2、3、4点，靠expire，total_point和cost_point三个字段就可以实时计算出值，不过感觉不是那么直观，相当于每次都需要做实时聚合操作；第5点，可以靠used_detail字段中的明细信息，在退款时保证正确的返还合规的积分。 从上面的分析我们应该得到了初步的结论，这么设计表结构，有它精妙的地方，也有不足的地方。可以看出，我们在设计这个结构之初，过于关注退款所带来的限制条件，而忽略了一个细节：相比其它，退款发生的频率应该是最低的。这意味着，在相对频繁的场景中（如可用积分总数），我们不得不进行复杂繁琐的计算。 那么我们来试着在第4点的时候，把程序要做的步骤列一下： 开启事务 SELECT * FROM points WHERE user_id=1 AND expire &gt; NOW AND total_point != cost_point ORDER BY expire asc FOR UPDATE； 根据客户想要使用的积分数，从第一步得到的数据集合中，遵照集合中的记录顺序，完成扣减计算； 更新需要更新的记录，并提交事务。 除了步骤多外，事务中的锁也会成为一个问题（当然同一个用户并发下单的可能性有待商讨）。但不可否认的是这么设计表结构是会导致这个场景中数据的使用是比较“麻烦”的~~不知道读到这里，你是否觉得这个表结构是可以接受的呢？是否觉得使用数据时并不是很复杂呢？ 那有没有更好一些的设计方案呢？","tags":[{"name":"积分设计","slug":"积分设计","permalink":"https://blog.kazaff.me/tags/积分设计/"},{"name":"有效期","slug":"有效期","permalink":"https://blog.kazaff.me/tags/有效期/"}]},{"title":"审批流程中的一个有趣的需求点","date":"2019-12-09T09:37:12.000Z","path":"2019/12/09/审批流程中的一个有趣的需求点/","text":"这篇文章，我们来讨论一个比较有趣的需求，可能曾经出现在你设计的系统中，又或者你见过类似的解决方案。不管怎样，我们抱着思考讨论的心态，一起来聊一下吧~ 先来简单的描述一下目标功能的大概内容：我们的系统提供一个模块，该模块有3种用户类型：一线销售人员，销售经理，市场总监。该模块最基本的一个流程是：销售人员提前填写未来周的工作计划（7天），提交后先由自己部门经理审批，通过后再由市场总监审批，最终流程完结。如果在审批过程中出现工作计划变更或审批拒绝，则回到流程原点。我们这里不把关注点放在讨论该流程是否合理的方面（毕竟如果人数众多的话，市场总监怎么可能了解所有销售人员的工作明细呢？）。我们假设这是一个用于十人的小型销售团队，此外该公司业务稳定，团队成员稳定。没毛病了吧？ 由于需求过于常见，所以并没有什么值得深究的，一周开发下来就交付给客户了。这个时候，对方提出了一个“小需求”：假如销售人员的工作计划中有错别字怎么办？ 当听到这个需求，我第一感觉是无语。系统也要照顾到这类问题吗？换句话说，在一个严肃的审批流程中，如果错别字影响工作计划了，主观直接拒绝就好啦，如果有强迫症，那就改一下然后重新走审批流程不得了~ 但客户不怎么想，站在客户的角度来看，如果只是因为一个错别字，就要麻烦领导重新审批，岂不是“非常的”不方便吗！！实不相瞒，这个问题直接造成了我们团队认知上的撕裂。到底这个“错别字”问题是否应该让系统来解决呢？ 经过一轮的讨论，大家最终妥协出一个方案：增加历史记录。但在我看来这已然严重破坏了这个模块的核心功能：审批。何为增加历史记录？就需要将“是否重新审批”的权利交给销售人员，假如他选择了不重新审批，则他做的变更系统需要做历史版本的记录。而他对一周的工作计划的变更的情况又非常复杂，包含了新增，删除，编辑其中的多项。开发量和复杂度都直线上升，而且我们仅仅为的是“错别字”这个有点玩笑的需求。我个人觉得真的是不值得~ 那有没有完美的方案呢？高端一点的自然语言识别来解决错别字纠错？搜了一下果然也是各种云服务可用，不过涉及到运营成本问题了。不知道为这个“错别字”问题追加运营成本，公司领导又会有怎样的思考呢？ 我们再说回“历史记录”这个方案，深入下去的话也还是有很多的小细节需要决策，例如系统默认显示的是更新后的版本还是更新前的版本呢？显示更新后的版本，出了问题再追溯历史版本？但在我们的业务场景中，识别是否出问题的成本比较高，这也就意味着用户很容易忽略历史版本。（那不是很好吗？本来历史版本也都是错别字才对吧）但如果是恶意的更新呢？就让它悄声无息的覆盖掉原始的审批版本吗？所以我更倾向于优先显示更新前版本，然后标识出该计划有最新版本变更，用户可以手动切换到最新版本来看，负责审批的角色也可以在心情好的时候对新版本进行审批，之后系统就会显示最新审批过的那个版本。 不过不管怎么说，我们都要为“错别字”买单，而提出这个需求的用户，有思考过一个问题吗？错别字到底发生的概率和产生的后果有多大？是否值得如此厚待呢？","tags":[{"name":"审批流程","slug":"审批流程","permalink":"https://blog.kazaff.me/tags/审批流程/"}]},{"title":"Go Get 如何指定库的版本","date":"2019-12-09T09:37:12.000Z","path":"2019/12/09/go get 如何指定库的版本/","text":"最近修改了一个很早前写的程序，然后在一台新的服务器上进行发布。本以为仅仅改了个参数，直接编译就搞定的事儿，没想到尴尬了！一开始的报错，着实让人懵逼。后来翻看源码，才发现是依赖的第三方库有了不向下兼容的新版本~ 然后继续天真的以为go get可以指定版本，可再次被打脸，才不可置信的发现原来老一点版本的golang，对包版本的管理真的很渣。查了一下资料，golang的包管理相关的解决方案也五花八门，我这边急着发布呢，怎可能静下心来修行呢？！ 转念一想，go get本来就是借助git去下载源码的，我为啥不直接去替换掉下载的源码文件呢？所以你应该知道我的粗暴方案了，就是直接找到那个库的源码文件，然后去它官网的代码仓库中，找到你之前使用的那个版本，然后把代码覆盖掉你本地go get的那版，就搞定啦~ 问题解决了，是时候静下心来看看golang新版本是如何优雅解决库版本依赖的问题了：传送门","tags":[{"name":"go get","slug":"go-get","permalink":"https://blog.kazaff.me/tags/go-get/"},{"name":"版本","slug":"版本","permalink":"https://blog.kazaff.me/tags/版本/"}]},{"title":"它是如何知道请求来自于Puppeteer的","date":"2019-10-28T09:37:12.000Z","path":"2019/10/28/它是如何知道请求来自于puppeteer的/","text":"一直以来，在采集的道路上，都是puppeteer与我相伴，感觉无往不利，无坚不摧。但内心是知道总有一天，随着越来越规范，你使用puppeteer的目的会被限缩到固定范围的，毕竟它存在的意义是端到端测试。而我们拿来作为采集数据的工具来用，总不算是正路~哇哈 闲话不多说，我们快入主题，我最近发现之前一直work的采集程序最近突然失败了，然后经过一番排查，发现目标网站识别出请求是来自于非人类的，而拒绝登录了。好家伙，没想到这一天来得这么快~好歹等我交接出去啊~~😂 期初以为是对方识别user-agent来做出判断，但我设置了各种常规user-agent，并使用抓包工具确认设置成功，但依然无法突破！这就奇怪了，也确实没有什么服务器可以用来识别的请求头了啊~ 查了一圈，最后还是在puppeteer的社区求得了大神的帮忙。原来确实这种基于浏览器编程接口的模式下，浏览器会默认设置一个标识位：navigator.webdriver，如果目标系统用js脚本判断这个变量是否被设置，就可以识别出本次访问到底是人类还是程序了。而这种机制，就是我前面提到的标准。puppeteer官方也不希望这个工具未来会被滥用以至于被唾弃。 不过进一步查了一下，navigator.webdriver这个标识位不仅仅用于puppeteer，其它界面测试的套件也都会设置这个标识位。下面说一下暂时如何突破这一点的： 1const browser = await puppeteer.launch(&#123;headless: false, ignoreDefaultArgs: [\"--enable-automation\"],&#125;); 目前，我们可以在启动puppeteer的时候，忽略--enable-automation这个设置来避免navigator.webdriver标识位被初始化。但我觉得未来可能就不会再有效了~ 除非，我们基于chrome源码自己开发一个浏览器来做我们想做的事儿~~ 参考文献一行js代码识别Selenium+Webdriver及其应对方案puppeteer slack","tags":[{"name":"puppeteer","slug":"puppeteer","permalink":"https://blog.kazaff.me/tags/puppeteer/"}]},{"title":"基于AES和RSA来保护数据","date":"2019-08-23T09:37:12.000Z","path":"2019/08/23/基于AES和RSA来保护数据/","text":"今天来说一下加解密相关的内容~ 在这个后Rest时代，基本上HTTP API已经满大街了，如果你还不了解这个细节，或者你的项目不提供Rest API，你都不好意思来看我这篇文章~~而通常我们简单的这些API，都不约儿童的包含了一个签名字段，主要目的是为了避免通信数据被恶意篡改，这也是https生而伟大的原因。 我们来进一步说一下这个签名逻辑：一般通信双方会提前约定好使用相同的一段字符串来作为签名的密钥，然后约定好使用的签名算法（例如md5），然后双方在发送数据时，会把数据按照约定好的顺序拼接成一个字符串，然后再加上密钥，最后做个MD5就生成了一个签名字符串。 因为其他人不知道密钥，所以尽管他们能看到明文的通信数据，但他们无法生成相同的签名。同样，他们可以修改明文的数据，但也无法为修改后的数据生成一个合法的签名。这样，只要接受数据的一方用接受到的明文数据+密钥做一次签名，然后和接受到的签名一比对，就知道数据是不是被篡改过了。 这并没有什么高深的思想，不过我们要注意一个细节：签名算法。一个比较好的签名算法，要求对碰撞几率足够低（即不同的字符串生成相同的签名值），更要求签名不可逆性，最后也要保证签名性价比高（不能太慢，不能太耗资源）。 而签名的另外一个需要注意的细节是：传输过程中数据是明文的。为何必须要包含明文呢？ 那么，如果传输的数据比较敏感呢？例如银行密码。那么只是签名肯定就是不够的了。这个时候就要对数据进行加密了。常见的加密类型分为：对称加密，和非对称加密。对称加密比较好理解，通信双方约定好使用相同的密钥，然后在传递数据之前，对所有数据进行加密即可。这样，只要密钥是安全的，中间人即便是抓包看到通信携带的数据，也因为无法解密而不能识别数据的含义。 听起来，岂不是不管三七二一，加就对了。那为何支付宝啊，微信啊，它们的接口都使用签名方式呢？这个问题留给大家自己琢磨吧，我们今儿的主题不是它~ 下面继续说一下非对称加密，这就有点意思了。一言以蔽之就是，加密和解密的时候需要使用不同的密钥来完成。神奇吧，什么？不知道这用来干啥么？举个例子，假如请求方的密钥泄露了，是不是知道这个密钥的人都可以解密数据了？而非对称加密就能解决这个问题，牛逼吧！ 那么，听起来，岂不是不管三七二一，非对称加就对了？那为何。。。（哎呀，别打我~）其实冷静后不难得出，它们各自有各自的价值。 需要注意的是，非对称加密最安全，但性能越最低，然后是对称加密，然后是签名。所以如果用在接口上，考虑到并发量，还是需要根据具体场景具体分析的。 另外，如果你最终就是选择使用非对称加密来保护你的数据，你还要注意一点是，非对称加密对被加密的数据容量有限制，所以并不是想用就用的。不过也别灰心，我们可以将它们结合在一起来使用： 使用创建的随机的AES对称加密算法使用的key对目标字符串进行对称加密 使用私钥对创建的随机key进行非对称加密 这样就可以避免数据量的问题。 不过我实际测试了一下，性能确实不理想啊。。。如果你对数据的明文传输并不介意，但想要一个非对称的签名，那就既能保证足够的安全，性能也可以照顾到了。幸运的是，RSA已经提供了这种非对称签名的实现，对这篇文章提到的内容有实战需求的朋友，可以看一下这个库： https://github.com/kazaff/CipherUtils 参考文献https://niels.nu/blog/2016/java-rsa.htmlhttps://www.novixys.com/blog/using-aes-rsa-file-encryption-decryption-java/https://blog.csdn.net/qq_32523587/article/details/79146977","tags":[{"name":"对称加密","slug":"对称加密","permalink":"https://blog.kazaff.me/tags/对称加密/"},{"name":"非对称加密","slug":"非对称加密","permalink":"https://blog.kazaff.me/tags/非对称加密/"},{"name":"非对称签名","slug":"非对称签名","permalink":"https://blog.kazaff.me/tags/非对称签名/"}]},{"title":"谈Puppeteer碰到的bug","date":"2019-08-22T09:37:12.000Z","path":"2019/08/22/谈puppeteer碰到的bug/","text":"什么是puppeteer？直观点说，就是一个提供以编程的方式控制浏览器（chromium）的nodejs库，非常的彪悍。有了它，你可以用来做端到端测试，也可用来采集一些比较复杂的网站。我之前文章有涉及到这个神器，所以就不再啰嗦了~ 这篇文章只是记录在使用中几个碰到的坑~ page.click 无限阻塞从官网手册上看，click函数在发现页面上没有对应的元素的时候，会报错。但我在实际使用的时候发现，puppeteer 1.12.x版本下程序可能会无限阻塞在这个函数上，不抛异常，也不会返回。而且发生这种情况的时候，页面上确实是有对应的元素的，手动点击该按钮也是可以正常触发页面响应的。 此外，注意到一点是，当出现这个情况的时候，我切换成下面的写法: 123await page.evaluate(() =&gt; &#123; jQuery('button').click();&#125;); 程序会直接报错，提示 执行期间，上下文被销毁 的异常信息。正常情况下，该方式和直接page.click都应该是可以触发按钮点击的，现在突然都失灵了~更增加排查难度的是，在循环执行时，一开始是成功的，平均到第三次时，发生这个bug的可能性就非常大。同样的代码，使用puppeteer 1.19.x则完全正常！ 和同事讨论了一下，他之前也碰到过这个问题，后来在网站找到另外一个解决方案： 123456// 直接click方法，会导致莫名其妙的阻塞在click调用上let submitBtn = await page.$('button');let submitBox = await submitBtn.boundingBox();let boxX = submitBox.x + (submitBox.width / 2);let boxY = submitBox.y + (submitBox.height / 2);await page.mouse.click(boxX, boxY, &#123;delay: 0&#125;); 这段代码就完全可以解决问题，它使用了更底层的一套接口，直接操作鼠标。。。不过实际使用的时候需要注意，该方式必须要将headless设置成false。 所以，目前的结论是，如果你不想升级版本的话，就是用鼠标接口，否则可以升级sdk版本来解决。 page.click 点击无效依然是和 page.click 函数相关，在我们项目的某个页面（日历插件），发现puppeteer 1.19.x下，点击后无法影响该输入项的当前值。但降低版本到puppeteer 1.18.x后，问题就解决了。 目前怀疑是最新版本的puppeteer可能存在bug（我们测试的时候，1.19.0才刚更新25天）。 以后有新的坑，再更新这个篇文章吧~祝大家顺利~","tags":[{"name":"puppeteer","slug":"puppeteer","permalink":"https://blog.kazaff.me/tags/puppeteer/"}]},{"title":"不安全证书提示下的请求是否依然基于Tls","date":"2019-07-23T09:37:12.000Z","path":"2019/07/23/不安全证书提示下的请求是否依然基于tls/","text":"好久没有更新Blog了，惭愧惭愧~ 今天咱们来聊一个之前总是忽略的问题： 我记得是好几年前，当时https风初见势头，很多大一些的互联网平台都https化的时候，我也跟风的研究并搭建过基于https的网站。不过那个时候证书还是需要花钱买的，所以跟着网上的教材我只能在本地搭建测试环境~ 不过这几年随着https证书贫民化，几乎所有网站都切换到了https下，什么？你还不知道如何免费https化？强力推荐使用Certbot。 有点跑题了，拉回来拉回来。假如我们使用自签名的证书，用浏览器打开的时候就会碰到上面截图的提示。这其实是浏览器的保护措施，不过它的提示感觉有点“过度”保护了~之前没有深究过，突然想知道，在浏览器提示不安全的私密连接的基础下执意访问呢？是否会退化到http？ 不光是自签名证书会碰到这个问题，只要是浏览器验证证书有效性的时候发现问题都会提示不安全，例如证书过期，域名不匹配等。 所以我本地搭建了个测试环境，然后利用抓包工具进行了分析，得到了最终的结论：不安全的私密连接，依然是基于https的，只是使用的证书由于没有通过验证，所以很可能是中间人的恶意证书。 本文章不会展开SSL和TLS的历史，也不会科普TLS握手流程，这些都可以在文章底部的参考链接中的文章里找到答案。 得到这个结论并非没有任何意义，假如我们使用curl命令或某个语言的http库，请求一个rest服务或基于http的api接口，在服务使用自签名证书的前提下，本次调用到底是否使用证书呢？ 12345678910$ curl https://192.168.1.142 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (60) SSL certificate problem: self signed certificateMore details here: https://curl.haxx.se/docs/sslcerts.htmlcurl failed to verify the legitimacy of the server and therefore could notestablish a secure connection to it. To learn more about this situation andhow to fix it, please visit the web page mentioned above. 可以看到，curl默认由于证书验证问题会最终放弃请求。所以我们必须明确告知curl忽略证书问题，可以使用这个参数-k。 那么如果是某种编程语言呢？我们来看几种语言的http库是什么情况： php12curl_setopt($cHandler, CURLOPT_SSL_VERIFYHOST, false);curl_setopt($cHandler, CURLOPT_SSL_VERIFYPEER, false); golang1234transCfg := &amp;http.Transport&#123; TLSClientConfig: &amp;tls.Config&#123;InsecureSkipVerify: true&#125;, // ignore expired SSL certificates&#125;client := &amp;http.Client&#123;Transport: transCfg&#125; nodejs1process.env[&quot;NODE_TLS_REJECT_UNAUTHORIZED&quot;] = 0; java123456789101112131415161718192021222324try &#123; TrustManager[] trustAllCerts = new TrustManager[] &#123; new X509TrustManager() &#123; public java.security.cert.X509Certificate[] getAcceptedIssuers() &#123; return null; &#125; public void checkClientTrusted(X509Certificate[] certs, String authType) &#123; &#125; public void checkServerTrusted(X509Certificate[] certs, String authType) &#123; &#125; &#125; &#125;; SSLContext sc = SSLContext.getInstance(\"SSL\"); sc.init(null, trustAllCerts, new SecureRandom()); CloseableHttpClient httpClient = HttpClients.custom().setSSLHostnameVerifier(NoopHostnameVerifier.INSTANCE).setSslcontext(sc).build(); String output = Executor.newInstance(httpClient).execute(Request.Get(\"https://127.0.0.1:3000/something\") .connectTimeout(1000) .socketTimeout(1000)).returnContent().asString(); &#125; catch (Exception e) &#123; &#125; # source: http://stackoverflow.com/questions/2703161/how-to-ignore-ssl-certificate-errors-in-apache-httpclient-4-0 目前，小弟我只实际抓包过curl -k的流程，稍后我会尝试抓一下golang的请求看看是否符合预期。 抓包工具如果你只是在浏览器端或postman测试，使用fiddler这款工具即可，简单够用~ 但我没有办法使用它抓到curl的包，所以只能拿神器wireshark来解决问题了。 不过友情提示，一定要去官方下载wireshark最新版（3.03+），安装的时候选择支持监听回环网卡（npcap loopback adapter），否则是无法抓住你本机回环的请求的。 更有意思的是，wireshark把我本机的内网ip（192.168.1.142）也当做回环地址来处理了，我本以为只要我不是用localhost或127.0.0.1，就应该可以监听到数据包的，结果花了好多时间才。。不说了，都是泪。 查看系统支持的根证书windows系统下，可以开始运行中输入certmgr.msc，打开的面板里查找Trusted Root Certification Authorities项就可以看到系统中已经包含的信任根证书列表了。 linux系统下，则可以查看/etc/pki/tls/certs/ca-bundle.crt文件，其中包含了已经信任的根证书。 参考链接 SSL/TLS协议运行机制的概述 TLS整理（下）：TLS如何保证安全 TLS 详解 如何用 wireshark 抓包 TLS 封包 细说 CA 和证书 HTTPS加密过程和TLS证书验证","tags":[{"name":"证书","slug":"证书","permalink":"https://blog.kazaff.me/tags/证书/"},{"name":"TLS","slug":"TLS","permalink":"https://blog.kazaff.me/tags/TLS/"},{"name":"HTTPS","slug":"HTTPS","permalink":"https://blog.kazaff.me/tags/HTTPS/"}]},{"title":"从华为事件谈自己造轮子的必要性","date":"2019-05-21T09:37:12.000Z","path":"2019/05/21/从华为事件谈自己造轮子的必要性/","text":"贸易战到现在，愈演愈烈，自从川普老头发布华为禁令后，先后很多美国本土企业都中断了和华为的合作关系，尤其是google。安卓系统对于华为的海外手机业务来说无疑是非常重要的，而华为内部发布出来的信息中有一句话让我这种程序员很有感觉： 所有“备胎”一夜之间全部转正！ 以前看过很多资料，总会不经意看到关于“重造轮子”的梗，多数都是用来表达“闭门造车”的愚蠢行为。相对比较正面的一句话应该是来自图灵书籍的封面： 站在巨人的肩膀上 看来，有时候巨人脾气也不咋地，不是你随随便便想站就站的啊~ 话说回来，今几年国内很多技术出众的大公司都在开源的项目上深耕已久，各种升级版层出不穷（我就不举例了），其实早就说明一个问题：我们是有能力也是有需要自造轮子的！当然，重造轮子不只是完全克隆，而且有所取舍的剪裁，有所目的的增强。所以，伙计们，撸起袖子造轮子吧~ 从这个事儿上，确实十分佩服华为的远见和魄力，魄力在于一个民营企业与一个世界第一强的国家对峙，何等的气概啊，想一下都热血沸腾！远见在于未雨绸缪，投入巨资自研“备胎”，从硬件到软件，足以看出华为是一家对“独立自由”理解是多么深刻的公司！ 作为一个小老百姓，在如今风云莫测的世界局势下，只有吃瓜的份儿？不，至少我会从自己下一部手机开始做华为的粉丝，给予其足够的耐心和热情，相信这样的企业总有天会改变世界~","tags":[{"name":"需求分析","slug":"需求分析","permalink":"https://blog.kazaff.me/tags/需求分析/"}]},{"title":"激活Apple ID的two-Factor认证","date":"2019-05-10T09:37:12.000Z","path":"2019/05/10/激活apple id的two-factor认证/","text":"事情是这样的，我司的一个ERP有一个ios app，因为当时没有通过Apple的个人开发审核，所以按照Apple的推荐选择使用企业账号类型来开发。企业app的证书有效期是1年，也就是说每年都要去更新证书。 问题来了，当我这几天登录我们的企业开发者账号时，被拦截了，提醒我该账号没有开启双重认证。。。这也太霸道了啊~这个开发者账号是属于公司的，不可能绑定在私人的手机上，怎么设置这个双重认证呢？ GG上查了很多文章，都不是十分符合我们的场景，而且大量的流程都已经过期了，Apple官网UI都已经变了~还好在Apple的官网找到了相关的描述： 需要先添加一个把开发者账号appl id添加到手机中：设置 -&gt; 密码和账号 -&gt; 添加账号，这可能需要先开通开发者账号的store功能？我不确定~因为我一开始以为需要用账号登录app store，所以就顺便开通了； 退出你手机中当前的iCloud账号，然后用开发者账号再次登录：吐槽一下，官网并没有描述正确如何退出，你只需要： 设置 -&gt; 账号（第一行） -&gt; 退出（滑动到最下面就看到了）-&gt; 退出iCloud 即可； 开启双重认证： 设置 -&gt; 账号（第一行） -&gt; 密码与安全 -&gt; 双重认证 开启即可。 退出我自己的iCloud账号后，导致我很多依赖iCloud存储的数据全部都没了（例如备忘录）~大家注意备份啊~苹果这么强制开通双重认证，并且又那么麻烦，真的很不人性化啊。为啥不基于google的双重认证服务啊。。。","tags":[{"name":"需求分析","slug":"需求分析","permalink":"https://blog.kazaff.me/tags/需求分析/"}]},{"title":"事无巨细Vs点到为止","date":"2019-04-29T09:37:12.000Z","path":"2019/04/29/事无巨细vs点到为止/","text":"最近在思考一个问题，在项目需求分析阶段，到底是应该“事无巨细” 还是 “点到为止” 呢？ 这个问题源自我最近参与公司其它项目的经历，作为一个“外来者”，按照他们组的规则按部就班的推进需求分析的进度。我发现的第一个与众不同是他们组开会特别频繁，而我们组正式会议几个月都很难一次，当然我们组存在频繁的小会议（问题参与者们自发组织的临时会谈）。 其实，会议频繁不算是问题，但每个会议必须有效率。我参与的这个组的会议中，经常出现参会者完全没有主题要表达，或者为了找一个文档花费好几分钟。这些是缺乏准备造成的（也可能是会议实在太多，大家都不以为然了~），也和组织会议的负责人在之前没有描述清楚会议主题有关系。这些都可以很轻易的解决~ 最大的问题是，会议中沟通时，大家多同一个需求，存在多套叙述语言。但这可是一个运行很久的项目了，竟然还没有“默契的”形成统一的沟通用语。这很难解决，在我们组，在新需求初期，我都会在讨论的初期阶段，时刻关注着大家的沟通用语，一旦发现有“恰到好处”的代名词，我都会反复强调它，并明示所有组员，该需求点以后只允许使用该代名词来表达，不管是在会议中还是在文档里。 最后绕回到我们的本文的主题：到底应该讨论到哪个程度才算合理？ 说说我个人的看法吧，我是个比较粗颗粒的人，往往比较在意全局层面的相关问题。前几年“全栈工程师”的概念比较火，我一度以为我之所以有这种思维方式，是因为前后端我都多多少少有所涉猎。但后来我发现也不完全因为如此，尽管全栈会让我的思考可能更全面，但在何时何地把握何种层面的思考，是需要刻意训练的。否则你知道的越多，你就越容易陷入“分析瘫痪”中。 另外一方面，团队会议中必须有人时刻保持对全局问题的敏感性，在负责各自领域的讨论者提出一个方案时，总应该有个人用全局的眼光来进行校验。这个听起来很自然~ 不过人往往都很难面面俱到，不是么？我在做这个角色的时候，就无法同时兼顾细节。例如我不会允许自己过分思考具体方案的细节问题，要尽量的站在方案之外，观察它是如何与其它部分协作的，是否会造成其它部分的困扰和麻烦。 所以我们组讨论的时候，我总是提一些很“笼统”的建议。笼统但绝不模糊，我会反复强调需求中包含的约束，也会提及方案应该避免的一些不好的方面。我会留一个大大的问号给这部分的实际负责人，但我同时会给他定好一个问题边界，我们在会议上多半就只能得到这种“点到为止”的结论。 我有思考过我这种“偷懒”到底源于什么？首先和我的性格有关系，还有就是我发现，开发人员多数（并非全部）都喜欢接受挑战，但这些挑战必须和技术密切相关。这个时候如果开发中所有的枝枝叶叶都在会议中确定了，这个活儿就失去了魅力~ 我认为总是应该留有一些空间给开发人员，再给他增加一些期许的目光和鼓励的对白，一般稍微有上进心的人都会给你交付一份完美的答案。 回到我提到的这个项目上，在会议中我看到项目经理会追着细节不放，不停的连续追问，而开发人员难以掩饰的不耐烦，加上笨拙的沟通技巧，原本十几行代码做的工作需要讲很久很久。。。 那这表明过度关注细节就一定不好么？这也是困扰我的原因，如果一种方法绝对的好，那任何关于它的讨论都显得多余不是么？ 关注细节的优势在于，对组内新人更友好（另一个问题是真的应该在项目中间加人么？），对项目更保险。最终交付的结果更容易把控。但这一切都是可以通过code review来弥补的。这好像也是技术管理离不开开发能力的本质原因？ 说了这么多，依然都是我的个人观点，我希望得到前辈们的指点，请留下宝贵的建议吧~","tags":[{"name":"需求分析","slug":"需求分析","permalink":"https://blog.kazaff.me/tags/需求分析/"}]},{"title":"你想真正掌握你的个人数据吗","date":"2019-03-20T09:37:12.000Z","path":"2019/03/20/你想真正掌握你的个人数据吗/","text":"时代在召唤，我们所生活在的星球，每隔一段时间都会自我颠覆一次，就像细胞的迭代一样。在前段时间各种新闻大事件（fb的泄露门，欧盟的隐私法案）的炒作下，越来越多的互联网问题暴露出来。不过，这个领域从来不缺少革命者，一些大师总是会在关键时刻站出来发表一个颠覆性的思想，引领世人。 之前，在了解区块链的时候，无意间有看到一个项目IPFS，去中心化的web应用多么有魅力啊。不过今天看到一个更热血沸腾的项目SoLiD，这种概念真的令人佩服啊~ 按照SoLiD的描述，我们的个人数据完全掌控在我们手中，而不是各大平台，听起来就爽的飞起。再也不需要为数据迁移发愁了，再也不需要受制于某个平台了！ 不过，作为开发人员，会格外好奇，如何设计一种数据结构来适配各种平台呢？如何在个性和共性之间找到平衡呢？另外，个人数据存储在哪个位置呢？如果还是托管在云端，似乎本质上还是被某个服务所“绑架”，若能存储在个人手持设备上，那岂不是美哉？ 喜欢折腾的小伙伴，学起来吧~","tags":[{"name":"solid","slug":"solid","permalink":"https://blog.kazaff.me/tags/solid/"}]},{"title":"Https证书申请","date":"2019-03-19T09:37:12.000Z","path":"2019/03/19/https证书申请/","text":"时间飞逝，现在https已经是web项目必选项了。早在很久之前，就有过一篇文章提https，里面提供的证书申请方案已经过时。 其实，一直以来都有机构或组织提供openssl证书服务，最接地气的就应该是Let’s Encrypt。去年年初，我们公司搭建环境的时候，就是用的是它提供的ACME.sh，可以看到它还是比较远古的，很多步骤，而且我们公司后来出现过它证书没有自动创建的问题~~ 在一个夜黑风高的晚上，马上钻被窝的我，被领导电话夺命追魂，因为公司官网无法访问了，吓得我瞬间睡意全无。打开电脑才发现是证书过期了~~ 慌忙中我怎么有耐心执行完acme.sh的所有步骤~~不能够的！google一查，原来已经有更智（lan）能（duo）的方案了：certbot 有了它，我们只需要两步即可完成证书替换： 1234$ wget https://dl.eff.org/certbot-auto$ chmod a+x certbot-auto$ sudo /path/to/certbot-auto --nginx 然后顺着导航提示回车即可。是不是很爽？ 睡觉~","tags":[{"name":"https","slug":"https","permalink":"https://blog.kazaff.me/tags/https/"},{"name":"openssl","slug":"openssl","permalink":"https://blog.kazaff.me/tags/openssl/"},{"name":"ACME","slug":"ACME","permalink":"https://blog.kazaff.me/tags/ACME/"},{"name":"certbot","slug":"certbot","permalink":"https://blog.kazaff.me/tags/certbot/"}]},{"title":"关于Mysql的insert for Update在多条插入记录下的使用","date":"2019-03-18T09:37:12.000Z","path":"2019/03/18/关于mysql的insert for update在多条插入记录下的使用/","text":"今天我们来讨论一个比较大众的问题：关于当在mysql中insert记录时违反了表的唯一键后要怎么办？其实一般而言，越来越多的系统在数据一致性和有效性问题上，都放在应用逻辑层来维护合法性。到底哪种方式好，在这篇文章中不做评论，我个人觉得尽量都在代码中体现才是王道。 回到问题，一般遇到这种情况，第一反应是使用insert for update语法，例如： 12INSERT INTO t1 (a,b,c) VALUES (1,2,3) ON DUPLICATE KEY UPDATE c=c+1; 这句sql很直观，基本上算是语义化了。但如果是批量insert呢，我们希望如果遇到冲突，update块使用对应插入记录的值来更新，而不是固定值或db中当前值，该怎么办呢？ 12INSERT INTO t1 (a,b,c) VALUES (1,2,3),(4,5,6) ON DUPLICATE KEY UPDATE c=VALUES(c), b=VALUES(b); 对，没错，mysql提供的内置函数VALUES()就是针对这种需求的。 参考文献https://dev.mysql.com/doc/refman/8.0/en/insert-on-duplicate.html","tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.kazaff.me/tags/mysql/"},{"name":"insert for update","slug":"insert-for-update","permalink":"https://blog.kazaff.me/tags/insert-for-update/"}]},{"title":"关于电商项目的需求难点","date":"2019-03-07T09:37:12.000Z","path":"2019/03/07/关于电商项目的需求难点/","text":"现在国内的电商产品真的是多如牛毛，功能其实都大同小异，质量参差不齐。有的当软件产品卖，有的依托在完整的SaaS平台中卖服务，百家齐放。 我司本身有运营着一个B2C的电商系统，基于国外一个开源电商项目二次开发而来，修修改改也使用了将近三年了，今年公司打算对其进行重构（基本上是推倒重来），并进一步将其产品化。之所以有这种想法，也是因为我们调研了一圈市面上的产品后，觉得这个市场还是有机会的。 我之前少有接触过此类项目，现在的电商系统早不再是简单的产品展示+在线支付的粗犷派，其中充斥着各种营销各种活动，应该说是及其复杂。本文简单的罗列出这段时间来我们团队梳理出的一些需求难点，供大家思考和讨论。 订单编辑订单应该是电商系统中的核心业务，几乎所有角色都会“与其有染”。教科书式的订单的简单模型是： 下单用户 购买的商品 支付方式 物流方式及收货地址 订单状态（待支付，已支付，已发货，已完成，已取消） 通常会包含这些基本的属性。但真实世界会更加黑暗和复杂，比方说下面这些属性： 使用的积分数 使用的优惠券 物流拆分（如常温与冷冻） 这些都会导致订单价格计算的时候引入复杂逻辑。当然，模型一旦确定，搞就完事儿了，不服就干嘛~ 实际开发中，我们得到的宝贵结论（教训）是：订单编辑才是万恶之首。我们参考了一些电商项目，它们提供订单编辑的操作往往都是在订单未付款的状态下。一旦用户付款就锁定订单不允许编辑了。 但其实，即便是未付款状态，订单编辑依然是很复杂的，考虑的情况非常的繁杂，下面我举一些例子： 编辑后优惠券条件不满足 编辑后影响积分使用条件 物流成本和方式变更 此外，还存在订单编辑带来的安全问题，账目核对问题等。尤其是在用户已经支付后再编辑订单的时候，一切就更加的邪恶。参考主流的电商平台，一般也都是让用户取消当前订单并再次下单。 其实上述问题不光涉及到开发复杂性，商户在使用的时候也很难完全理解每一种情况。而下单用户呢？多数下单用户发现订单被编辑了，我相信一定还是会感到不安，即便是已经线下和商户协商好了，这种协商往往很灵活（各种突发奇想的操作），或者很“死板”（直接退单再下）。 针对已付款的订单进行编辑，从金额角度会有三种情况：价格不变，变低，变高。前两种好办，可以不麻烦下单用户就完成，而价格变高就必须让用户补差价了，之前很多电商平台中的商户都会创建一个虚拟商品专门用来补差价。 总结来说就是，由于导致订单编辑的原因和编辑波及的方面太多，无法轻易收敛，所以目前建议放弃模式匹配的思维。可能提供“松散”的策略会更易于开发和使用： 可以在此思路上继续往走下去。 营销活动只有你想不到，没有做不到的营销，水深的很。我们看一下有赞电商后台提供的营销类型： 简直是眼花缭乱，试问谁能抵抗这么多类型的诱惑完成荷包保卫战？你们先聊，我去下单了~ 这些营销活动，最终都会直接影响购物流程，甚至改变购物流程。要想让系统能够轻松扩展出各种影响活动，对系统的扩展性就要提出很高的要求才行。 这些活动中，有些是独立入口，有些则辅佐在主流程的某些步骤中，有些只是逻辑扩展，有些会涉及到页面展示。这就要求系统在页面渲染和计算流程中处处提供扩展点。这些扩展点有些是同步执行，有些可以异步调用。针对各种场景，我们可以在扩展点处提供Hook，或事件消息机制，尽可能让这些营销活动模块化插件化。 另外一方面，商户新建一个营销活动，除了规则和玩法以外，还需要设置该活动的范围，这里所说的范围，分为： 商品维度范围 用户维度范围 时间维度范围 各种维度的交集才是有效范围。在电商系统中展示商品时，要动态计算出该商品所处于哪个活动范围也是一个不小的问题。解决这类问题，需要前后端配合，单一方的优化总感觉不够完美。 例如，我们系统的首页，加载商品数据后，异步加载活动规则，前端计算出商品和活动的匹配关系。又或者在活动创建时强绑定和商品的关系，在商品CURD时维护这种关系。 在时间维度方面，可以借助定时任务触发活动上线和下线时机。具体怎么做，还是根据具体情况来决策。 最后想说的是，涉及到抢购的那种活动比较危险，有条件的话应该隔离部署这些活动，并做抢购相关的优化。关于抢购可以在GG上搜到大量的技术方案分享。 页面布局自定义回顾我个人的上网经验，最早看到这种功能还是在QQ空间，后来很多不同类型的网站也都提供此类功能，主要是因为我们人类关于个性的追求。在电商系统中，常见的页面布局主要是首页，首页又分成pc端和mobile端。这种自定义，可简单可复杂，开发成本较高。我不确定业界都是怎么实现的，个人感觉借助现代前端框架组件化方案，再设计一种简单的DSL来序列化存储用户的配置，应该是一个完整的解决方案。 总之，直觉告诉我，这一块感觉坑不浅。 SEO之前看过一篇文章，好像是18年年底的，主要是讲google目前的爬虫是否能识别js代码。结论是google对外宣称这一点的时候，用了“maybe”，实际测试虽然比较理想，但官网却不提供任何保证。更何况其它搜索引擎呢？所以就目前来看，SEO还是需要考虑的！ 所以，在选择前端技术的时候，要多考虑SSR（Server-side Render），一般主流框架都有完善的SSR方案。","tags":[{"name":"电商","slug":"电商","permalink":"https://blog.kazaff.me/tags/电商/"}]},{"title":"Yum Remove害死人","date":"2018-12-20T09:37:12.000Z","path":"2018/12/20/yum remove害死人/","text":"前几天犯了个天大的错，简单的说就是我把公司线上服务器的nginx卸载了。。。然后整整花了将近7个小时才让公司的系统重新上线！我想如果在BAT，我肯定要滚蛋了~~ 事情的缘由比较啰嗦就不细说了，主要说一下我是如何干掉nginx的吧，哈哈哈： 1yum remove GeoIP -y 我本来是打算删除这个库的，执行完命令后它竟然把依赖它的其它软件也给删除了。。。。当然，重新安装nginx分分钟的事儿，之所以停机7小时是因为它还删除了一些网络相关的库，导致我们的aws ec2的节点无法ssh。。。 不过重点是，yum remove会删除依赖的库，这就有点尴尬了，搜了一下，网上推荐使用rpm -e --nodeps xxx这种删除软件的命令。你是不是也是第一次听说这个问题呢？想想我以前干的事儿，真的有点小激动呢。。。。","tags":[{"name":"centos","slug":"centos","permalink":"https://blog.kazaff.me/tags/centos/"},{"name":"yum","slug":"yum","permalink":"https://blog.kazaff.me/tags/yum/"},{"name":"rpm","slug":"rpm","permalink":"https://blog.kazaff.me/tags/rpm/"}]},{"title":"你Php的session_start慢么","date":"2018-12-18T09:37:12.000Z","path":"2018/12/18/你php的session_start慢么/","text":"事情发生在一个晴朗的早上，客户投诉说公司运营的一个web电商系统出现偶发的504问题。“偶发”这种事儿，真心不好排查啊。我们检查了一遍所有的相关配置，包括nginx，php，php-fpm，mysql等等等等，调整了一些参数后情况并没有变好~~不仅如此，还有相当多的其它因素干扰着问题的排查，这其中就包含今天的这个主题。 实在没辙了，只能开启所有的log来追查了。 经过对日志的分析，发现“偶发”的504都会伴随着几次php的slow log记录，其中包含了两个比较惹人注目的函数调用：curl和session_start。前者很好理解，肯定是curl的目标地址出现了响应问题，经过确认也证实了如此，原来是之前依赖的一个第三方接口停止服务了。而之所以“偶尔”，是因为这个服务有时候响应null，有时候直接阻塞到超时。。。 这不是今天的重点，我们来看session_start慢的问题。在用户浏览器上表现的行为是，当出现了某个页面504，接下来的一个时间段内，该浏览器访问同一个二级域名下的所有页面都会返回504，但立即切换浏览器或更换二级域名即可立即正常访问。 同事提出一种看法：所有的同一个会话的请求都会最终交给同一个php-fpm的worker进程来处理，所以会出现这种现象。这和我的认知有点不同，毕竟要想实现session粘度处理，一般往往是要在整个请求链路的各个节点都要做设置的，不可能默认就提供这样的“高级功能”才对。可问题表现出来的情况恰恰又和同事的看法吻合，一时间很苦恼。 那就按照这个观点，我们调整了php-fpm的一个参数：request_terminate_timeout。将它开启并设置一个较短的等待时间，确实发现上面提到的那个浏览器的504等待时间缩短了，这仿佛又一次验证了同事的观点。 一直到排查的尾声，我也一直没有特别接受这个观点，直到我们看到了session_start的慢日志。第一感觉是，一个内置函数怎么可能会慢呢？GG了一下，发现大量的相关文章，原来是文件锁导致的。简单的说，php默认的session机制是靠文件，浏览器携带的cookie中指明的session_id到服务端后，我们的php脚本会在入口统一调用session_start来开启会话，假如一个请求开启session后卡住了，后面的同一个会话的请求就会在session_start调用上等待，直到第一个请求释放文件锁。对应我们的问题场景就是处理第一个请求的worker进程被kill。 这对这种问题，大家给出的方案是，依靠php提供的session handler机制把session存储在memcache或db里。或者使用session_write_close()方法来释放写锁，在session_start后直接调用该方法，可以化解锁冲突，而且你依然可以继续读取session中的值，只是无法编辑，若想编辑，再次调用@session_start()函数即可。 长远来看，还是推荐大家讲session存储在独立的节点上，方便日后的集群扩展。 今天的分享就到这里~","tags":[{"name":"session","slug":"session","permalink":"https://blog.kazaff.me/tags/session/"},{"name":"文件锁","slug":"文件锁","permalink":"https://blog.kazaff.me/tags/文件锁/"}]},{"title":"迟来的成人礼","date":"2018-11-29T09:37:12.000Z","path":"2018/11/29/迟来的成人礼/","text":"刚刚过去的两周，对我来讲，如时间旅行一样，众多的“人生第一次”像赶集一样接踵而来。庆幸的是我及我的家人犹如神助，正如几个月前我从朋友口中得到的那句启示“一切都是最好的安排”！ 此刻花时间来整理自己的思绪，那些镜头还会在眼前复现，上次如此深刻的画面还是在姥姥去世的时候。不同的是这次是多了一个新的生命，一个我和我爱的人的生命的延续。 一直以来我都没有感觉生活马上就要有所变化，直到我在产房外听到声嘶力竭的叫喊声时，心突然悸动。不知道因为担心失去还是害怕得到，那一刻懂得了“坐立难安”的意思，不去在乎别人的目光，不停的踱步。 看到爱人和孩子的那一刻，就好像喝了一口冰镇甘甜的泉水，滋润了焦燥的心田，安抚了慌乱的思想。也是那一刻，知道自己突然长大成人了，因为世界上多了一个生命需要依靠我，需要依靠我们。 这种心理现象应该深刻在人类基因里吧？不然为何一代人总会哺育下一代？面对孩子，“牺牲自我”根本不需要一秒钟的思想准备，一切都是那么自然，一切都是早有安排。 出院后的日子里，我的她没日没夜的守候在孩子身边，我们两人的爸妈及家属也轮番的关心指导。所有的这一切，皆因一个新的生命的诞生，而我们终于不再是家族中最重要的那个人，却没有丝毫的失落。 直到我在公司电脑面前的某一刻，脑子突然开始思念一个刚认识不到2周的人；直到搜索引擎历史中出现各种婴儿相关的记录，各种电商平台不停的推送给我婴儿相关产品；直到我察觉了这些细节后，我才知道我已经真真切切的经历了一场迟来的成人礼！ 他的人生，一定和我不同，也不应该相同。之前一直和朋友吹牛哔，对自己的下一代唯一想做到的只是帮助他尽可能早的找到他的人生目标。现在，却慌了，才知道担心和质疑自己到底是否有能力做到，自己是否有积累足够的人生来与他分享。 在没有更多想法之前，唯一值得叮嘱自己的是“尊重他”，平等对待，就把他当做是一起探索世界的战友吧~愿他一生安康，愿我身边的所有人能平平安安普普通通的过日子。 ============================= 毕竟是技术博客，那么分享一个知识点吧，如果你发现自己的win10系统无法修改文件默认打开方式，不妨试一试这个。","tags":[{"name":"新生命","slug":"新生命","permalink":"https://blog.kazaff.me/tags/新生命/"},{"name":"成长之重量","slug":"成长之重量","permalink":"https://blog.kazaff.me/tags/成长之重量/"}]},{"title":"单台服务器同时跑多套Php","date":"2018-10-21T09:37:12.000Z","path":"2018/10/21/单台服务器同时跑多套php/","text":"一套web系统，后台往往比前台功能要复杂丰满许多，但后台的访问权限往往更容易控制，毕竟前台是给宇宙中所有的生物在使用的。为了安全起见，我们公司的一套web电商系统开始尝试做前后台功能隔离，根据整理出来的报告，后台功能需要大量的风险高的php内置函数。前台基本上db的存存取取，没有什么图片管理，报表导出，文件打包等等功能，所以我们需要让它们分别跑在不同的php安全配置下。 服务器是一台AWS EC2，跑着默认提供的免费linux镜像，webserver用的是nginx，它使用fastcgi的方式把php交给php-fpm来处理。所以，我们的方案就很直观了： 为前后台搭建独立的虚拟主机； 前后台各自的虚拟主机将php请求分配给各自的php-fpm管理器； 运行两套不同php配置的php-fpm进程。 前两步关于nginx虚拟主机的内容这里就不多讲了，我们直奔主题：同时运行两套php-fpm进程 clone一套已有的php-fpm配置（/lib/systemd/system/php-fpm.service）: 1234567891011121314[Unit]Description=The PHP FastCGI Process Manager For Honmi Admin SystemAfter=syslog.target network.target[Service]Type=notifyPIDFile=/run/php-fpm/php-fpm-admin.pidExecStart=/usr/sbin/php-fpm --nodaemonize -c /etc/php-admin.ini -y /etc/php-fpm-admin.confExecReload=/bin/kill -USR2 $MAINPIDPrivateTmp=true[Install]WantedBy=multi-user.target 注意上面我们在ExecStart项中设置了php-admin.ini和php-fpm-admin.conf，这样新运行的这套php-fpm进程将会使用独立的php设置和php-fpm设置。 打开php-fpm-admin.conf文件，我们需要配置它使用的独立的端口号： 1234...[www]listen = 127.0.0.1:9001... 默认那套跑的在9000端口上，这样我们现在就完成了第三步。 知识点 高版本的linux，不再使用service命令管理服务，所以使用它是总会看到Redirecting to /bin/systemctl start xxxx.service提示，因为系统默认会帮你转发到systemctl命令下 执行systemctl enable xxxxx.service来将服务设置成开机启动 参考资料添加服务到开机自动启动（centos7开机自启动nginx，php-fpm）linux下多版本php共存的原理、方法","tags":[{"name":"php-fpm","slug":"php-fpm","permalink":"https://blog.kazaff.me/tags/php-fpm/"},{"name":"nginx","slug":"nginx","permalink":"https://blog.kazaff.me/tags/nginx/"},{"name":"linux","slug":"linux","permalink":"https://blog.kazaff.me/tags/linux/"}]},{"title":"php5.x禁用eval","date":"2018-10-19T09:37:12.000Z","path":"2018/10/19/php5.x禁用eval/","text":"如果是我的死忠粉，你应该看得出来，这几天我在修补公司服务器的安全漏洞。我并不是专家，干的也都只是苦力活，但也要记录一下，人老了，总忘。 这次我们来说如何禁止php代码中执行eval函数，本来以为直接修改php.ini中的disable_function即可~但现实往往并不是那么如意，查了一下GG，发现原来eval并非函数，而是php底层提供的一种特性。 幸好有前辈提供了php扩展来禁用万恶的eval：suhosin一开始发现是需要给php打补丁，我是拒绝的，但确实没有找到更好的方法。不过实际安装下来，真的很方便： 123456789101112131415yum install wget make gcc gcc-c++ zlib-devel openssl openssl-devel pcre-devel kernel keyutils patch perlcd /usr/local/srcwget http://download.suhosin.org/suhosin-对应的版本.tgztar zxvf suhosin-对应的版本.tgzcd suhosin-对应的版本/usr/bin/phpize./configure --with-php-config=/usr/bin/php-configmake &amp; make install 编译完后会提示你库文件的位置，例如: /usr/lib64/php/modules 我们只需要在php.ini中增加对应的扩展即可：12extension=/usr/lib64/php/modules/suhosin.sosuhosin.executor.disable_eval=On 重启php-fpm进程后，就可以在phpinfo中看到suhosin扩展已经装好了~仔细看增加的配置项，其实很多控制的点，得慢慢研究啊~","tags":[{"name":"php","slug":"php","permalink":"https://blog.kazaff.me/tags/php/"},{"name":"linux","slug":"linux","permalink":"https://blog.kazaff.me/tags/linux/"},{"name":"eval","slug":"eval","permalink":"https://blog.kazaff.me/tags/eval/"}]},{"title":"ec2安装php5.x","date":"2018-10-18T09:37:12.000Z","path":"2018/10/18/ec2安装php5.x/","text":"最近在aws上拼命装服务器，使用的镜像是：Amazon Linux 2 AMI 2.0.20181008 x86_64 HVM gp2。它上面默认提供了aws的源，提供了很多常用的软件，但版本都比较新。我们的项目代码依赖php5.6，所以，尴尬了。 疯狂的搜了一遍，找到个可以安装的源： 1234567891011121314sudo yum -y updatesudo yum install –y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmsudo wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmsudo wget https://centos7.iuscommunity.org/ius-release.rpmsudo rpm -Uvh ius-release*.rpmsudo yum -y updatesudo yum -y install php56u php56u-opcache php56u-xml \\ php56u-mcrypt php56u-gd php56u-devel php56u-mysql \\ php56u-intl php56u-mbstring php56u-bcmath php56u-soap 如果你用的是nginx，那么你还需要装一下php-fpm管理组件： 1sudo yum -y install php-fpm-nginx","tags":[{"name":"ec2","slug":"ec2","permalink":"https://blog.kazaff.me/tags/ec2/"},{"name":"php","slug":"php","permalink":"https://blog.kazaff.me/tags/php/"},{"name":"linux","slug":"linux","permalink":"https://blog.kazaff.me/tags/linux/"}]},{"title":"Nginx的vhost文件加载顺序","date":"2018-10-17T09:37:12.000Z","path":"2018/10/17/nginx的vhost文件加载顺序/","text":"这几天频繁的创建主机搭建站点，突然碰到个与自己直觉不匹配的情况： 当没有配置默认的虚拟主机时，用一个无法匹配server_name的链接访问服务器时，会如何？ 我直觉认为会显示错误信息。但现实世界是残酷的，nginx找了一圈都没有匹配后，会直接路由到它找到的第一个配置的虚拟主机上（前提是监听的端口是一样的）。 何为第一个？是靠虚拟主机的conf文件名字的字母表顺序！惊不惊喜？意不意外？？","tags":[{"name":"nginx","slug":"nginx","permalink":"https://blog.kazaff.me/tags/nginx/"},{"name":"虚拟主机","slug":"虚拟主机","permalink":"https://blog.kazaff.me/tags/虚拟主机/"}]},{"title":"Linux权限中x的含义","date":"2018-10-16T09:37:12.000Z","path":"2018/10/16/linux权限中x的含义/","text":"一直觉得linux提供的权限挺简单的：读(r)，写(w)，执行(x)。但总是在配置的时候碰到“莫名其妙”的错，所以这次好好学习记录一下。 读（read) 读取本文件内容的权限 浏览本文件夹下文件列表的权限 写（write) 新增，删除，修改，覆盖本文件的权限 新增，删除，修改，覆盖本文件夹中的文件的权限 执行（execute) 执行本文件的权限 进入本文件夹的权限 到目前为止，虽然描述的很清晰，但用起来还是会一头雾水。因为这几个权限并不是独立的，是存在优先级的。举几个场景： 给某个文件夹赋予了rw，但没有x：很明显，根本无法访问目录，但能看到它 给某个文件夹赋予了x，但没有给r：能cd进入该文件夹，但啥也看不到 只允许某个文件夹下的文件被读，但不允许被新增修改删除：文件夹赋予rx, 文件赋予r 结合nginx来理解x权限上面第三种情况，设置成nginx的web目录是不足够的，访问的时候会显示403错误，还记得给文件赋予x权限！！！ 好了，先记录到这里吧~~","tags":[{"name":"权限","slug":"权限","permalink":"https://blog.kazaff.me/tags/权限/"},{"name":"linux","slug":"linux","permalink":"https://blog.kazaff.me/tags/linux/"}]},{"title":"GraphQL的前提","date":"2018-09-05T10:37:12.000Z","path":"2018/09/05/GraphQL的前提/","text":"很有可能，这会是最后一篇关于这个主题的文章了，呃，这算是真的从入门到放弃么？啊哈哈哈哈哈~ 东西是好东西，思想是好思想，不过感觉用在项目中的成本和风险有些不好把握，怂了怂了，这次是真怂了。 即便如此，我们还是需要讨论一下几个相关问题： 如何设计schema（edges, nodes） 1+N问题解决原理(batch, cache) 学习曲线和资料 schema一开始接触Graphql（后称：GQL），一定是学习关于schema相关的知识，我想对于前端的同学这方面的东西也是最有新鲜感的。资料上给的例子，都非常的简化，不管你读起来感觉多么的实战，但不得不承认依然十分的简单。但实际项目中，你会遇到复杂得多的场景，并不是说GQL无法支持复杂场景，而是想说场景复杂后，会要求开发者把控schema的能力要十分的强。 这让我不禁想到了领域模型的概念，不过话说回来，即便是现在早已普世的RESTful，真的用在项目里，也依然要搞清楚它本质的概念（面向资源）。殊途同归，最终都是要先消化业务，抽象模型，再来谈最基本的语法。只不过，GQL的schema中为模型里的很多对应概念都提供了对应的元素：edges, nodes。 不知道你有深究过GQL名字的含义么？其中graph到底表达了什么含义？ 了解Neo4j这种基于图理论的数据库，就很容易找到对应~所以，我们的GQL也基于图的概念哟~上图中每个圆点就代表了nodes，每个灰色的线条就代表了edges。延续这种思路，套用在真实项目中，就需要你有深厚的领域建模功力，甚至也要了解服务拆分和部署相关的知识，没错，要求你是架构师啦。呃，至少是业务专家吧。当然，如果你并不完全遵守规则也可以（就好像大多数生成自己是Rest的接口，其实仅仅是http+json的rpc而已）~~ 1+N在之前的一篇翻译文章中就提到过这个问题，不过现在看来，作者撒了个谎，或者说至少他并没有提到使用dataloader必须要遵守的规则，恰恰是这两个规则会导致你方案的复杂性（尤其是对于计划让GQL代理后端服务的项目）。 如果我们仔细看过dataloader开发者提供的视频，你会很明确的知道，要想正常的使用dataloader，你就不得不要确保： 数据源（可以是db，也可能是service）返回的数据条目数必须完全匹配dataloader受理的请求个数 请求的顺序和数据源响应的结果的顺序也要保持一致 这两点在你研读dataloader的源码后就会完全明白~ 那官方文档的例子来说，假如你在一个时间循环中多次调用load方法，dataloader打包了你的请求，打包后的请求为[ 2, 9, 6, 1 ]，那么意味着数据源返回的数据必须如下： 123456[ &#123; id: 2, name: &apos;San Francisco&apos; &#125;, &#123; id: 9, name: &apos;Chicago&apos; &#125;, null, &#123; id: 1, name: &apos;New York&apos; &#125;] 请注意条目数和顺序的一致性。 大家可以思考一下，我们要如何保证这两点呢？是否会造成已有的服务必须调整呢？（老实说，这是我最头疼的一点）。 学习曲线除了学习GQL外，我的观点已经很明确，你还需要很多其他知识来辅佐，否则很难讲你会在实际项目里把GQL发挥到最大功效。而目前为止，互联网上关于QGL的实战资料很少，我能找到的都还停留在概念讲解和语法分析上，这意味着我们这样的小团队想开箱即用基本上是不太可能的。不过还是长期看好这个东西，详细它会成为下一代API的趋势。 扩展阅读https://blog.apollographql.com/explaining-graphql-connections-c48b7c3d6976https://github.com/facebook/dataloader#batchinghttps://platform.github.community/t/difference-between-using-edges-node-nodes/1883/3https://medium.com/@gajus/using-dataloader-to-batch-requests-c345f4b23433","tags":[{"name":"GraphQL","slug":"GraphQL","permalink":"https://blog.kazaff.me/tags/GraphQL/"},{"name":"DDD","slug":"DDD","permalink":"https://blog.kazaff.me/tags/DDD/"},{"name":"pagination","slug":"pagination","permalink":"https://blog.kazaff.me/tags/pagination/"}]},{"title":"GraphQL运行原理","date":"2018-08-29T10:37:12.000Z","path":"2018/08/29/GraphQL运行原理/","text":"最近依然在关注GraphQL这个技术，感觉它非常有前景。结合现有的业界关于网关服务的实践和总结，再融合GraphQL的思想，瞬间一个先进无比的网关服务出现在了眼前。 之前翻译过一篇文章，感觉是读过的最完整的一篇深入浅出graphql的实战文章。今天打算继续消化一下文章中引用的一份文档：Execution。这篇文档解释了GraphQL是如何根据定义的schema来完成数据的组装和聚合的，应该算是整个GraphQL架构中最核心的设计之一了，值得了解。下面废话不多说，正文走起。 Execution在必要的数据校验环境后，GraphQL（译：后面简称GQL）服务端会根据每一个query的实际要求来剪裁恰如其分的数据结构以进行响应，通常来说是JSON格式。 GQL非常依赖其类型模型（type system），我们来看一个实际的例子来演示如何执行一个query。这个例子和文档中其他部分是一致的： 12345678910111213141516171819type Query &#123; human(id: ID!): Human&#125;type Human &#123; name: String appearsIn: [Episode] starships: [Starship]&#125;enum Episode &#123; NEWHOPE EMPIRE JEDI&#125;type Starship &#123; name: String&#125; 为了了解query执行的细节，我们来看一个例子： 1234567891011121314151617181920212223242526272829303132// 请求&#123; human(id: 1002) &#123; name appearsIn starships &#123; name &#125; &#125;&#125;// 响应&#123; &quot;data&quot;: &#123; &quot;human&quot;: &#123; &quot;name&quot;: &quot;Han Solo&quot;, &quot;appearsIn&quot;: [ &quot;NEWHOPE&quot;, &quot;EMPIRE&quot;, &quot;JEDI&quot; ], &quot;starships&quot;: [ &#123; &quot;name&quot;: &quot;Millenium Falcon&quot; &#125;, &#123; &quot;name&quot;: &quot;Imperial shuttle&quot; &#125; ] &#125; &#125;&#125; 你可以将在GQL查询中每一个字段（field）看做是前一种类型的函数或方法，它返回下一种类型。事实上，这就是GQL的工作原理。在GQL服务端，每一个类型的每一个字段背后都是靠一个被称为resolver的函数来支撑的（译：提供数据）。每当一个字段被要求返回，与其对应的resolver函数就会被执行，并产生下一个值（译：进入新一轮resolver执行）。 如果发现字段返回的是一个标量值，如字符串或数字，此时执行就算告一段落了。然而如果该字段执行后返回的是一个对象值，那么执行器会继续试图获取该对象值包含的字段，一直到最终得到一个标量值为止。 Root fields &amp; resolvers每个GQL服务的最顶层类型是一个包含一切的统一API入口类型，通常我们称之为Roottype或Querytype。在我们的例子中，Querytype提供了一个字段叫human，它接受一个参数id。这个字段对应的resolver函数通过操作数据库并构建和返回human对象。 1234567Query: &#123; human(obj, args, context, info) &#123; return context.db.loadHumanByID(args.id).then( userData =&gt; new Human(userData) ) &#125;&#125; 这个例子是用JavaScript写的，但GQL服务端可以用多种语言来实现。resolver函数接受4个参数： obj: 前一个对象（译：触发该resolver的字段所在的对象），这个参数对最顶层字段没有意义（译：当然啊，不然嘞~） args: 来自GQL query context: 提供所有resolver依赖的资源，例如数据库连接对象，当前登录的用户信息等 info: 包含与当前查询相关的特定于字段的信息的值，以及模式细节，可以参考graphqlobjecttype Asynchronous resolvers我们来近距离看一下这个resolver函数的细节： 12345human(obj, args, context, info) &#123; return context.db.loadHumanByID(args.id).then( userData =&gt; new Human(userData) )&#125; context参数中包含了数据库连接对象，用于数据查询来得到GQL query中要求的id数据。查询数据库是一个异步调用，所以返回一个promise。promise是javascript中的异步调用概念，不过其他很多语言也有对应的实现方式，通常被称之为futures，tasks或者Deferred。当数据库操作返回后，我们就可以构建和返回一个新的Human对象啦。 需要注意的是尽管resolver函数返回的是promise，但GQL query并不是异步的，它会期望human携带了所有要求返回的数据。在执行过程中，GQL会一直等到Promise,Futures或Tasks完结后才会继续并最大化保持并发度（译：这一点很重要）。 Trivial resolvers现在我们已经得到了一个Human对象，接下来GQL执行器将继续处理其下的字段。 12345Human: &#123; name(obj, args, context, info) &#123; return obj.name &#125;&#125; GQL服务依靠类型系统来决定如何继续执行下去。甚至是在human返回任何结果之前，GQL就可以根据类型系统要求提供的human类型声明得到下一步应该处理的字段有哪些。 在这个例子里name字段的处理是非常简单明了的。传入name resolver函数的obj参数就是前一步返回的那个new Human对象。例子中我们期望得到的human对象包含name字段，已经如愿以偿。 事实上，很多GQL类库都不需要你提供这种简单的resolver，它们会默认自动从obj中读取并返回对应名字的属性（译：默认解析器规则）。 Scalar coercion当name字段被处理过后，appearsIn和starships字段会被同时处理。appearsIn字段也有一个trivial resolver，我们来仔细看一下： 12345Human: &#123; appearsIn(obj) &#123; return obj.appearsIn // returns [ 4, 5, 6 ] &#125;&#125; 注意，我们的类型系统声明appearsIn将返回一个枚举类型，然而这个函数却返回的是number数组！实际上，如果我们查看结果，我们将看到相应的Enum值被归还。发生了什么？ 这就是一个Scalar coercion的例子。类型系统知道应该返回什么，并将解析器返回的数据转换成了API声明要求的类型。在这个例子中，在服务的其他位置应该存在一个枚举类型的定义来标识4,5,6对应的枚举值。 List resolvers通过appearsIn，我们已经看到了当一个字段需要一个返回多条数据时的细节。它返回了一个枚举值数组，然后类型系统将每个值转换成了对应的枚举值。那starships字段解析的细节有是什么呢？ 123456789Human: &#123; starships(obj, args, context, info) &#123; return obj.starshipIDs.map( id =&gt; context.db.loadStarshipByID(id).then( shipData =&gt; new Starship(shipData) ) ) &#125;&#125; 这个字段的resolver不只是返回一个promise，它返回了一个promise数组（译：屌不屌）。human对象拥有一个starships的id数组，但我们需要加载所有这些id关联的starship对象。 GQL会等待所有的promise并发的完成后才会继续，当所有starship对象都得到后，GQL会继续并发的尝试获取这些对象的name字段。 Producing the result当所有字段都处理完毕后，结果值构建成一个从叶子节点到根节点全链路的键值对映射，键为字段名，值为resolver返回的结果，最终按照请求的结构返回给客户端对应的数据结构（JSON结构）。","tags":[{"name":"GraphQL","slug":"GraphQL","permalink":"https://blog.kazaff.me/tags/GraphQL/"}]},{"title":"Golang依赖包下载被禁解决方案","date":"2018-08-27T10:37:12.000Z","path":"2018/08/27/golang依赖包下载被禁解决方案/","text":"每次装环境，都会卡在这里半天，实在不知道曾经的golang.org到底做错了什么而被封杀，有谁知道可以留言一下吗？ 查了几个方案，一开始一心想按照前辈的经验搭建一个代理来下载目标库文件，不过后来发现实在是太麻烦了就放弃了。 今天我说一个土鳖但直观的方案，那就是手动下载项目依赖包文件： 确保系统上安装了git git clone https://github.com/golang/xxx到本地指定目录（其中xxx表示你要安装的某个依赖库） 在$GOPATH/src/golang.org/x/下手动创建对应的文件夹结构 将步骤2中下载来的目标文件目录拷贝到步骤3手动创建的目录中 重新执行go get xxxxx来完成安装 就是这么简单，或者更简单的是，你自己本地搭建一个web环境，然后把golang.org配置在hosts里指向自己的web环境即可。 举一个实际的例子吧，假如我们要安装github.com/go-resty/resty，直接执行go get你会得到下面的错误： 1package golang.org/x/net/publicsuffix: unrecognized import path &quot;golang.org/x/net/publicsuffix&quot; (https fetch: Get https://golang.org/x/net/publicsuffix?go-get=1: dial tcp 216.239.37.1:443: i/o timeout) 那怎么办？我们按照上面的步骤，先git clone https://github.com/golang/net到本地。然后在本地的GOPATH目录下创建/src/golang.org/x/net/目录纵深。然后直接把git clone的目录中的publicsuffix文件夹拷贝到手动创建的目录中即可，最后再次执行go get github.com/go-resty/resty即可。","tags":[{"name":"golang","slug":"golang","permalink":"https://blog.kazaff.me/tags/golang/"},{"name":"墙","slug":"墙","permalink":"https://blog.kazaff.me/tags/墙/"}]},{"title":"不惑不足而立有余之年","date":"2018-08-07T09:37:12.000Z","path":"2018/08/07/不惑不足而立有余之年/","text":"虚岁35了，刚好卡在所谓的“四十不惑”与“三十而立”之间，很尴尬。尴尬之处在于，自己并没有感觉自己有何成就，谈不上立业，更谈不上不惑。 可能人生就是这样吧，不然为何身边的同龄人不少也和我一样恐慌呢？不得而知。总之，假如我这一生总长为70年，我也刚好卡在中间，走过的那一半碌碌无为，未来的那一半可能也只能随波逐流。 我在自己的少年和青年时代，活的像孩童，我在中年时代则活的像弱智，我不希望步入老年后还那么愚蠢。所以不知为何开始对历史产生了兴趣，而曾经嘴边总说“只搞技术，不谈政治”的我，对技术的热情逐渐降温。我甚至都不清楚是因为什么让我否定之前的信仰。 我开始不自觉的思考很多问题，关于自己的，关于身边人的，关于这个社会的，关于这个国家的，关于这个世界的，然后发现，这个世界都是这么的混乱。然后我又开始看世界历史，发现仿佛一直都那么乱，这让我觉得有种窒息的感觉。原来，单纯的沉浸在技术的那段时间里，是上天赐予我最好的礼物，难道说这就是那句老话“傻人有傻福”的哲学含义么？ 我生而不是富二代，而且现在看来，我的孩子也注定不是富二代，这也算是一种世袭吧。这并不可悲，我只是希望Ta能比我更早的养成更多的好习惯，能真的找到热爱的人和事，能生活在更自由更和平更繁荣的新世界。不要过度追求名利，不要过度渴望别人的赞美，也不要在被他人讨好中迷失自己，成为这样的一个拥有自我意识的人，才可能感受到身边的幸福。 最近一段时间来，觉得时间过得飞快，仿佛时间的最小单位是天，不知道是记性变差了，还是太浮躁了导致的。如果这样下去，真的很担心不知不觉就到了人生终点，可还什么感悟都没有呢。","tags":[]},{"title":"Ethereum开发环境搭建","date":"2018-04-30T09:37:12.000Z","path":"2018/04/30/ethereum开发环境搭建/","text":"区块链，哼，我倒要看看能有多火，能有多难！ 本次我们来搭建一个以太坊的私有链，大体思路和前一篇Bitcoin的也很类似。 早先其实就在笔记本上搭建过一个环境，不过玩了几次就荒废了。这次记录下来，以备不时之需~~我们还是借助docker来快速搭建一套开发环境，所以第一步还是选择一个方便一点的镜像。 我找了好几个，都感觉不是太顺手，所以在别人的基础上修改了一下： 1234567891011121314151617181920212223242526272829303132FROM ubuntu:latestLABEL maintainer=&quot;edisondik@gmail.com&quot;ARG GETH_URL=https://gethstore.blob.core.windows.net/builds/geth-alltools-linux-amd64-1.7.2-1db4ecdc.tar.gzARG GETH_MD5=c17c164d2d59d3972a2e6ecf922d2093ARG DEBIAN_FRONTEND=noninteractiveRUN apt update &amp;&amp; \\ apt install wget -y &amp;&amp; \\ cd /tmp &amp;&amp; \\ wget &quot;$GETH_URL&quot; -q -O /tmp/geth-alltools-linux-amd64.tar.gz &amp;&amp; \\ echo &quot;$GETH_MD5 geth-alltools-linux-amd64.tar.gz&quot; &gt; /tmp/geth-alltools-linux-amd64.tar.gz.md5 &amp;&amp; \\ md5sum -c /tmp/geth-alltools-linux-amd64.tar.gz.md5 &amp;&amp; \\ tar -xzf /tmp/geth-alltools-linux-amd64.tar.gz -C /usr/local/bin/ --strip-components=1ENV GEN_NONCE=&quot;0xeddeadbabeeddead&quot; \\ DATA_DIR=&quot;/root/.ethereum&quot; \\ CHAIN_TYPE=&quot;private&quot; \\ GEN_CHAIN_ID=70213WORKDIR /opt# like ethereum/client-goEXPOSE 30303EXPOSE 8545ADD src/* /opt/RUN chmod +x /opt/*.sh# ENTRYPOINT [&quot;/opt/startgeth.sh&quot;]ENTRYPOINT [&quot;/bin/sh&quot;] 上面这个dockerfile文件的同级目录下需要src文件夹，在改文件夹下放2个文件: genesis.json:12345678910111213141516171819&#123; &quot;config&quot;: &#123; &quot;chainId&quot;: $&#123;GEN_CHAIN_ID&#125;, &quot;homesteadBlock&quot;: 0, &quot;eip155Block&quot;: 0, &quot;eip158Block&quot;: 0 &#125;, &quot;alloc&quot; : &#123; $&#123;GEN_ALLOC&#125; &#125;, &quot;coinbase&quot; : &quot;0x0000000000000000000000000000000000000000&quot;, &quot;difficulty&quot; : &quot;0x20000&quot;, &quot;extraData&quot; : &quot;&quot;, &quot;gasLimit&quot; : &quot;0x2fefd8&quot;, &quot;nonce&quot; : &quot;$&#123;GEN_NONCE&#125;&quot;, &quot;mixhash&quot; : &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;, &quot;parentHash&quot; : &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;, &quot;timestamp&quot; : &quot;0x00&quot;&#125; startgeth.sh:123456789101112131415161718192021222324252627282930#!/bin/shGEN_ARGS=# replace varsecho &quot;Generating genesis.nonce from arguments...&quot;sed &quot;s/\\$&#123;GEN_NONCE&#125;/$GEN_NONCE/g&quot; -i /opt/genesis.jsonecho &quot;Generating genesis.alloc from arguments...&quot;sed &quot;s/\\$&#123;GEN_ALLOC&#125;/$GEN_ALLOC/g&quot; -i /opt/genesis.jsonecho &quot;Generating genesis.chainid from arguments...&quot;sed &quot;s/\\$&#123;GEN_CHAIN_ID&#125;/$GEN_CHAIN_ID/g&quot; -i /opt/genesis.jsonecho &quot;Running ethereum node with CHAIN_TYPE=$CHAIN_TYPE&quot;# empty datadir -&gt; geth initDATA_DIR=$&#123;DATA_DIR:-&quot;/root/.ethereum&quot;&#125;echo &quot;DATA_DIR=$DATA_DIR, contents:&quot;ls -la $DATA_DIRecho &quot;DATA_DIR &apos;$DATA_DIR&apos; non existant or empty. Initializing DATA_DIR...&quot;geth --datadir &quot;$DATA_DIR&quot; init /opt/genesis.jsonGEN_ARGS=&quot;--datadir $DATA_DIR&quot;# [[ ! -z $NET_ID ]] &amp;&amp; GEN_ARGS=&quot;$GEN_ARGS --networkid=$NET_ID&quot;# [[ ! -z $MY_IP ]] &amp;&amp; GEN_ARGS=&quot;$GEN_ARGS --nat=extip:$MY_IP&quot;GEN_ARGS=&quot;$GEN_ARGS --nat=any&quot;exec /usr/local/bin/geth --nodiscover --rpc --rpcaddr &quot;0.0.0.0&quot; --rpcport &quot;8545&quot; --rpccorsdomain * --rpcapi db,eth,net,web3,personal $GEN_ARGS 然后创建这个镜像即可（docker build -t kz-geth .）。 等待片刻（其实很久），我们接下来就可以执行./startgeth.sh命令来启动以太坊节点了。不过，稍微解释一下dockerfile里的细节，“GEN_CHAIN_ID”一定要写一个自己的值，如果默认用1的话，启动的以太坊节点会去同步主链上的数据，很恐怖的哟~ 另外，一定要注意我们运行节点时命令的参数，尤其是--rpcaddr &quot;0.0.0.0&quot;和--rpccorsdomain *，还有--rpcapi db,eth,net,web3,personal，如果你不想在后面web3js通信的时候碰到问题的话。 启动完毕后，我们可以再开启一个新的终端，在其中执行geth attach命令，它会自动开启一个console并链接到我们自己的节点上。在这个console界面，我们就可以执行一些常见的命令来和以太坊节点交互了。 下面我列一下简单的命令来完成一次转账流程： eth.accounts 该命令列出当前帐号有哪些，目前应该是空 personal.newAccount(“123”) 创建一个密码为123的帐号，需要执行两次，第一次生成的那个帐号默认会作为coinbase来接受挖矿奖励 miner.start() 开启挖矿，这样我们第一个帐号就会不停的得到以太币 eth.getBalance(“第一个帐号地址”) 可以查看指定帐号的余额 eth.sendTransaction({from:”第一个帐号地址”, to: “第二个帐号地址”, value: web3.toWei(1)}) 这样就能完成一次转账 当然，目前为止，我们只是完成了最最简单的一步，后面还有更多好玩的内容，期待~","tags":[{"name":"docker","slug":"docker","permalink":"https://blog.kazaff.me/tags/docker/"},{"name":"golang","slug":"golang","permalink":"https://blog.kazaff.me/tags/golang/"},{"name":"geth","slug":"geth","permalink":"https://blog.kazaff.me/tags/geth/"}]},{"title":"Bitcoin开发环境搭建","date":"2018-04-24T09:37:12.000Z","path":"2018/04/24/bitcoin开发环境搭建/","text":"区块链，哼，我倒要看看能有多火，能有多难！ 我们这次就来搭建一下bitcoin的开发环境，作为一个新手，肯定少不了要了解非常多的基本概念和常识，才能顺利的完成开发环境的搭建，所以这篇文章不仅仅是搭建一个简单的开发环境而已，也会争取尽可能多的记录一些基础知识点。这样，我觉得才算是有价值。 bitcoin的钱包客户端不止一种，而我比较中意的是golang的btcd。据说基于golang的性能出众，目前很多矿机上都是跑的golang客户端，不确定是否属实，也可能人家都进行了各自的改良，但我目前就打算以btcd作为入口，希望你也和我的选择一样。 项目介绍在github上btcd主项目下根据功能和层次，拆分了好多不同的子项目，我们首先就来了解一下它们： blockchain: 这个模块应该是核心模块，负责区块链基础功能（创建，校验，存储区块），不过目前对我们这种小白来说，可以略过 btcec: 提供加解密和数字签名相关库 btcjson: 用于针对bitcoin JSON-RPC API规范进行参数编码解码的库 chaincfg: 提供了预设的bitcoin网络配置项 cmd: 提供了一些命令工具 connmgr: 负责bitcoin网络链接管理（链接的生命周期管理） database: 存储区块数据和一些元数据 mempool: 暂存和管理待处理交易信息的模块 mining: 应该是挖矿相关功能模块 netsync: 提供一个并发安全的同步锁协议 peer: 创建和管理网络节点的模块 rpcclient: 实现了一个基于websocket的json-rpc客户端 wire: 提供了bitcoin的通信协议实现 上面每个模块的描述是从项目文件夹中对应的README.md中翻译的，由于目前我也处于入门阶段，可能理解错了一些含义，希望大家可以帮我纠错~ 值得关注的一点，是来自官方的一段描述： btcd和bitcoin core客户端功能上最大的不同点是btcd并不包含钱包相关功能，这是有意而为之的一个设计决策。这意味着，只安装btcd并不能进行交易，还需要安装运行btcwallet才行。 这段描述对我们搭建环境很重要，下面咱们就回到主题，搭建环境喽~ 环境搭建根据我的个人习惯，开发环境还是用docker来搭建好，你不需要安装go语言环境，不需要自己下载btcd编译环境等等，只需要简单的找到一个满足自己的镜像就可以了。我根据自己找到的镜像，修改后的dockerfile共享出来： 12345678910111213141516171819202122232425262728293031323334353637383940FROM golang:1.9-stretchMAINTAINER edisondik@gmail.com# Expose mainnet ports (server, rpc)EXPOSE 8333 8334# Expose testnet ports (server, rpc)EXPOSE 18333 18334# Expose simnet ports (server, rpc)EXPOSE 18555 18556# Expose segnet ports (server, rpc)EXPOSE 28901 28902RUN go env GOROOT GOPATHRUN go get -u github.com/Masterminds/glide \\&amp;&amp; git clone https://github.com/btcsuite/btcd $GOPATH/src/github.com/btcsuite/btcd \\&amp;&amp; cd $GOPATH/src/github.com/btcsuite/btcd \\&amp;&amp; glide install \\&amp;&amp; go install . ./cmd/... \\&amp;&amp; btcd --version \\&amp;&amp; cd $GOPATH/src/github.com/btcsuite/btcd \\&amp;&amp; git pull &amp;&amp; glide install \\&amp;&amp; go install . ./cmd/... \\&amp;&amp; btcctl --versionRUN go get -u github.com/Masterminds/glide \\&amp;&amp; git clone https://github.com/btcsuite/btcwallet $GOPATH/src/github.com/btcsuite/btcwallet \\&amp;&amp; cd $GOPATH/src/github.com/btcsuite/btcwallet \\&amp;&amp; glide install \\&amp;&amp; go install . ./cmd/... \\&amp;&amp; cd $GOPATH/src/github.com/btcsuite/btcwallet \\&amp;&amp; git pull &amp;&amp; glide install \\&amp;&amp; go install . ./cmd/...ENTRYPOINT [&quot;/bin/sh&quot;] 然后根据这个配置我们来创建一个容器，终端切到文件所在的文件夹下后执行：docker build -t bctd-test . 进入到我们刚创建的容器中，执行： 12btcd --simnet --rpcuser=kazaff --rpcpass=123456btcwallet --simnet --username=kazaff --password=123456 如果碰到错误提醒： 123[WRN] BTCW: open /root/.btcwallet/btcwallet.conf: no such file or directoryorError creating a default config file: open /root/.btcwallet/btcwallet.conf: no such file or directory 不要怕，只需要在对应目录下手动创建对应文件即可。官方说明中提示，需要在对应的配置文件中定义好rpc和rpclimited对应帐号，btcd才会开始rpc服务，但我们通过上面的命令启动btcd后，就已经可以进行rpc请求了。 接下来我们来执行查询命令： 1btcctl --simnet --wallet --rpcuser=kazaff --rpcpass=123456 getinfo 注意，我们增加的--wallet，目前我理解的是，这个参数指明我们是去连接的wallet服务，wallet服务也会去调用btcd服务。上面的这个命令其实也可以去掉该参数，只是你得到的返回结果不太一样。但，个别命令只有wallet服务提供，所以还是都加上这个参数的稳妥一些。 是的，没错，用docker来搭建开发环境就是这么简单，基本上我们已经搭建好了一个模型。接下来我们来逐个测试命令。 命令介绍1btcctl --simnet --wallet --rpcuser=kazaff --rpcpass=123456 listaccounts 该命令会列出目前btcd中包含的帐号列表。 我们还可以查询指定帐号下的钱包地址列表： 1btcctl --simnet --wallet --rpcuser=kazaff --rpcpass=123456 getaddressesbyaccount &quot;default&quot; 该命令查看了默认帐号下所有的钱包地址。 那到底有多少可以互动的命令呢？ 1btcctl --simnet --wallet --rpcuser=kazaff --rpcpass=123456 help 超级多我找到一个不确定是否过期的文档，可以快速对可用命令了解一个大概 后面我还会分享更多的关于用代码和钱包服务进行交互的内容，长期关注吧~","tags":[{"name":"docker","slug":"docker","permalink":"https://blog.kazaff.me/tags/docker/"},{"name":"golang","slug":"golang","permalink":"https://blog.kazaff.me/tags/golang/"},{"name":"btcd","slug":"btcd","permalink":"https://blog.kazaff.me/tags/btcd/"}]},{"title":"重谈Url签名","date":"2018-04-13T09:37:12.000Z","path":"2018/04/13/重谈url签名/","text":"今天和同事讨论一个老生常谈的问题：如何防止请求参数被篡改。这个问题应该很老了，不管是C/S或B/S模式，诞生之初就应该已经面临这个问题了。 写过支付宝或微信支付对接的同学肯定对解决方案不陌生，那就是【url签名】。我们只需要将原本发送给server端的明文参数做一下签名，然后在server端用相同的算法再做一次签名，对比两次签名就可以确保对应明文的参数没有被中间人篡改过。 没错，很简单不是么，一般大家都选择一种不可逆算法来签名，例如MD5。但问题来了，如果签名的时候没有一个“私钥”的存在，中间人也可以自己重新签名来达到篡改的目的啊。等等，那支付宝和微信的签名怎么做的？别忘了人家是提供了私钥的哟！回忆一下相关文档就真相大白了。 那既然私钥很重要，那就加上呗。但最最最困难的问题诞生了，如何在前后端通信的基础上传递这个私钥呢？私钥总是需要双方都知晓的不是么？支付宝和微信之所以需要考虑这个问题，是因为它们的api是直接对接服务端的，代码不会跑在浏览器端，所以是非常安全的。 假如中间人从最早就存在，那你无论如何你都不可能传递一个私钥给对端，那HTTPS是怎么做到的？相信你查了一圈资料后也会知道它的安全也是有前提的。总之，我们得找一种方法传递一个私钥到对端。 HTTPS借助非对称加密算法来实现，前提是通信两端确实创建了一个连接，哪怕这个连接是处在不安全信道上的。但如果客户端从来都没有直连服务端，那神仙也帮不了你。所以还是要保证自己的网络环境的安全，公共wifi下还是不要上淘宝了。但你依然还是要保证通信要基于https，因为绝大多数情况下HTTPS提供的保障都是足够的。 我们接下来讨论一下一个小众的场景，看看在一些特殊场景下是否可以更好的来完成防止中间人篡改的问题，我们就假如现在要从客户端提交一个请求来告诉服务端说“你很爱你的老婆王铁柱”。为了避免你的情敌在中间给你篡改信息，我们的系统要怎么做？ 你一定用过手机网银app对吧？你有没有记得你每次登录系统后，会有个弹窗欢迎语：欢迎登录，吴彦祖。（对，没错，我真名就是吴彦祖）这个名字是你之前就设置好的，系统每次你登录的时候返回给你这个提示，就是确保没有中间人攻击的。简单，但非常有效，因为用户可以几乎没有额外的操作就能判断出是否存在风险。这是一种值得借鉴的方案，它用来解决 在连接创建之初通信环境是安全的。（当然，中间人也是可以代理来返回的） 接下来，依然可能突然在通信中间出现了中间人，毕竟http请求是独立的。上面提到了，不要靠https了，那我们就得想办法创建一个独立的通信渠道来完成秘钥的传递。 不考虑运营成本的话，短信不错，当客户端提交请求时，会收到一条短信验证码，客户端利用验证码做秘钥进行签名即可，但短信是要钱的！你是否听说过google-authenticator，就好像早先的将军令，一种基于时间片的算法。我们也可以利用它生成的code来做签名秘钥，完美！ 最后，为了保证完全没问题，我们还可以增加一个让用户确认的页面，在页面上显示服务器端收到的信息内容。（当然，中间人也是可以修改的） 我们做了这么多，除了短信和google-authenticator两步外，其它步骤都仅仅是增加了中间人攻击的门槛而已。但核心的这两步却对用户使用体验伤害比较大，使用的时候需要斟酌。 不说了，再考虑一下吧~ 补充：针对https安全提升方案里，可以看看 https://imququ.com/post/http-public-key-pinning.html","tags":[{"name":"google","slug":"google","permalink":"https://blog.kazaff.me/tags/google/"},{"name":"中间人","slug":"中间人","permalink":"https://blog.kazaff.me/tags/中间人/"},{"name":"篡改","slug":"篡改","permalink":"https://blog.kazaff.me/tags/篡改/"},{"name":"md5","slug":"md5","permalink":"https://blog.kazaff.me/tags/md5/"}]},{"title":"无法获取Google Oauth的Refresh Token的小坑","date":"2018-04-06T10:37:12.000Z","path":"2018/04/06/无法获取Google Oauth的Refresh Token的小坑/","text":"最近在项目需要和google的服务对接，google开放了大量的rest api来供第三方使用。而要使用其接口，第一步就是认证授权。 Oauth相关的内容老早就分享过，不过那时候应该是针对qq和微信，差别不大~~可以通过这篇文章看到是如何快速简单的对接google Oauth2的。 现在分享东西，都免不了吐槽。google官方提供的文档很详实，但它引用的第三方类库就不那么给力了，golang语言的类库基本上就没有文档，用法全靠自己在网上摸索~~ 不过，今天要说的这个注意事项和类库无关，是google oauth的一个设计特性。我在测试的时候，按照上面给的那篇文章的代码run了一遍，起初是卡在了“用code换取token”的这一步，总是提醒 tcp io time out。一开始以为是使用的这个类库过时了，毕竟官方文档例子中使用的接口url和类库源码中看到的不一样~~ 后来才发现，原来是请求被墙了导致的，而之所以不是在用户授权页面就卡住的原因是因为我本地机器是基于sss代理来访问googl的，而这个代理并不能被程序使用。不过这个问题属于golang开发者的老生常谈问题了。推荐装一个 Proxifier，这样不用写一行代码就可以做到让电脑任何的请求都达到翻墙的效果了。 这算是个题外话。我们今天主要说的是，一旦你以上面的代码那样配置，并首次获取到了token后，你在改变一些配置时会发现并没有拿到期望结果~~总感觉你的代码和google之间有一层“缓存”的存在。原因就在prompt这个参数。官方文档里有对这个参数的详细解释，只是新手很难拿它和这个问题关联起来。 我碰到的表象是，我首次运行测试用例时，并没有设置access_type=offline，这就意味着我只能获取到access_token，而没有refresh_token。而前者的有效期一般只有1小时，而我们的项目需要长久的授权来达到数据同步的目的。 虽然我修改代码，增加了对应的设置： 1url := googleOauthConfig.AuthCodeURL(oauthStateString, oauth2.AccessTypeOffline) 但谁曾想到重新运行后依然拿不到refresh_token。这就是我今天打算分享的坑。 我们显然已经解释过原因了： promptOptional. A space-delimited, case-sensitive list of prompts to present the user. If you don’t specify this parameter, the user will be prompted only the first time your app requests access. 该参数的作用用来设置是否重复提醒用户进行授权的，如果我们没有设置，它默认就是只在第一次尝试授权时提醒用户。之后你修改了参数，google还是基于第一次授权时的配置进行响应。 网上找到了另外一个解决方案，不过需要用户配合：用户访问https://myaccount.google.com/u/0/permissions页面，该页面时google提供给用户用来管理自己授权信息的，用户可以在页面列表中找到我们的project，然后删除掉现有的这个授权即可。 这样，再次运行实例，用户就需要重新授权，我们也就能提供新的配置参数，从而获取到我们想要的值了。意不意外？惊不惊喜？真不真实？ 补充经过测试，发现，即便咱们设置了oauth2.AccessTypeOffline，在多次尝试获取token的时候，也是只有第一次能拿到refresh token，我想这也是合理的，毕竟官方多次提醒你应该持久化refresh token，一旦你错过了第一次，那就只能靠prompt了（未测试）。","tags":[{"name":"google","slug":"google","permalink":"https://blog.kazaff.me/tags/google/"},{"name":"Oauth","slug":"Oauth","permalink":"https://blog.kazaff.me/tags/Oauth/"},{"name":"refresh token","slug":"refresh-token","permalink":"https://blog.kazaff.me/tags/refresh-token/"}]},{"title":"Docker搭建360atlas环境","date":"2018-04-05T10:37:12.000Z","path":"2018/04/05/docker搭建360atlas环境/","text":"今天我们继续来完成我们的测试环境，上一篇我们完成了一个mysql主从延迟同步的环境搭建。今天我们在它的基础上，再通过docker镜像搭建一个360atlas即可。 360的这个atlas中间件已经很久没有更新了，最近看到和它相关的新闻是美团的一款基于它的改造版，推荐有需要的同学优先考虑。 回到主题，我们既然要用docker来搭建，那么就需要快速找一个它的镜像，我好像只找到了一个：docker-360Atlas。那也只能用它了，不要以为我不懂感恩，是这个镜像真的有点糙啊！好歹给一个运行命令的文档啊。。。让我这种小白怎么快乐的开箱即用？！不过还好它有Dockerfile，可以看到明细。 结合360官方给的atlas安装说明（也是tm糙的一笔），我开始运行容器： 1docker run -it --name 360atlas -v /c/Users/kazaff/atlas/conf:/usr/local/mysql-proxy/conf -p 1234:1234 -p 2345:2345 mybbcat/docker-360atlas /bin/bash 注意这里我们需要在容器启动后立刻链接到容器内部，因为上面这个命令是无法成功启动atlas服务进程的。。。因为镜像的设定是去加载/usr/local/mysql-proxy/conf/docker-atlas.cnf配置文件，但是偏偏在镜像中把这个配置文件声明成了文件夹。。。草，这tm可折腾死我了。docker无法把文件映射到文件夹。。。我也不能。 好吧，我们只能在容器内部手动启动atlas进程，并使用我们创建的test.cnf： 1/usr/local/mysql-proxy/bin/mysql-proxyd test start 配置文件细节我就不贴了，这部分官方文档还是解释的够可以的。 至此为止，我们的环境已经搭建完成了，项目代码测试走起~~","tags":[{"name":"dataloader","slug":"dataloader","permalink":"https://blog.kazaff.me/tags/dataloader/"},{"name":"resolver","slug":"resolver","permalink":"https://blog.kazaff.me/tags/resolver/"}]},{"title":"Docker搭建mysql主从延迟同步环境","date":"2018-04-04T10:37:12.000Z","path":"2018/04/04/docker搭建mysql主从延迟同步环境/","text":"最近打算为项目的数据库进行优化，考虑读写分离。目前打算再搭建一个中间件Atlas，放在mysql服务器前面，这样数据库的读写分类就不会被项目业务代码感知到。当然，也不是完全感知不到，一些业务还是需要强制读主库的。 考虑到Atlas提供的/*master*/语法我不是百分百确定，因为项目用到各种语言的mysql库，我觉得有必要测试一下在所有语言下这个特性都能得到正确的执行。基于这个原因，我决定要手动搭建一个测试环境。 这个测试环境主要是要模拟主从同步延迟，我们就靠这个延迟的时间窗口，来判断到底程序中的逻辑是与主库还是从库打交道。可能有其它的更便利的方案来完成这个判断，但我确实不知道，有知道的同学可以留言教教我。 不过，搭建这个环境的过程中还是收获不少呢（坑真多）！下面就分享给大家~ 首先，我在自己的工作机上来模拟环境，意味着我要在window10系统中来搭建整个环境，为了便利，我会使用docker来分别安装主从mysql节点。GG了一下相关的文章，还是相当多的，我选择了mysql官方提供的docker镜像，按照简书的这篇分享来完成主从设置。起初觉得一切都很顺利哟。然而一切都是tm的假象！ 由于我使用的是mysql官方镜像，它的设置mysql配置的方式和参考文章中的不太一致，我按照官方规范来自定义mysql配置文件： 1docker run --name mysql_master -v /d/atlas/master:/etc/mysql/conf.d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:latest 由于我用的是window10下的docker，所以我无法像教程中那样自然的把本地文件(/d/atlas/master)映射到容器里，注意我这里故意写的不是 系统用户路径，如果你上来就使用的系统用户路径，那你可能会绕过这个坑哟~，但我觉得谁都不会把项目代码直接丢在系统用户目录下吧？！ 无法映射，意味着mysql容器无法自定义，GG了一下，发现是因为win环境下需要手动在虚拟主机上做一下文件共享设置，详细步骤可以看segmentfault上的一个提问。 好了，第一个坑搞定了。 别高兴太早哟，我继续按照教程启动mysql容器，一切正常，但发现在主节点上执行show master status;这个命令无法看到结果，登录到容器内部执行service mysql status命令发现报警告： 1Warning: World-writable config file &apos;/etc/mysql/conf.d/config-file.cnf&apos; is ignored 原因是mysql的安全机制认为配置文件的权限分配的太大，直接将其忽略了。。。。继续GG，试了好几个方案都没有搞定。就在我决定切换到linux环境下从头再来的时候，在stackoverflow看到了一个简单粗暴的解决方案： 1234567//启动容器$ docker run --name mysql_slave -v /c/Users/youyou/atlas/slave:/kazaff -p 3307:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:latest// 进入容器内部执行下面命令cp /kazaff/config-file.cnf /etc/mysql/conf.d/ &amp;&amp; chmod 644 /etc/mysql/conf.d/config-file.cnf 这样我们就曲线救国式的创建了一个满足mysql安全要求的配置文件。第二个坑搞定了。 按照教程我执行START SLAVE命令，发现从库的状态始终是Slave_IO_Running=NO，查了一圈，网上各种解决方案，但我却不知道哪个适合我。我连上节点查看mysql的erro.log也没有看到对应的错误信息。最后竟然发现，执行SHOW SLAVE STATUS命令的结果里，其实包含了错误信息：Last_IO_Error，Last_SQL_Error。一看就明白了，原来我主从节点配置文件里的server_id写重复了（其实是因为从节点中错误导入了主节点的配置文件）。第三个坑搞定了。 继续按照教程做，直至结束。我们就得到了一个实时同步的主从环境。 接下来我们链接到从库上，执行： 123STOP SLAVE;CHANGE MASTER TO MASTER_DELAY=30;START SLAVE; 我们目的达到了，现在主从节点之间同步数据会有个30秒的延迟时间窗口，接下来我们就可以来搭建Atlas节点了。放在下一篇文章继续吧~~嘿嘿嘿","tags":[{"name":"dataloader","slug":"dataloader","permalink":"https://blog.kazaff.me/tags/dataloader/"},{"name":"resolver","slug":"resolver","permalink":"https://blog.kazaff.me/tags/resolver/"}]},{"title":"ali云ECS的一次迁移事故","date":"2018-04-03T10:37:12.000Z","path":"2018/04/03/ali云ECS的一次迁移事故/","text":"下午明明还有点瞌睡，可是被ali的ECS升级给我吓了一激灵。本以为会和AWS那样的无痛升级，就没多想立即响应了官方的要求。结果升级完以后一对事儿~快速把问题记录一下，如果你也碰到了，那就太。。。倒霉了。 SSH无法访问这次升级，官方给的介绍是会从ECS会从经典网络环境迁移到专用网络下。迁移后完成后，在ali云web后台可以看到ECS已经运行了，可SSH始终无法链接，跑去VPC安全组一看，默认阻塞所有的服务器入站请求，好吧，手动配置安全组规则后搞定。 ECS无法访问外网SSH可以登录服务器了，启动所有需要的服务后，发现服务器无法访问外网资源，curl也好，ping也好都统统不行。后来发现是系统的网卡DNS配置问题，改成8.8.8.8和1.1.1.1解决。 补充：如果使用上面提到的dns的话，我发现系统的yum完全无法使用了，所以只能更换成阿里提供的dns：100.100.2.136, 100.100.2.138 SSS无法启动我们服务器上部署着sss服务，用于工作使用，不知道sss的同学可以无视这段。启动失败是因为原先经典网络下的ECS是直接拥有2个网卡的，我们的SSS配置是直接绑定外网IP的。迁移到专用网络后，相当于ECS不再直接拥有那个外网ip。所以，只需要把sss配置绑定ecs的内网ip即可。","tags":[{"name":"VPC","slug":"VPC","permalink":"https://blog.kazaff.me/tags/VPC/"},{"name":"DNS","slug":"DNS","permalink":"https://blog.kazaff.me/tags/DNS/"},{"name":"SSS","slug":"SSS","permalink":"https://blog.kazaff.me/tags/SSS/"}]},{"title":"再品GraphQL","date":"2018-03-25T10:37:12.000Z","path":"2018/03/25/再品GraphQL/","text":"很久之前其实就关注过这个技术，记得当时还是React刚刚崭露头角的时期吧。总之那时候，GraphQL感觉还只是概念完备阶段，除了FB自己内部大量使用外，好像社区并不是很健全，不过大家应该都在疯狂的讨论和跟进吧。过了2年，如今再回过头来看，已经涌现出各种开源或商用服务专注于这个领域，各种语言的框架和工具也都很完备了，感觉是时候重新接触GraphQL了。如果你的项目正处于技术选型，你正在犹豫选择一种接口风格的时刻，不妨了解一下这个神奇而强大的玩意儿~~ 本文打算翻译一篇感觉很解惑的文章，主要围绕着GraphQL的server端实现，因为相比client端，server端包含了更多的内容。后面如果有机会，也会尝试提供关于client端相关的内容，不过前端同学可以先看一下这里：howtographql，这里有各种最佳实践，应该总会找到和你正在使用相关的前端框架的整合方案，好像有个对应的中文版~ 关于GraphQL概念的内容，这篇文章并没有涉及太多，不过假如你用搜索引擎去搜的话，相信有非常多的相关文章供你学习，这里就不再重复了~ 原文在这里，怀疑我翻译能力的同学可以去看原位哦~ 相信读完整个文章，对于GraphQL Server会有一个完整的了解。我们开始吧~ 下面开始正文：（备注，我并不打算逐字翻译全文，我也不确定这么做是否被授权，如果出了问题，就删帖吧:-(） 目录 目标 一切从Schema开始 创建一个简单的GraphQL服务端 GraphiQL，一个Graphql领域的postman 编写Resolvers 处理数据依赖关系 对接真正的数据库 1+N查询问题 管理自定义Scalar类型 错误处理 日志 认证 &amp; 中间件 Resolvers的单元测试 查询引擎的集成化测试 Resolvers拆分 组织Schemas 结语 目标我们的目标是针对一个移动app端界面显示所需要的数据，提供支撑，可以实现单一请求次数下就可以获取足够的数据。我们将会用Nodejs来完成这个任务，因为这个语言我们已经在marmelab用了4年了。但你也可以用任何你想用的语言，例如Ruby，Go，甚至PHP，JAVA或C#。 为了显示这个页面，服务端必须能提供下面的响应数据结构： 123456789101112131415161718192021222324252627282930313233&#123; \"data\": &#123; \"Tweets\": [ &#123; \"id\": 752, \"body\": \"consectetur adipisicing elit\", \"date\": \"2017-07-15T13:17:42.772Z\", \"Author\": &#123; \"username\": \"alang\", \"full_name\": \"Adrian Lang\", \"avatar_url\": \"http://avatar.acme.com/02ac660cdda7a52556faf332e80de6d8\" &#125; &#125;, &#123; \"id\": 123, \"body\": \"Lorem Ipsum dolor sit amet\", \"date\": \"2017-07-14T12:44:17.449Z\", \"Author\": &#123; \"username\": \"creilly17\", \"full_name\": \"Carole Reilly\", \"avatar_url\": \"http://avatar.acme.com/5be5ce9aba93c62ea7dcdc8abdd0b26b\" &#125; &#125;, // etc. ], \"User\": &#123; \"full_name\": \"John Doe\" &#125;, \"NotificationsMeta\": &#123; \"count\": 12 &#125; &#125;&#125; 我们需要模块化和可维护的代码，需要做单元测试，听起来这很难？你会发现借助于GraphQL工具链，这并不比开发Rest客户端难多少。 一切从Schema开始当我开发一个GraphQL服务时，我总会从在白板上设计模型开始，而不是上来就写代码。我会和产品和前端开发团队一起来讨论需要提供哪些数据类型，查询或更新操作。 如果你了解领域驱动设计方法，你会很熟悉这个流程。前端开发团队在拿到服务端返回的数据结构之前是没有办法开始编码的。所以我们需要先对API达成一致。 Tip命名很重要！不要觉得把时间花在为变量起名字上很浪费。特别是当这些名称会长期使用的时候 - 记住，GraphQL API并没有版本号这回事儿，所以，尽可能让你的Schema具有自解释特性，因为这是其他开发人员了解项目的入口。 下面是我为这个项目提供的GraphQL Schema： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556type Tweet &#123; id: ID! # The tweet text. No more than 140 characters! body: String # When the tweet was published date: Date # Who published the tweet Author: User # Views, retweets, likes, etc Stats: Stat&#125;type User &#123; id: ID! username: String first_name: String last_name: String full_name: String name: String @deprecated avatar_url: Url&#125;type Stat &#123; views: Int likes: Int retweets: Int responses: Int&#125;type Notification &#123; id: ID date: Date type: String&#125;type Meta &#123; count: Int&#125;scalar Urlscalar Datetype Query &#123; Tweet(id: ID!): Tweet Tweets(limit: Int, sortField: String, sortOrder: String): [Tweet] TweetsMeta: Meta User: User Notifications(limit: Int): [Notification] NotificationsMeta: Meta&#125;type Mutation &#123; createTweet(body: String): Tweet deleteTweet(id: ID!): Tweet markTweetRead(id: ID!): Boolean&#125; 我在这个系列的前一篇文章中简短的介绍了Schema的语法。你只需要知道，这里的type类似REST里的resources概念。你可以用它来定义拥有唯一id键的实体（如Tweet和User）。你也可以用它来定义值对象，这种类型嵌套在实体内部，因此不需要唯一键（例如Stat）。 Tip尽可能保证Type足够轻巧，然后利用组合。举个例子，尽管stats数据现在看来和tweet数据关系很近，但是请分开定义它们。因为它们表达的领域不同。这样当有天将stats数据换其它底层架构来维护，你就会庆幸今天做出的这个决定。 Query和Mutation关键字有特别的含义，它们用来定义API的入口。所以你不能声明一个自定义类型用这两个关键字 - 它们是GraphQL预留关键字。你可能会对Query下定义的字段有个困扰，它们总是和实体类型名字一样 - 但这只是个习惯约定。我就决定把获取Tweet类型数据的属性名称定义成getTweet - 记住，GraphQL是一种RPC（译者注：有别于RESTful的资源概念）。 官方GraphQL提供的schema文档提供了所有细节，花十分钟来了解一下对你定义自己的schema会很有帮助。 Tip你可能看过有些GraphQL教程使用代码风格来定义schema，例如GraphQLObjectType。别这么做，这种风格显得非常的啰嗦，也不够清晰。 创建一个简单的GraphQL服务端用Nodejs实现一个HTTP服务端最快的方式是使用express microframework。稍后我们会在http://localhost:4000/graphql下接入一个GraphQL服务。 1&gt; npm install express express-graphql graphql-tools graphql --save express-graphql库会基于我们定义的schema和resolver函数来创建一个graphQL服务。graphql-tools库提供了schema的解析和校验的独立包。这两个库前者是来自于Facebook，后者源于Apollo。 123456789101112131415161718// in src/index.jsconst fs = require('fs');const path = require('path');const express = require('express');const graphqlHTTP = require('express-graphql');const &#123; makeExecutableSchema &#125; = require('graphql-tools');const schemaFile = path.join(__dirname, 'schema.graphql');const typeDefs = fs.readFileSync(schemaFile, 'utf8');const schema = makeExecutableSchema(&#123; typeDefs &#125;);var app = express();app.use('/graphql', graphqlHTTP(&#123; schema: schema, graphiql: true,&#125;));app.listen(4000);console.log('Running a GraphQL API server at localhost:4000/graphql'); 执行下面命令来让我们的服务端跑起来： 12&gt; node src/index.jsRunning a GraphQL API server at localhost:4000/graphql 我们可以使用curl来简单请求一下我们的graphQL服务: 1234567&gt; curl &apos;http://localhost:4000/graphql&apos; \\&gt; -X POST \\&gt; -H &quot;Content-Type: application/graphql&quot; \\&gt; -d &quot;&#123; Tweet(id: 123) &#123; id &#125; &#125;&quot;&#123; &quot;data&quot;: &#123;&quot;Tweet&quot;:null&#125;&#125; 正常！ Graphql服务根据我们提供的schema定义，在执行请求携带的查询语句之前进行了必要的校验，如果我们的查询语句中包含了一个没有声明过的字段，我们会得到一个错误提醒： 123456789101112&gt; curl &apos;http://localhost:4000/graphql&apos; \\&gt; -X POST \\&gt; -H &quot;Content-Type: application/graphql&quot; \\&gt; -d &quot;&#123; Tweet(id: 123) &#123; foo &#125; &#125;&quot;&#123; &quot;errors&quot;: [ &#123; &quot;message&quot;: &quot;Cannot query field \\&quot;foo\\&quot; on type \\&quot;Tweet\\&quot;.&quot;, &quot;locations&quot;: [&#123;&quot;line&quot;:1,&quot;column&quot;:26&#125;] &#125; ]&#125; Tipexpress-graphql包生成的GraphQL服务端同时支持GET和POST请求。 Tip世界上还有一个不错的库可以让我们基于express，koa，HAPI或Restify来建立GraphQL服务：apollo-server。使用的方法和我们用的这个没有太多差异，所以这个教程同样适用。 GraphiQL，一个Graphql领域的postmancurl并不是一个很好用的工具来测试我们的GraphQL服务。我们使用GraphiQL来做可视化工具。可以把它想象成是Postman（译：用于测试Rest服务的工具，chrome app）。 因为我们在使用graphqlHTTP中间件时声明了graphiql参数，GraphiQL已经启动了。我们可以在浏览器访问http://localhost:4000/graphql就能看到Web界面了。它会从我们的服务中拿到完整的schema结构，并创建一个可视化的文档。可以点击页面右上角的Docs链接来查看： 有了它，我们的服务端就相当于有了自动化API文档生成功能，这就意味着我们不再需要Swagger啦~ Tip文档中每个类型和字段的解释来自于schema中的注释（以#为首的行）。尽可能提供注释，其它开发者会痛哭流涕的。 这还不是全部：使用schema，GraphiQL还提供了自动补全功能： 这种杀手级应用，每个Graphql开发者都值得拥有。对了，不要忘记在产品环境关闭掉它哟~ Tip你可以独立安装graphiQL工具，它基于Electron。跨平台的哦，下载链接 编写Resolvers到目前为止，我们的服务也只能返回空结果。我们这里会添加resolver定义来让它返回一些数据。我们先简单使用一些直接定义在代码里的静态数据来演示一下： 123456789101112const tweets = [ &#123; id: 1, body: 'Lorem Ipsum', date: new Date(), author_id: 10 &#125;, &#123; id: 2, body: 'Sic dolor amet', date: new Date(), author_id: 11 &#125;];const authors = [ &#123; id: 10, username: 'johndoe', first_name: 'John', last_name: 'Doe', avatar_url: 'acme.com/avatars/10' &#125;, &#123; id: 11, username: 'janedoe', first_name: 'Jane', last_name: 'Doe', avatar_url: 'acme.com/avatars/11' &#125;,];const stats = [ &#123; tweet_id: 1, views: 123, likes: 4, retweets: 1, responses: 0 &#125;, &#123; tweet_id: 2, views: 567, likes: 45, retweets: 63, responses: 6 &#125;]; 然后我们来告诉服务如何使用这些数据来处理Tweet和Tweets查询请求。下面列出了resover映射关系，这个对象按照schema的结构，为每个字段提供了一个函数： 1234567891011121314const resolvers = &#123; Query: &#123; Tweets: () =&gt; tweets, Tweet: (_, &#123; id &#125;) =&gt; tweets.find(tweet =&gt; tweet.id == id), &#125;, Tweet: &#123; id: tweet =&gt; tweet.id, body: tweet =&gt; tweet.body &#125;&#125;;// pass the resolver map as second argumentconst schema = makeExecutableSchema(&#123; typeDefs, resolvers &#125;);// proceed with the express app setup Tip官方express-graphql文档建议使用rootValue选项来代替使用makeExecutableSchema()。我不推荐这么做！ 这里resolver的函数签名是(previousValue, parameters) =&gt; data。目前已经足够我们的服务来完成基础查询了： 12345678910111213// query &#123; Tweets &#123; id body &#125; &#125;&#123; data: Tweets: [ &#123; id: 1, body: &apos;Lorem Ipsum&apos; &#125;, &#123; id: 2, body: &apos;Sic dolor amet&apos; &#125; ]&#125;// query &#123; Tweet(id: 2) &#123; id body &#125; &#125;&#123; data: Tweet: &#123; id: 2, body: &apos;Sic dolor amet&apos; &#125;&#125; 内部工作流程是这样的：服务会由外向内依次处理查询块，为每个查询块执行对应的resolver函数，并传递外层调用是的返回结果为第一个参数。所以，{ Tweet(id: 2) { id body } }这个查询的处理步骤为： 最外层为Tweet，对应的resolver为(Query.Tweet)。因为是最外层，所以调用resolver函数时第一个参数为null。第二个参数传递的是查询携带的参数{ id: 2 }。根据schema的定义，该resolver函数会返回满足条件的Tweet类型对象。 针对每个Tweet对象，服务会执行对应的(Tweet.id)和(Tweet.body)resolver函数。此时第一个参数为第一步得到的Tweet对象。 目前我们的Tweet.id和Tweet.bodyresolver函数非常的简单，事实上我根本不需要声明它们。GraphQL有一个简单的默认resolver来处理缺少对应定义的字段。 Mutation resolver的实现并不会难多少，如下： 123456789101112131415161718const resolvers = &#123; // ... Mutation: &#123; createTweet: (_, &#123; body &#125;) =&gt; &#123; const nextTweetId = tweets.reduce((id, tweet) =&gt; &#123; return Math.max(id, tweet.id); &#125;, -1) + 1; const newTweet = &#123; id: nextTweetId, date: new Date(), author_id: currentUserId, // &lt;= you'll have to deal with that body, &#125;; tweets.push(newTweet); return newTweet; &#125; &#125;,&#125;; Tip保持resolver函数的简洁。GraphQL通常扮演系统的API网关角色，对后端领域服务提供了一层薄薄封装。resolver应该只包含解析请求参数并生成返回数据要求的结构的功能 - 就好像MVC框架中的controller层。其它逻辑应该拆分到对应的层，这样我们就能保持GraphQL非侵入业务。 你可以在Apollo官网找到关于resolvers的完整文档。 处理数据依赖关系接下来，最有意思的部分要开始了。如何让我们的服务能支持复杂的聚合查询呢？如下： 12345678910111213&#123; Tweets &#123; id body Author &#123; username full_name &#125; Stats &#123; views &#125; &#125;&#125; 如果是在SQL语言，这可能需要对其它两个表的joins操作（User和Stat），其背后SQL执行器要运行复杂的逻辑来处理查询。在GraphQL中，我们只需要为Tweet类型添加合适的resolver函数即可： 12345678910111213141516const resolvers = &#123; Query: &#123; Tweets: () =&gt; tweets, Tweet: (_, &#123; id &#125;) =&gt; tweets.find(tweet =&gt; tweet.id == id), &#125;, Tweet: &#123; Author: (tweet) =&gt; authors.find(author =&gt; author.id == tweet.author_id), Stats: (tweet) =&gt; stats.find(stat =&gt; stat.tweet_id == tweet.id), &#125;, User: &#123; full_name: (author) =&gt; `$&#123;author.first_name&#125; $&#123;author.last_name&#125;` &#125;,&#125;;// pass the resolver map as second argumentconst schema = makeExecutableSchema(&#123; typeDefs, resolvers &#125;); 有了上面的resolvers，我们的服务就可以处理前面的查询并拿到期望的结果： 12345678910111213141516171819202122232425262728&#123; data: &#123; Tweets: [ &#123; id: 1, body: \"Lorem Ipsum\", Author: &#123; username: \"johndoe\", full_name: \"John Doe\" &#125;, Stats: &#123; views: 123 &#125; &#125;, &#123; id: 2, body: \"Sic dolor amet\", Author: &#123; username: \"janedoe\", full_name: \"Jane Doe\" &#125;, Stats: &#123; views: 567 &#125; &#125; ] &#125;&#125; 看到这个结果我不知道大家什么反映，反正我第一次被震到了，这简直是黑科技。凭什么这么简单的resolver函数就能让服务支持这么复杂的查询？ 我们再来看一下执行流程： 12345678910111213&#123; Tweets &#123; id body Author &#123; username full_name &#125; Stats &#123; views &#125; &#125;&#125; 对于最外层的Tweets查询块，GraphQL执行Query.Tweetsresolver，第一个参数为null。resolver函数返回Tweets数组。 针对数组中的每个Tweet，GraphQL并发的执行Tweet.id、Tweet.body、Tweet.Author和Tweet.Statsresolver函数。 注意这次我并没有提供关于Tweet.id和Tweet.body的resolver函数，GraphQL使用默认的resolver。对于Tweet.Authorresolver函数，会返回一个User类型的对象，这是schema中定义好的。 针对User类型数据，查询会并发的执行User.username和User.full_nameresolver，并传递上一步得到的Author对象作为第一个参数。 State处理同样会使用默认的resolver来解决。 所以，这就是GraphQL的核心，非常的酷炫。它可以处理复杂的多层嵌套查询。这就是为啥成它为Graph的原因吧，此刻你应该顿悟了吧？！啊哈~ 你可以在graphql.org网站找到关于GraphQL执行机制的描述。 对接真正的数据库在真实项目中，resolver需要和数据库或其它API打交道来获取数据。这和我们上面做的事儿没有本质不同，除了需要返回一个promises外。假如tweets和authors数据存储在PostgreSQL数据库，而Stats存储在MongoDB数据库，我们的resolver只要调整一下即可： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647const &#123; Client &#125; = require('pg');const MongoClient = require('mongodb').MongoClient;const resolvers = &#123; Query: &#123; Tweets: (_, __, context) =&gt; context.pgClient .query('SELECT * from tweets') .then(res =&gt; res.rows), Tweet: (_, &#123; id &#125;, context) =&gt; context.pgClient .query('SELECT * from tweets WHERE id = $1', [id]) .then(res =&gt; res.rows), User: (_, &#123; id &#125;, context) =&gt; context.pgClient .query('SELECT * from users WHERE id = $1', [id]) .then(res =&gt; res.rows), &#125;, Tweet: &#123; Author: (tweet, _, context) =&gt; context.pgClient .query('SELECT * from users WHERE id = $1', [tweet.author_id]) .then(res =&gt; res.rows), Stats: (tweet, _, context) =&gt; context.mongoClient .collection('stats') .find(&#123; 'tweet_id': tweet.id &#125;) .query('SELECT * from stats WHERE tweet_id = $1', [tweet.id]) .toArray(), &#125;, User: &#123; full_name: (author) =&gt; `$&#123;author.first_name&#125; $&#123;author.last_name&#125;` &#125;,&#125;;const schema = makeExecutableSchema(&#123; typeDefs, resolvers &#125;);const start = async () =&gt; &#123; // make database connections const pgClient = new Client('postgresql://localhost:3211/foo'); await pgClient.connect(); const mongoClient = await MongoClient.connect('mongodb://localhost:27017/bar'); var app = express(); app.use('/graphql', graphqlHTTP(&#123; schema: schema, graphiql: true, context: &#123; pgClient, mongoClient &#125;), &#125;)); app.listen(4000);&#125;;start(); 注意，由于我们的数据库操作只支持异步操作，所以我们需要改成promise写法。我把数据库链接句柄对象保存在GraphQL的context中，context会作为第三个参数传递给所有的resolver函数。tontext非常适合用来处理需要在多个resolver中共享的资源，有点类似其它框架中的注册表实例。 如你所见，我们很容易就做到从不同的数据源中聚合数据，客户端根本不知道数据来自于哪里 - 这一切都隐藏在resolver中。 1+N查询问题迭代查询语句块来调用对应的resolver函数确实聪明，但性能可能不太好。在我们的例子中，Tweet.Authorresolver被调用了多次，针对每个从Query.Tweetsresolve中得到的Tweet。所以我们请求了1次Tweets，结果产生了N次Tweet.Author查询。 为了解决这个问题，我使用了另外一个库：Dataloader，它也是Facebook提供的。 1npm install --save dataloader DataLoader是一个数据批量获取和缓存的工具库。首先我们会创建一个获取所有条目并返回promise的函数，然后我们为每个条目创建一个dataloader： 1234567const DataLoader = require('dataloader');const getUsersById = (ids) =&gt; pgClient .query(`SELECT * from users WHERE id = ANY($1::int[])`, [ids]) .then(res =&gt; res.rows);const dataloaders = () =&gt; (&#123; userById: new DataLoader(getUsersById),&#125;); userById.load(id)函数会收集多个单独的item调用，然后批量的获取一次。 Tip如果你不太熟悉PostgreSQL，WHERE id = ANY($1::int[])的语法就类似于WHERE id IN($1,$2,$3)。 我们把dataloader也保存在context中： 12345app.use('/graphql', graphqlHTTP(req =&gt; (&#123; schema: schema, graphiql: true, context: &#123; pgClient, mongoClient, dataloaders: dataloaders() &#125;,&#125;))); 现在我们只需要稍微修改一下Tweet.Authorresolver即可： 12345678const resolvers = &#123; // ... Tweet: &#123; Author: (tweet, _, context) =&gt; context.dataloaders.userById.load(tweet.author_id), &#125;, // ...&#125;; 大功搞成！现在{ Tweets { Author { username } }查询只会执行2次查询请求：一次用来获取Tweets数据，一次用来获取所有需要的Tweet.Author数据！ 你需要注意一个细节：在graphqlHTTP配置时，我传递进去的是一个函数(graphqlHTTP(req =&gt; ({ ... })))，而非之前的对象(graphqlHTTP({ ... }))。这是因为Dataloader实例还提供缓存功能，所以我需要确保所有请求使用的是同一个Dataloader对象。 但这次变动会导致前面的代码报错，因为pgClient在getUsersById函数的上下文中就不存在了。为了传递数据库链接句柄到dataloader中，这有点绕，看下面的代码： 12345678910111213const DataLoader = require('dataloader');const getUsersById = pgClient =&gt; ids =&gt; pgClient .query(`SELECT * from users WHERE id = ANY($1::int[])`, [ids]) .then(res =&gt; res.rows);const dataloaders = pgClient =&gt; (&#123; userById: new DataLoader(getUsersById(pgClient)),&#125;);// ...app.use('/graphql', graphqlHTTP(req =&gt; (&#123; schema: schema, graphiql: true, context: &#123; pgClient, mongoClient, dataloaders: dataloaders(pgClient) &#125;,&#125;))); 实际开发中，你可能不得不在所有的resolver函数中都使用dataloader，不管是否会查询数据库。这是产品环境下的必备啊，千万别错过它！ 管理自定义Scalar类型你可能注意到了我到现在为止都没有获取tweet.date数据，那是因为我在schema中定义了自定义的scalar类型： 123456type Tweet &#123; # ... date: Date&#125;scalar Date 不管你信不信，反正graphQL规范中并没有定义Date scalar类型，需要开发者自行实现。这算是个好的机会我们来演示一下创建自定义scalar类型，用来校验和类型转换数据。 和其他类型一样，scalar类型也需要resolver。但它的resolver函数必须支持将数据从其它resolver函数中转换为响应所需的格式，反之亦然： 123456789101112131415161718192021222324252627282930313233const &#123; GraphQLScalarType, GraphQLError &#125; = require(&apos;graphql&apos;);const &#123; Kind &#125; = require(&apos;graphql/language&apos;);const validateValue = value =&gt; &#123; if (isNaN(Date.parse(value))) &#123; throw new GraphQLError(`Query error: not a valid date`, [value]);&#125;;const resolvers = &#123; // previous resolvers // ... Date: new GraphQLScalarType(&#123; name: &apos;Date&apos;, description: &apos;Date type&apos;, parseValue(value) &#123; // value comes from the client, in variables validateValue(value); return new Date(value); // sent to resolvers &#125;, parseLiteral(ast) &#123; // value comes from the client, inlined in the query if (ast.kind !== Kind.STRING) &#123; throw new GraphQLError(`Query error: Can only parse dates strings, got a: $&#123;ast.kind&#125;`, [ast]); &#125; validateValue(ast.value); return new Date(ast.value); // sent to resolvers &#125;, serialize(value) &#123; // value comes from resolvers return value.toISOString(); // sent to the client &#125;, &#125;),&#125;; 错误处理正是因为咱们有schema，所有错误的查询请求都会被服务端捕获，并返回一个错误提醒： 1234567891011121314// query &#123; Tweets &#123; id body foo &#125; &#125;&#123; &quot;errors&quot;: [ &#123; &quot;message&quot;: &quot;Cannot query field \\&quot;foo\\&quot; on type \\&quot;Tweets\\&quot;.&quot;, &quot;locations&quot;: [ &#123; &quot;line&quot;: 1, &quot;column&quot;: 19 &#125; ] &#125; ]&#125; 这让调试变得易如反掌。客户端用户可以看到到底发生了什么事儿。 但这种在响应中显示错误信息的简单处理，并没有在服务端记录错误日志。为了帮助开发者跟踪异常，我在makeExecutableSchema中配置了logger参数，它必须传递一个拥有log方法的对象： 12345const schema = makeExecutableSchema(&#123; typeDefs, resolvers, logger: &#123; log: e =&gt; console.log(e) &#125;,&#125;); 如果你打算在响应中隐藏错误信息，可以使用graphql-errors包。 日志除了数据和错误外，graphQL的响应中还可以包含extensions类信息，你可以在其中放你想要的任何数据。我们用它来显示服务的耗时信息再好不过了。 为了添加扩展信息，我们需要在graphqlHTTP配置中添加extension函数，它返回一个支持json序列化的对象。 下面我添加了一个timing到响应中： 123456789app.use('/graphql', graphqlHTTP(req =&gt; &#123; const startTime = Date.now(); return &#123; // ... extensions: (&#123; document, variables, operationName, result &#125;) =&gt; (&#123; timing: Date.now() - startTime, &#125;) &#125;;&#125;))); 现在我们所有的graphQL响应中都会包含请求的耗时信息： 1234567// query &#123; Tweets &#123; id body &#125; &#125;&#123; &quot;data&quot;: [ ... ], &quot;extensions&quot;: &#123; &quot;timing&quot;: 53, &#125;&#125; 你可以按你的设想为你的resolver函数提供更细颗粒度的耗时信息。在产品环境下，监听每个后端响应耗时非常有意义。你可以参考apollo-tracing-js： 12345678910111213141516171819202122232425&#123; \"data\": &lt;&gt;, \"errors\": &lt;&gt;, \"extensions\": &#123; \"tracing\": &#123; \"version\": 1, \"startTime\": &lt;&gt;, \"endTime\": &lt;&gt;, \"duration\": &lt;&gt;, \"execution\": &#123; \"resolvers\": [ &#123; \"path\": [&lt;&gt;, ...], \"parentType\": &lt;&gt;, \"fieldName\": &lt;&gt;, \"returnType\": &lt;&gt;, \"startOffset\": &lt;&gt;, \"duration\": &lt;&gt;, &#125;, ... ] &#125; &#125; &#125;&#125; Apollo公司还提供一个叫Optics的GraphQL监控服务，不妨试试看。 认证 &amp; 中间件GraphQL规范中并没有包含认证授权相关的内容。这意味着你不得不自己来做，可以使用express对应的中间件库（你可能需要passport.js）。 一些教程推荐使用graphQL的Mutation来实现注册和登录功能，并且在resolver函数中实现认证逻辑。但我的观点是，这在多数场景中都显得过火了。 请记住，GraphQL只是一个API网关，它不应该处理太多的业务需求。（译：但很多成熟API网关服务都提供认证授权服务吧？！但不知为何我挺支持原作者的观点） Resolvers的单元测试resolver是简单函数，所以单元测试非常简单。在这篇教程里，我们会使用同样是Facebook提供的Jest，因为它基本上开箱即用： 1&gt; npm install jest --save-dev 让我们开始为之前写的resolver函数User.full_name来写个测试用例。为了能测试它，我们需要先把它单独拆分到自己的文件中： 12345678910111213// in src/user/resolvers.jsexports.User = &#123; full_name: (author) =&gt; `$&#123;author.first_name&#125; $&#123;author.last_name&#125;`,&#125;;// in src/index.jsconst User = require('./resolvers/User');const resolvers = &#123; // ... User,&#125;;const schema = makeExecutableSchema(&#123; typeDefs, resolvers &#125;);// ... 现在就可以对它写测试用例了： 123456789// in src/user/resolvers.spec.jsconst &#123; User &#125; = require('./resolvers');describe('User.full_name', () =&gt; &#123; it('concatenates first and last name', () =&gt; &#123; const user = &#123; first_name: 'John', last_name: 'Doe' &#125;; expect(User.full_name(user)).toEqual('John Doe') &#125;);&#125;) 运行./node_modules/.bin/jest,然后就可以看到终端显示的测试结果了。 那些和数据库打交道的resolver测试起来可能稍微麻烦一些。不过因为context会被当做参数，我们利用它来传入测试数据集也没什么难的。如下： 12345678910111213141516171819202122232425262728// in src/tweet/resolvers.jsexports.Query = &#123; Tweets: (_, _, context) =&gt; context.pgClient .query('SELECT * from tweets') .then(res =&gt; res.rows),&#125;;// in src/tweet/resolvers.spec.jsconst &#123; Query &#125; = require('./resolvers');describe('Query.Tweets', () =&gt; &#123; it('returns all tweets', () =&gt; &#123; const queryStub = q =&gt; &#123; if (q == 'SELECT * from tweets') &#123; return Promise.resolve(&#123; rows: [ &#123; id: 1, body: 'hello' &#125;, &#123; id: 2, body: 'world' &#125;, ]&#125;); &#125; &#125;; const context = &#123; pgClient: &#123; query: queryStub &#125; &#125;; return Query.Tweets(null, null, context).then(results =&gt; &#123; expect(results).toEqual([ &#123; id: 1, body: 'hello' &#125; &#123; id: 2, body: 'world' &#125; ]); &#125;); &#125;);&#125;) 注意这里依然需要返回一个promise，并且将断言语句放在then()回调中。这样Jest会知道是异步测试。我们刚才是手动编写测试数据的，在真实产品中，你可能需要一个专业的类库来帮忙：Sinon.js。 如你所见，测试resolver就是这么小菜一碟。把resolver定位为一个纯函数，是GraphQL设计者们的另一个明智之举。 查询引擎的集成化测试那么，如何来测试数据依赖，类型和聚合逻辑呢？这是另一种类型的测试，一般叫集成测试，需要在查询引擎上跑。 这需要我们运行一个http server来进行继承测试么？然而并不是。你可以单独对查询引擎进行测试而不需要跑一个服务，使用graphql工具即可。 在集成测试之前，我们需要调整一下代码结构： 1234567891011121314151617181920212223// in src/schema.jsconst fs = require('fs');const path = require('path');const &#123; makeExecutableSchema &#125; = require('graphql-tools');const resolvers = require('../resolvers'); // extracted from the express appconst schemaFile = path.join(__dirname, './schema.graphql');const typeDefs = fs.readFileSync(schemaFile, 'utf8');module.exports = makeExecutableSchema(&#123; typeDefs, resolvers &#125;);// in src/index.jsconst express = require('express');const graphqlHTTP = require('express-graphql');const schema = require('./schema');var app = express();app.use('/graphql', graphqlHTTP(&#123; schema, graphiql: true,&#125;));app.listen(4000);console.log('Running a GraphQL API server at localhost:4000/graphql'); 现在我就可以单独的测试schema： 1234567891011121314151617181920212223242526272829303132333435363738394041// in src/schema.spec.jsconst &#123; graphql &#125; = require(&apos;graphql&apos;);const schema = require(&apos;./schema&apos;);it(&apos;responds to the Tweets query&apos;, () =&gt; &#123; // stubs const queryStub = q =&gt; &#123; if (q == &apos;SELECT * from tweets&apos;) &#123; return Promise.resolve(&#123; rows: [ &#123; id: 1, body: &apos;Lorem Ipsum&apos;, date: new Date(), author_id: 10 &#125;, &#123; id: 2, body: &apos;Sic dolor amet&apos;, date: new Date(), author_id: 11 &#125; ]&#125;); &#125; &#125;; const dataloaders = &#123; userById: &#123; load: id =&gt; &#123; if (id == 10 ) &#123; return Promise.resolve(&#123; id: 10, username: &apos;johndoe&apos;, first_name: &apos;John&apos;, last_name: &apos;Doe&apos;, avatar_url: &apos;acme.com/avatars/10&apos; &#125;); &#125; if (id == 11 ) &#123; return Promise.resolve(&#123; &#123; id: 11, username: &apos;janedoe&apos;, first_name: &apos;Jane&apos;, last_name: &apos;Doe&apos;, avatar_url: &apos;acme.com/avatars/11&apos; &#125;); &#125; &#125; &#125; &#125;; const context = &#123; pgClient: &#123; query: queryStub &#125;, dataloaders &#125;; // now onto the test itself const query = &apos;&#123; Tweets &#123; id body Author &#123; username &#125; &#125;&#125;&apos;; return graphql(schema, query, null, context).then(results =&gt; &#123; expect(results).toEqual(&#123; data: &#123; Tweets: [ &#123; id: &apos;1&apos;, body: &apos;hello&apos;, Author: &#123; username: &apos;johndoe&apos; &#125; &#125;, &#123; id: &apos;2&apos;, body: &apos;world&apos;, Author: &#123; username: &apos;janedoe&apos; &#125; &#125;, ], &#125;, &#125;); &#125;);&#125;) 这个独立的graphql查询引擎的api方法签名是(schema, query, rootValue, context) =&gt; Promise，（文档）。很简单对吧？顺便说一句，graphqlHTTP内部就是调用它来工作的。 另一种Apollo公司比较推荐的测试手段是使用来自graphql-tools中的mockServer来测试。基于文本化的schema，它会创建一个内存数据源，并填充伪造的数据。你可以在这个教程中看到详细步骤。然而我并不推荐这种方式 - 它更像是一个前端开发者的工具，用来模拟GraphQL服务，而不是用来测试resolver。 Resolvers拆分为了能测试resolver和查询引擎，我们不得不把代码拆分到多个独立的文件中。从开发者角度来看这是一个值得的工作 - 它提供了模块化和可维护性。让我们完成所有resolver的模块化拆分。 1234567891011121314151617181920212223242526272829// in src/tweet/resolvers.jsexport const Query = &#123; Tweets: (_, _, context) =&gt; context.pgClient .query('SELECT * from tweets') .then(res =&gt; res.rows), Tweet: (_, &#123; id &#125;, context) =&gt; context.pgClient .query('SELECT * from tweets WHERE id = $1', [id]) .then(res =&gt; res.rows),&#125;export const Mutation = &#123; createTweet: (_, &#123; body &#125;, context) =&gt; context.pgClient .query('INSERT INTO tweets (date, author_id, body) VALUES ($1, $2, $3) RETURNING *', [new Date(), currentUserId, body]) .then(res =&gt; res.rows[0]) &#125;,&#125;export const Tweet = &#123; Author: (tweet, _, context) =&gt; context.dataloaders.userById.load(tweet.author_id), Stats: (tweet, _, context) =&gt; context.dataloaders.statForTweet.load(tweet.id),&#125;,// in src/user/resolvers.jsexport const Query = &#123; User: (_, &#123; id &#125;, context) =&gt; context.pgClient .query('SELECT * from users WHERE id = $1', [id]) .then(res =&gt; res.rows),&#125;;export const User = &#123; full_name: (author) =&gt; `$&#123;author.first_name&#125; $&#123;author.last_name&#125;`,&#125;; 然后我们需要在一个地方合并所有的resolver： 1234567891011121314// in src/resolversconst &#123; Query: TweetQuery, Mutation: TweetMutation, Tweet,&#125; = require('./tweet/resolvers');const &#123; Query: UserQuery, User &#125; = require('./user/resolvers');module.exports = &#123; Query: Object.assign(&#123;&#125;, TweetQuery, UserQuery), Mutation: Object.assign(&#123;&#125;, TweetMutation), Tweet, User,&#125; 就是这样！现在，模块化拆分后的代码结构，更适合理解和测试。 组织SchemasResolvers现在已经结构化了，但是schema呢？把所有定义都放在一个文件中一听就不是个好设计。尤其是对一些大项目，这会导致根本无法维护。就像resolver那样，我也会把schema拆分到多个独立的文件中。下面是我推荐的项目文件结构，靠模块思想来搭建： 12345678910111213src/ stat/ resolvers.js schema.js tweet/ resolvers.js schema.js user/ resolvers.js schema.js base.js resolvers.js schema.js base.js文件中包含了schema的基础类型，和空的query和mutation类型声明 - 其它片段schema文件会增加对应的字段到其中。 123456789101112131415161718// in src/base.jsconst Base = `type Query &#123; dummy: Boolean&#125;type Mutation &#123; dummy: Boolean&#125;type Meta &#123; count: Int&#125;scalar Urlscalar Date`;module.exports = () =&gt; [Base]; 由于GraphQL不支持空的类型，所以我们不得不声明一个看起来毫无意义的query和mutation。注意，文件最后导出的是一个数组而非字符串。后面你就会知道是为啥了。 现在，在User schema声明文件中，我们如何添加字段到已经存在的query类型中？使用graphql关键字extend： 12345678910111213141516171819// in src/user/schema.jsconst Base = require('../base');const User = `extend type Query &#123; User: User&#125;type User &#123; id: ID! username: String first_name: String last_name: String full_name: String name: String @deprecated avatar_url: Url&#125;`;module.exports = () =&gt; [User, Base]; 正如你看到的，代码最后并没有只是导出User，也导出了它所以来的Base。我就是靠这种方法来确保makeExecutableSchema能拿到所有的类型定义。这就是为啥我总是导出数组的原因，快夸我。 Stat类型也没有什么特殊的： 1234567891011// in src/stat/schema.jsconst Stat = `type Stat &#123; views: Int likes: Int retweets: Int responses: Int&#125;`;module.exports = () =&gt; [Stat]; Tweet类型依赖多个其它类型，所以我们要导入所有依赖的类型定义，并最终全部导出： 123456789101112131415161718192021222324252627282930// in src/tweet/schema.jsconst User = require('../user/schema');const Stat = require('../stat/schema');const Base = require('../base');const Tweet = `extend type Query &#123; Tweet(id: ID!): Tweet Tweets(limit: Int, sortField: String, sortOrder: String): [Tweet] TweetsMeta: Meta&#125;extend type Mutation &#123; createTweet (body: String): Tweet deleteTweet(id: ID!): Tweet markTweetRead(id: ID!): Boolean&#125;type Tweet &#123; id: ID! # The tweet text. No more than 140 characters! body: String # When the tweet was published date: Date # Who published the tweet Author: User # Views, retweets, likes, etc Stats: Stat&#125;`;module.exports = () =&gt; [Tweet, User, Stat, Base]; 最后，确保所有类型都在主schema.js文件中，我简单的传递一个typeDefs数组： 12345678910111213141516// in schema.jsconst Base = require('./base.graphql');const Tweet = require('./tweet/schema');const User = require('../user/schema');const Stat = require('../stat/schema');const resolvers = require('./resolvers');module.exports = makeExecutableSchema(&#123; typeDefs: [ ...Base, ...Tweet, ...User, ...Stat, ], resolvers,&#125;); 不需要担心类型重叠问题。每个类型makeExecutableSchema只会接受一次。 Tip子schema导出一个函数而不是一个数组，是因为它要确保不会发生环形依赖问题。makeExecutableSchema函数支持传递数组和函数参数。 结语我们的服务端现在已经搞出来了，并且也进行了测试。是时候放松一下了！你可以从Github上下载这个教程的完整代码。欢迎使用它来作为你新项目的脚手架。 其实还有一些我没有提到的关于服务端GraphQL开发的细节： 安全：客户端可以随意的创建复杂查询，这就增加了服务风险，例如被DoS攻击。可以看一下这篇文章：HowToGraphQL: GraphQL Security 订阅：很多教程使用WebSocket，可以阅读HowToGraphQL: Subscriptions或 Apollo: Server-Side Subscriptions来了解更多细节 输入类型：对于mutations，GraphQL支持有限的输入类型。可以从Apollo: GraphQL Input Types And Client Caching了解更多细节 Persisted Queries：这个主题会在后续的文章中涉及。 注意：这篇教程中提到的大多数js库都源自Facebook或Apollo。那么，Apollo到底是哪位？它是来自于Meteor团队的一个项目。这些家伙为GraphQL贡献了很多的高质量代码。顶他们！但他们同时也靠售卖GraphQL相关服务来盈利，所以在盲目遵从他们提供的教程之前，你最好能有个备选方案。 开发一个GraphQL服务端需要比REST服务端更多的工作，但同样你也会得到加倍的回报。如果你在读这篇教程的时候被太多名词给吓到失禁，先别慌着擦，你回忆一下当初你学RESTful的时候（URIs，HTTP return code，JSON Schema，HATEOAS），但现在你已经是一个REST开发者了。我觉得多花一两天你也就掌握GraphQL了。这是非常值得投资的。 警告：这个技术依然很年轻，并没有什么权威的最佳时间。我这里分享的只是我个人的积累。在我学习的过程中我看过大量的过时的教程，因为这门技术在不停的发展和进化。我希望这篇教程不会那么快就过时！","tags":[{"name":"dataloader","slug":"dataloader","permalink":"https://blog.kazaff.me/tags/dataloader/"},{"name":"resolver","slug":"resolver","permalink":"https://blog.kazaff.me/tags/resolver/"}]},{"title":"Golang下http的一个怪问题","date":"2018-03-19T10:37:12.000Z","path":"2018/03/19/golang下http的一个怪问题/","text":"最近用golang的github.com/gorilla/mux库来实现一个简单的http server。在实际测试的时候发现了一些问题，大多数都是因为自己不熟悉导致的，不过其中一个我觉得值得拿来分享一下。 大概情况是这样的，我计划实现一个简单的服务，接受post请求，并解析请求携带的json数据。一切都是这么的自然，和简单。不过部署测试的时候蒙了，客户端代码提交上来的请求，虽然正确的触发了服务端绑定的函数，但请求携带的json数据丢了。。更费解的是，用postman模拟提交相同的数据，就完全正常？！ 这就很奇怪了，debug了一下，发现原来客户端代码和postman提交请求存在一个不同点，因为客户端代码请求时url是拼接出来的，导致了一个问题： 12代码用的url： http://127.0.0.1//api/hola (注意这里出现了两个//)postman用的url：http://127.0.0.1/api/hola 当然，postman的url才是我们希望使用的。修正这个后就一切恢复了正常！ 这一点如果放在其它语言写的服务端中，是完全没有问题的。勾起了我的好奇心~翻了一下源码，发现在mux.go定义的方法里有一个逻辑，注意下面代码中的第三个if判断，它会格式化请求时使用的地址，并尝试格式化，一旦它对url进行了任何形式的格式化，代码就会返回客户端一个301重定向响应，而这次重定向就导致了 为何触发了服务端绑定的正确函数，但请求数据丢了 。 12345678910111213141516171819...func (r *Router) ServeHTTP(w http.ResponseWriter, req *http.Request) &#123; if !r.skipClean &#123; path := req.URL.Path if r.useEncodedPath &#123; path = req.URL.EscapedPath() &#125; if p := cleanPath(path); p != path &#123; url := *req.URL url.Path = p p = url.String() w.Header().Set(\"Location\", p) w.WriteHeader(http.StatusMovedPermanently) return &#125; &#125; ... 说了这么多，为了避免这个问题，其实可以在初始化router时，设置一个开关，即可避免这个坑： 1router := mux.NewRouter().StrictSlash(true).SkipClean(true) 我不确定这个逻辑是不是还用于匹配其它场景，如果不能简单的设置这个标识位，那也可以在客户端代码中拼接url时做足够多的逻辑判断避免发生在我身上的问题。 祝好运~","tags":[{"name":"http","slug":"http","permalink":"https://blog.kazaff.me/tags/http/"},{"name":"rewrite","slug":"rewrite","permalink":"https://blog.kazaff.me/tags/rewrite/"},{"name":"body","slug":"body","permalink":"https://blog.kazaff.me/tags/body/"}]},{"title":"Golang下的mysql坑","date":"2018-03-17T10:37:12.000Z","path":"2018/03/17/golang下的mysql坑/","text":"初学golang，感觉一切都很新鲜。直入正题，简单分享一下项目中碰到的几个小坑。 关于sql语句占位符按照大量文档上的样板代码，一般我们查询mysql，会这么写： 12name := \"kz\"row := db.QueryRow(\"SELECT * FROM users WHERE name=?\", name) 需要注意的是，name的类型为string，所以golang的mysql库在替换占位符?时，会自动增加引号在值两边（防止注入）。所以你是不需要在sql语句中自己增加引号的。详情可以看源码github.com\\go-sql-driver\\mysql\\connection.go的大概287行： 12345678910...case string: buf = append(buf, '\\'') if mc.status&amp;statusNoBackslashEscapes == 0 &#123; buf = escapeStringBackslash(buf, v) &#125; else &#123; buf = escapeStringQuotes(buf, v) &#125; buf = append(buf, '\\'')... 但是，如果你想这么查询mysql，就有点悲剧了：&quot;SELECT * FROM users WHERE id NOT IN(?)，我们希望用一个拼接的字符串来替换占位符，例如“1,2,3”，由于我们拼接的字符串是动态的，此时只能放弃使用占位符了，老老实实的字符串拼接sql吧。。 sql执行的timeout问题在我的场景里，是存在写锁（SELECT FOR UPDATE）的，实现的过程中发现要想让查询能快速从阻塞中退出，首先想到的是修改session的innodb_lock_wait_timeout外，但由于golang的mysql库是自带连接池的，这就意味着我们要记得修改回默认值，这件事太麻烦了。其次，打算自己来实现一个timeout channel来辅助恢复阻塞，但这样的话需要了解一下mysql库内部的机制，因为我担心虽然我靠自己的通道拿到了控制权，但数据库链接依然阻塞，这样后续的针对这个链接的操作依然无法执行（未求证）。 在寻找答案的过程中，发现了Context概念，而mysql库目前也支持了利用这个机制来实现timeout，代码如：123456newCtx, cancel := context.WithTimeout(context.Background(), 3*time.Second)defer cancel()row := tx.QueryRowContext(newCtx,\"SELECT * FROM users WHERE name=? FOR UPDATE\", \"kz\")if err != nil &#123; // 锁申请超时，放弃本次执行，稍后重新尝试&#125; 不过，在实际使用的时候，由于库设计者的哲学，该超时的链接还是会放回连接池中，这会导致下次从池中获取可用连接时，可能会取出这个已经超时的链接，不过mysql库会自从重新创建一个新的可用链接，并在终端打印出一条警示：driver: bad connection，源码如下：1234567891011121314func (db *DB) BeginTx(ctx context.Context, opts *TxOptions) (*Tx, error) &#123; var tx *Tx var err error for i := 0; i &lt; maxBadConnRetries; i++ &#123; tx, err = db.begin(ctx, opts, cachedOrNewConn) if err != driver.ErrBadConn &#123; break &#125; &#125; if err == driver.ErrBadConn &#123; return db.begin(ctx, opts, alwaysNewConn) &#125; return tx, err&#125; 可以看到，该警示无害，库总会无条件申请一个新的可用链接~ 今天的分享就到这里，88","tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.kazaff.me/tags/mysql/"},{"name":"timeout","slug":"timeout","permalink":"https://blog.kazaff.me/tags/timeout/"},{"name":"query-placeholder","slug":"query-placeholder","permalink":"https://blog.kazaff.me/tags/query-placeholder/"}]},{"title":"Mysql获取锁超时方案","date":"2018-03-09T10:37:12.000Z","path":"2018/03/09/mysql下的获取锁超时/","text":"今天聊一个老话题，这个在Oracle直接就支持，而对我这种mysql之辈，可能就比较头疼了。老规矩，先来描述一下问题背景： 我们在很多需要解决并发修改db的业务场景中，通常会依赖mysql的Innodb引擎的表来做事务及行级锁。目的就是为了确保并发操作数据问题。在web项目中，几乎任何时候都应该考虑这个问题，毕竟每个来自前端的请求都是并发的被处理（当然你可能在业务代码中做了锁）。考虑到锁的性能问题，有很多高明的设计则是尽可能保证在并发的流程中避免并发数据修改（Netty, kafka）。 web请求这个例子，不是很凸显问题，让我们换个业务场景来继续讨论：假如你电商平台需要实现在用户注册后发送邮件通知，这是一个很常见的需求。一般简单的做法是在db中创建用户条目后，程序调用邮件发送逻辑。处于性能和保证消息高可达的目的，可能会引入消息中间件，来异步处理这个逻辑。但假如程序在向消息中间件投递任务之前宕机了，如何确保邮件一定会投递？这涉及到 分布式事务 ，降低难度后我们也至少要保证任务至少投递一次。 说了这么多，不知道我在讲什么？别着急骂街，原谅我好久没写技术文章，有点生疏。其实我的意思，再不引入复杂的分布式事务实现的情况下，我们可能需要借助db来做数据一致性问题。大概方案就是，将用户信息插入和邮件提醒任务放在一个事务中插入到db（确保任务不丢失），再由一个定时任务（或监听进程）来解决邮件补发消息队列的问题。这听起来似乎并不是什么好方案（挺麻烦）。 下面来说说这个定时任务的逻辑，它需要频繁的去查邮件提醒任务数据表，获取到待投递的任务后将其投递到消息中间件。之前提到了，允许邮件重发，所以在定时任务进程中我们只需要保证数据的最终以执行即可。 接下来是今天的主题，我们需要获取邮件提醒表中的数据时，避免并发操作（假设这个定时任务是多线程，在获取到提醒任务后还要做一系列操作），在获取每一条数据时，都使用select for update来添加一个写锁。每当线程完成消息投递后则删除对应条目，并重新去数据表中获取新的待处理数据。为了避免在获取新数据时由于锁冲突而导致线程阻塞，我们希望能使用nowait特性，但Mysql不支持（好像记得新版本支持了），我们有什么其它方案呢？（终于把预备内容交代完毕了） 在GG上一顿狂搜后，找到了innodb_lock_wait_timeout这个设置项：here 不难理解，只需要如此使用：12345BEGIN;SET innodb_lock_wait_timeout=1;SELECT * FROM test WHERE id=1 FOR UPDATE;....COMMIT; 这样当多个线程同时试图获取id=1的数据时，除了获得锁的线程外，其它线程都会在阻塞1秒后自动报异常：12错误代码： 1205Lock wait timeout exceeded; try restarting transaction 其实mysql默认也是设置了获取锁超时的，只是默认值比较大(50秒)，可以通过执行SHOW GLOBAL VARIABLES LIKE &#39;innodb_lock_wait_timeout&#39;来获取默认配置。 当然，你也可以在程序中实现超时机制来避免线程阻塞。看你的喜好啦~","tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.kazaff.me/tags/mysql/"},{"name":"select for update","slug":"select-for-update","permalink":"https://blog.kazaff.me/tags/select-for-update/"},{"name":"锁超时","slug":"锁超时","permalink":"https://blog.kazaff.me/tags/锁超时/"}]},{"title":"2018的节","date":"2018-02-14T09:37:12.000Z","path":"2018/02/14/2018的节/","text":"Long time no update.好多原因，主要是并没有什么值得分享的，惭愧~还有一个小时左右，公司的春节假期就正式启动了，未来7天属于自己。 随着年龄的增加，很多之前鄙视的，不信的，也都一一应验，例如中年危机，例如大龄程序员专业，例如。。。这些问题最近一直困扰着自己，所以浮躁且迷茫~ 不过，人生就是如此，懂得取舍，懂得感恩，挺过人生道路上每一个必经的困惑，这个过程本身就是有价值的。我觉得，如果积累足够的思考，能够在下一代遇到相同困境的时候给予一个有意义的建议，是一件很酷的事儿~ 去年许的愿望，吹的牛逼，多数都没有兑现，就连我老婆也看出来我迷失了很久。少了几年前的激情，同时又多了许多新的错综复杂的纠结，2017年的后半年对我和我的小家庭来说确实不顺利。不过，这又能算什么呢？总还能瘫在沙发上吃鸡，就已经应该懂得知足感恩了~ 18年的目标是什么呢？为了不打脸，还是不说出来了哈。","tags":[{"name":"春节","slug":"春节","permalink":"https://blog.kazaff.me/tags/春节/"},{"name":"情人节","slug":"情人节","permalink":"https://blog.kazaff.me/tags/情人节/"}]},{"title":"迷思","date":"2017-11-29T09:37:00.000Z","path":"2017/11/29/迷思/","text":"序还差一个月，2017就要say boodbye了。这一年的我，成长有限，这一点从我博客的更新频度上也不难发现。一个33岁的小屁孩儿，似乎有那么点儿资格迷茫，也有理由颓废了。 失去了25岁时的棱角，就连写出的文字，都无法再自信的使用叹号了。不过，内心仍抱有理想，依然对美好的事物心存向往。只是，多了诸多现实带过我的约束。 前两周，工作的原因，飞了一趟资本主义的天堂–美国。虽然不至于像什么山炮进城，但很多方面还是在自己内心里激起了涟漪：原来调整时差真的如此痛苦，原来热水和中餐对我来说真的那么重要，原来有信仰的人不都是存在明显缺陷的。。。 哪怕是短暂的在一个全新的环境下生活，都很容易让一个人暴露出人格上的缺陷，或自大，或自卑。每个人可能都无法看透自我，但总能看穿他人，所以从别人的身上，找到自我鄙视的那一面，也是一种有深意的修行。我不知道是什么神秘力量让自己有勇气写一些文字来剖析自己，也许这种错觉也是一种自以为是吧。 信仰你一旦离开了中国，你就不再是大多数。当我坐在一家美国的中餐馆，看到邻桌坐着的几个外国人，还没来得及细细打量，脑中突然意识到，在这里，我才是外国人。这种恍然大悟的尴尬，让同行的人觉得可笑。 工作之余，大家聊到很多，从马云，到南京大屠杀，最终聊到了信仰这个话题。之前曾经有机会和对佛学有研究的牛人聊过一次，他们对佛家经书的参悟，绝对不仅仅是那种浅显的盲从。而这次，同样让我觉得，他们信的好像也并非是我轻易就可以识破的。从哪里来，到哪里去这样的哲学问题，在他们看来似乎从来都是毋庸置疑的。可能是身处异乡的缘故吧（在家里的话我确信我绝对是不会有所行动的），原本对这些就有好奇心的我，突然决定尝试花一些时间和精力去了解一下他们口中的上帝了。 我想以一个旁观者的视角，以一个中立的态度，去了解一下这个上帝的一些神迹，不论真伪，不管对错，仅仅是了解更多的细节，哪怕仅仅是为了日后的闲聊能有所谈资。于是翻起别人送的圣经，硬着头皮读了个开头，并不得其所。娃哈哈，这种结果并不难预料，所以找了一本简单一点的读物来了解圣经里的故事。读了几天，感觉把这些故事当作犹太人的历史来理解，似乎可以看到人类的共性。 有所得，就觉得很开心。希望读完后能得到我想要的答案。 属于世界的文化不管是在纽约市区中或高大或古老的建筑缝隙间，还是在爱迪生城市的某个稀疏的社区小路旁，我都很轻易的可以看到各种肤色各种民族的人，虽然大家彼此没有任何眼神或肢体上的互动，但还是能感到一种友善。这种感觉，是在你身为大多数派的时候，很难察觉的。这可能是最主要的原因吧，毕竟在这个移民国度里，谁都是小众，谁都是个体，时刻谦卑或许才能生存。 另一面，也是这种内在的独立感，更加容易让人接受多元化。如果意识不到这一点，我觉得很难生活在这里。在中国，我们只要保证站在大多数派的队伍里，就可以安心的活下去，甚至可以对别人的独特指指点点。所以我们想尽办法迎合大众，活的失去自我。 我对自己说，如果有一天，我走在街上，对面走来的任何人类，都不会让我潜意识觉得是外国人，也不会觉得自己是外国人，这时候才说明我有能力生活在这个环境里。否则，我就不会是我，我就无法找到自我，就会下意识的披上一层或虚伪或过度谦卑的表皮。这也可能是这段时间异常疲惫的原因吧~~ 说不出到底什么是好什么是差，我不觉得中国就不好（除了环境），我也说不出美国这种移民国度的好，但内心还是多少有些向往这样的生活，可能是出于对未知事物的好奇，也可能是崇洋媚外，但我宁愿相信自己欣赏的，是那种全人类无国度的共和形态。 人与人之间的台阶当两个陌生人在旅途中偶遇，由于没有任何的利益往来，所以即便是在了解到对方的背景之后，优越的一方即便是在自大，劣势的一方也可以轻易的选择终止与对方的往来而寻求一种自我保护，亦或称之为反击。 但，若存在一种利益往来，让劣势的一方不得不保持这种沟通的通畅，此时即便是优越的一方非常的谦逊，谁有能保证劣势的一方表现的可以恰到好处，而不是过于奉承或自卑呢？这可能算是一门学问吧。 就好比站在不同的台阶上，人总不得不仰视或俯视对方，如何让这种情形带来的伤害降到最低，我感觉可以作为衡量一个人情商的水准。之所以有这种感悟，多半是因为看到随行的同事在和美国同事沟通时的言行，让我不自觉的有些厌恶。我不知道在别人眼里，自己是否也是如此，但只是这种方式是我个人非常鄙视的。 不过，能发现自己厌恶的东西，且自我反省，最终修正自身的恶习，这个过程本身就是一种正能量。 业务价值谈到这个话题，其实是此行给我带来最有感触的一方面。曾经刚入行的我，满脑子充斥着编程语法、设计模式、框架和算法等等很IT的东西。曾经向往成为平台架构师，对基础架构、系统中间件和底层基础设施实现充满了幻想。在做程序员的头几年，大部分时间不是花在写代码，就是在阅读这方面的资料了，成长很快。 尽管这个阶段在我回忆中是非常充实的，但那时的我，面对IT圈终极哲学问题，自己的内心也是毫无头绪的：写代码能写到几岁？或者说30岁的程序员都必须转行管理了？ 尽管在公开场合对这种观点痛斥过，鄙视过。但内心身处，更多的是迷茫和恐慌。我相信，所有年轻的程序猿有这样的挣扎，毕竟我们从代码中得到太多太多，所以让我们“背叛”这种，总是会纠结的。 但是不得否认的是，很多情况下，价值总附着在能为人带来直接利益的东西上。单纯的编程语言，或一个对客户来说虚无缥缈的软件架构，更不要说那些让所有人头疼的算法或设计模式了，这些东西可能太深奥，或者太间接，总很难被直接换算成价值。（这段文字可能存在争议，但争议应该只源于我不及格的表达能力上，论点本身应该是毫无争议的） 那，作为一个即将步入大龄程序员阵营的我来说，是否写下这片文章的同时，就意味着我彻底投降，这是否又一次应征了那个IT怪圈？如果只看表象，似乎就是这样的。 不过，我的这种观点的改变，并不是毫无理由的。如果你和我一样是个悲观主义者，我相信你和我一样脑子里时常总是存在一种莫名的危机感，即便是你掌握了再多的编程语言，懂得再多的开发经验，还是不踏实，毕竟技术更新迭代太快了，毕竟人总是会遗忘，毕竟。。。我不想再罗列太多残酷的理由。 恍惚记得之前也写过类似内容的文章，呵呵，说明我确实再不停的为自己的所作所为寻找一个说得过去的理由。但内心却总还是不甘心，为什么就不能拥有一个单纯的编码人生呢？可是，事实是，一旦你开始去寻求开发的本质，就会越发的觉得，绝大多数烧脑的难题都来自于现实世界的问题，或者说都和业务有紧密的关系。而不简简单单的可以通过一段代码就能搞定的，这很有趣不是麽？ 有一天，我发现你需要花更多的精力去思考为什么的时候，往往这些问题都和人有关。更复杂，也更有诱惑力，因为它们更靠近价值本身。所以我突然意识到，为什么很多前辈慢慢的就不再分享纯技术的文章，慢慢的在社区里消失了（也可能有我不知道的更伟大的事儿在做）。 扯了这么多，简单点总结的话，就是我意识到自己做了快两年的项目，竟然对业务自身的了解还少的可怜，这就暴漏了很大的问题。即便是未来我换了个项目，但谈及以往的项目，我无法对其有一个很好的总结，这多少有些说不过去。况且，那么有趣的业务问题，如果不深究，总感觉是一种浪费，不是吗？~ 完结绕着地球飞了一圈，回到岗位上，一时间有太多的事儿要做，够忙几个月的了。剩下的人生岁月，如何能活的精彩，得再花多一些思考，多一些实践。不过，至少，我向往的，比以往又更清晰了一点呢。","tags":[{"name":"宗教","slug":"宗教","permalink":"https://blog.kazaff.me/tags/宗教/"},{"name":"世界","slug":"世界","permalink":"https://blog.kazaff.me/tags/世界/"},{"name":"工作","slug":"工作","permalink":"https://blog.kazaff.me/tags/工作/"},{"name":"感情","slug":"感情","permalink":"https://blog.kazaff.me/tags/感情/"},{"name":"自我意识","slug":"自我意识","permalink":"https://blog.kazaff.me/tags/自我意识/"}]},{"title":"Visa","date":"2017-10-25T09:37:00.000Z","path":"2017/10/25/visa/","text":"从九月中旬到今天，一直都在忙着一件大事儿：申请美签。因为工作的缘故，要去美国和客户商谈产品需求方面的事宜，所以要搞定签证。 对于我这种乡巴佬来说，签证申请的每个环节都毫无头绪，除了网上查的大部分信息外，就全靠运气了。此时此刻，我的护照已经在EMS的路上了，所以在此我想花点时间总结一下整个签证申请过程中的事宜，给大家一个参考，毕竟网上很多资料都非常老了，甚至一些旅行社的资讯也都严重过时了。 第一步： 护照其实护照我老早就申请过了，不过最近我又帮我老婆申请了护照，所以最新的申请方式我也想说一下。很多旅行社都会对你说，要回户口所在地去申请护照，或者让你去省会办理手续。其实这是完全没有必要的，你拿着你的身份证直接去当地的市民之家就可以办理（不要跨省），跨省的我不太清楚，总之护照的申请非常的简单了已经，到办理大厅，基本上都是自动化流程，非常简单的。 注意一点，外地身份证的申请者，只能使用物流方法来领取护照~ 在下一步之前，你必须先拿到护照，因为之后要用护照号。 第二步： 填写DS-160表格实话说，我一共申请了两次，第一次被拒签了。我详细说一下这个环节吧~ 首先，申请美国签证是要先填写DS-160表格的。可以直接自己在线填写的哟，不过注意是全英文的，也是因为这一点，我第一次申请的时候找了马蜂窝平台代理帮我申请，想找代理做另一方面是因为节省时间，毕竟他们很专业，会给你提供大量现成的资料，而且费用很便宜，一个人才一百多而已。 虽然第一次被拒签了（后面会分析理由），但如果你没有时间或能力自行申请的话，我还是很推荐马蜂窝的代理服务的。另外我要分享一个信息，其实中信银行提供免费的服务，帮你填写这个表格的哟~~你可以去当地中信银行咨询一下。 如果你想自行填写，也没问题哟，表格是有中文提示的，并且网上也有详细的步骤分享，这里我就啰嗦一点：一定要实事求是的填，没有必要扯谎~ 如果你之前被拒签过，不管是在大使馆面签被拒，还是在美国海关处被拒，你在填写DS-160时都要多填写一项：拒签理由。这个也要用英文哟，你只需要写清楚上次拒签的时间、地点和你认为被拒签的理由（拒签是不会给你理由的）即可。 DS-160表格一旦提交，就无法再编辑，所以你最好认真检查一下哦~创建表格会给你生成一个应用ID号，这个号很重要，不要丢，后面要用。 注意，如果你英文不好，在表格里对应的语言项就填写中文即可。否则，小心面试官给你pk英文呀~（如果你是出国留学，不会英文是拿不到签证的，除非有学校给你开的证明来说明你不会英文是有原因的~） 对了，还有一点，D-160上会要求你上传照片，该照片对尺寸和构图还是有要求的，不建议你自己手机拍摄，可以找个照相馆照一张数码照片，或者在机场和高铁站或者市民之家都应该能找到自助照片机器（不确定是否可以拿到是数码文件）。你不需要洗出来的，只需要电子版文件即可，上传到DS-160表格后，系统会审核照片是否满足要求的。 第三步：缴费其实是可能免面签的，但我不知道具体条件，你可以去官网详细查看，也可以直接打大使馆电话询问。不管你是否需要面签，缴费总是需要的。缴费的系统和DS-160是分开的，其实DS-160只是一个表格系统，你可以想象成在柜台领取了一个表格而已。 而缴费系统是需要注册帐号的，请使用你常用的邮箱，我这里建议，如果是商务签证，最好使用你的公司邮箱哟~后续大使馆会给这个邮箱发邮件通知。之所以我推荐你用公司邮箱，这些细节都是凭证，证明你的出行目的的。 注册登录后，点击“申请预约”即可，然后会让你再选择一次签证类型等等，最后会让你输入你的护照号码和DS-160给你的那个ID号，一定要填写对啊，否则你这次缴费可能就白瞎了。 由于我第一次是找的代理，他们其实会用自己的缴费帐号来帮你缴费，所以你就不需要注册这个系统了。但是当你自己预约时，系统会提示你你的护照号码已经绑定其它帐号，不允许你提交了。 不过不要方，你可以打大使馆电话让他们帮忙修改即可，提供你最新的邮箱和护照号就行了。如果不想电话骚扰他们，也可以在缴费系统里提交反馈，写明：“我需要更改帐号，之前的邮箱是别人帮我代理注册时用的，我现在需要变更成自己的邮箱：xxxx@xxxx。谢谢”。只要是工作时间，一般很快就会收到反馈的。然后你可能需要重新登录你的缴费系统，它会提醒你修改密码，然后就可以预约了。 回到刚才那一步可以保存后，你还需要填写一些护照回收方式，可以选择中信银行领取，也可以直接EMS。EMS是到付，所以缴费时不包含这笔费用的~ 然后系统就会让你缴费，你如果有中信卡，可以直接网银缴费，如果需要去柜台，你就打印出来系统生成的缴费凭证去中信银行缴费即可。记得，缴费后让工作人员给你开缴费证明，用于报销使用。 第四步：预约如果是ATM缴费，工作人员会建议你打印出来收据，上面有个流水号是原先用来在缴费系统里输入的，可现在系统是自动关联的~~回到缴费系统，接下来就开始预约时间了。根据你自己的情况选择时间即可，预约好后是可以取消的，不过有时间限制，我记得是预约时间2天前都可以变更~ 如果你预约的时间刚好赶上什么春运啊国庆啊，你最好提前看好火车票（如果需要的话）。 预约好以后，你就可以开始准备材料了。最重要的3个，分别是护照、ds-160和缴费后的那两个文件（我忘记叫啥名字了），没有这两个东西，你是进不去领事馆的。而且我前面说了，这两个文件上的编号是关联的，你一定要看一下是否匹配，打印出来，黑白彩色都可以的~ 不要以为有了它们就一切就绪了~你还要根据自己的签证类型，准备尽可能详细的支持性材料，用来证明你的出行目的和回国必然性。我这里就只说商务签证相关的支持性材料吧： 商务名片（有的话） 公司个人工作牌（有的话） 公司营业执照复印件+公章（必须有） 公司英文简介（尽可能有） 在职证明英文版+公章（尽可能有） 邀请函（必须有） 行程单（必须有） 和邀请方的工作邮件记录（有的话） 随行人的护照复印件（有签证的话也要印出来） 户口本 身份证 房本或购房合同（有的话） 保险合同（有的话） 结婚证（有的话） 学历学位证（有的话） 英文简历（尽可能有） 存款证明（3W+，但这一项对于商务签证来说似乎不必要） 尽可能把它们按照关联性放好，最好放一个透明袋子里，但这一点并不是必须的，我见了很多人都拿着手提袋进去也ok的。所以，不要在大使馆门口上当，买高价透明袋哟~ 也要准备好签证官要问的问题哟，后面会提到。 第五步：面签虽然官网提醒你不要去的太早，提前30分钟到即可。但我这边建议你还是尽可能早去，比方说你预约的是下午2点，你十二点左右在附近吃点饭，1点之前到都可以呀因为我去了两次面签，每次都是提前去，只要人不多，你可以提前入领事馆的对了，门口存包的，现在是收费30~不用担心找不到他们，他们会主动找到你的~~因为所有电子设备是不允许携带进去的，手机，随身听，相机，手环，等等。。 跟着队伍，你的护照会被贴上编号，录十指指纹，最后就是面签了，不要紧张。一般，每个人也就两三分钟，其实凭我经验来说，时间越长反而越好，否则你可能根本来不及表达，就被拒签了（当然也可能秒过哟~）。 像我这种运气不太好的人来说，还是期望签证官给我足够多的时间来证明自己的。 签证官的中文是非常好的，一点口音都没有，所以你完全不用担心听不懂，如果听不清的话，你礼貌性的要求对方再问一遍即可。但，切记抢答和答非所问，这样签证官可能会很反感，直接拒签你。 我相信，面签这一步，你准备的再充分，也是要看运气的。签证官的心情其实占很大比重，心情好的时候可能随便问你个问题就让你过了，心情糟糕的时候随便问一个问题就让你死了。。。不过，不管签证官的心情到底如何，他们和你沟通时是非常有礼貌的，不要担心会被甩脸子。 我来说一下我两次的面签细节吧，供大家参考一下。我只写出核心问题，并非签证官的原话，再次强调，他们非常有礼貌的！ 第一次（被拒）： VO: 出行目的？me: 商务会议，这里是我的邀请函和行程单 vo: 什么时候去，呆几天？me: （如实回答，确保和DS-160一致） vo: 什么职务？me: 软件工程师（我没有即时出示自己的名片和在职证明） vo: 学历是？me: 本科（我没有出示自己的学历学位证明，虽然都带了） vo: 公司名字？me: XXXXX（我没有出示公司的相关信息） vo: 你有房子么？me: 有，这是我的购房合同 vo: 有名片么？me: 有，您看一下（并没有顺带提供公司的营业执照复印件） vo: 之前出国过么？me: 没有 vo: 有亲人在美国么？me: 没有 然后签证官对着电脑噼里啪啦一阵敲，最后对我说：抱歉，我不能给你签证~ 大家可以看出来问题么？从大使馆出来后我仔细分析，假如我是签证官，我也不会让这个人过。为什么？因为没有足够有理的证明他说的内容。再分析一下，我准备了那么多，为啥现场都没出示？因为我TM在网上看很多人强调：签证官问什么，你答什么，不要说废话，不要硬塞给签证官他不想看的证明材料。所以我就一直在等签证官问我要证明，但事实证明我想多了。人家才不会主动给你要证明，你不给，他就认为是你没有，即便是你可能正握在手里等待被索要。 从商用礼仪上来理解，这话说的对。但别忘了，这是面试，面试就要是表现自己，过于保守的话，会失去很多机会的。当然，你也要有智慧，在签证官面前装逼是必死无疑的。说白了，面签就是考核你的素质，财力，还有你和自己祖国的羁绊。 第一次拒签，我拿到的是214b文件，也就是说无法证明我一定会回国。我在网上花时间查了一下，其实中国有一些地区是敏感地区，这些地区很多去美国打黑工的，所以如果你无法证明你自己，签证官就会按照他的理解来判断。人家就有理由断定你会非法滞留美国，完全说得通。所以，被拒签了，不要抱怨，仔细分析一下整个面签过程，争取不要遗漏一些重要的细节。 被拒签后，签证官会退回给你护照，你拿着护照只要一出大使馆，肯定会有人来给你说，他们可以调出你的拒签资料。关于这个，我也不太明白，我在网上查了一下，确实有很多机构提供这个服务，但并没有找到真实的用户。所以我不是很推荐你去购买这样的服务，即便是真的拿到你的拒签原因了，如果你没有掌握面签心得，你下次依然会被拒的。对吧？ 幸运的是，214b拒签，我是可以立刻再申请一次的，并不是终身拒签。我花了一些时间在所有流程上，按照优先级罗列了一下所有可能被拒的因素，包括找代理填写的ds-160，包括公司名片上写的公司官网当时无法访问（在维护中），甚至包括客户公司的邀请函有问题。种种的猜想吧，当然我也意识到了自己面签时的问题：过于保守，太在意签证官的心情了。 So，我又申请了一次（当然，前提是得到公司领导的支持），下面是第二次面签过程： vo: 出行目的？me: 商务出行，您可以看一下我的邀请函和行程单 vo: 职业？me: 软件工程师，可以看一下我的名片，还有在职证明和公司的相关信息（小心思，我将这些资料用曲别针捆绑在一起了） vo: 有邀请函么？me: 有，这是邀请函和行程单 vo: 会英文么？me: 抱歉不会 vo: 那你怎么和客户进行沟通呢？me: 客户的中文很好，您可以看一下我们的工作邮件往来信件 vo: 你被拒签过？me: 是的，大概在XXX号，我面签被拒了 vo: 那和上次拒签时的情况比起来，你这次有什么变化么？（这个问题我感觉签证官表达的也很吃力，但意思不难理解）me: 老实说，和上次比起来我的情况没有发生任何变化。不过上次被拒，我分析可能是因为我没有提供足够的材料来证明我的答案。 vo: 你这次去具体做什么？me: 去了解一下我们的产品在客户那边的使用情况 vo: 可以简单介绍一下你们的产品么？me: 可以。XXXXXXXX vo: 你负责的具体是哪一款？me: 嗯，好的。我是负责XXXXXXX vo: 为什么你一定要去美国呢？不可以在skype或电话里沟通么？（这个问题说明签证官已经对你的情况足够了解了，否则他不会想到这个问题哟）me: 嗯，我明白您的意思，这次去现场，是因为XXXXXXX（这个问题其实很简单的，你尽管说一个理由即可，毕竟这种问题很主观，无法核实的，但前面不要被这种问题给吓到，如果脑子一片空白说话语无伦次可能就会让签证官认为你在胡说，记住要淡定） vo: 有房子么？me: 有的，这是我的购房合同，还有存款证明。对了，还有我的保险合同您需要看一下么？vo：额，可以看一下（我感觉签证官明显觉得不需要，但可能是目前为止他对我的表现还算满意，所以就顺着我了~） vo: 之前去过其它国家么？me: 没有，不过之前去过香港，这是我的港澳通行证（我还没来得及拿，签证官就摆了摆手） vo: 有亲戚在美国么？me: 没有，您需要看一下我的户口本和结婚证么？（签证官眼睛并没有放在我身上，而是对着电脑一边打字一边轻轻摇了摇头） 然后就又陷入了蜜汁尴尬，玻璃里签证官一阵疯狂的敲击键盘，让我不禁想到了上一次被拒签的情境。就在我觉得没希望的时候，签证官对我微笑的说：好了，你通过了，祝你旅途平安。 我强忍住内心的喜悦，淡定的说了两边谢谢~~ 你可以看出，两次面签的问题几乎很类似。我相信所有商务类型的签证申请人，都会被问这些问题。但，第二次由于我干净利索的提供了足够多的材料证明，所以签证官开始根据我的具体情况开始问有针对性的问题。其实只要你如实回答，和DS-160保持一致，没有撒谎，你确实没有理由害怕或恐慌，实事求是的回答所有问题就够了。 第六步： 查进度面签通过，还不是真的就100%通过了，还是有概率在行政审查阶段被的，所以你可能还是要提心吊胆的过几天。从你面签日结束开始，每天你都可以在缴费系统里看一下进度哟（节假日除外）。你也可以在这里查到状态，只要确定看到的是“ISSUED”，基本上就没问题了。稍后你邮箱会收到一封邮件，其中会携带一个AWD号码，如果你缴费时填写的是中信银行代取，那这个AWD号码你拿着去指定的银行柜台领取就行了，如果是EMS，这个号码就是运单号，可以在EMS官网跟踪订单状态（其实也可以直接用你的手机号）。 注意，不要试图在中信银行提供的签证进度查询系统里查状态，我的至今都显示没有数据。其次，一般EMS单号取件当天查不到，要等次日才能查到进度，所以不要惊慌，你现在其实已经可以开始订机票了。 第七步： EVUS登记如果你的签证是十年的B签证，你还需要登记EVUS系统，具体步骤看这里。好了，我也祝你一路顺风~~ 最后叮嘱的是，拿到签证不意味着你就一定能在美国海关处入境哦，真正决定你是否可以入境以及能待多久的，是美国海关。所以，在飞机上好好练习英文吧~~","tags":[{"name":"美国签证","slug":"美国签证","permalink":"https://blog.kazaff.me/tags/美国签证/"},{"name":"商务","slug":"商务","permalink":"https://blog.kazaff.me/tags/商务/"}]},{"title":"EC2自动备份","date":"2017-10-13T09:37:12.000Z","path":"2017/10/13/EC2自动备份/","text":"昨儿我的一个同事不幸把AWS上的EC2搞坏了，幸而只是由于错误的权限设置导致服务器无法登录，上面运行的服务还可以正常访问。然后花了一下午时间的折腾，总算搞定了这个问题。方法很麻烦，但至少有效，其实就是再创建一个EC2，挂载有问题的EBS，然后更改设置… 今天的主题，并不是讲如何恢复，而是主要来看一下备份相关的设置。AWS的EC2管理后台，并没有明确的菜单来提供自动备份的设置，我搜了一圈，发现很多文章都是教你使用AWS CLI + 脚本来做这个事儿，甚至还有专门收费的帮你自动备份EC2的商用服务（每个月1刀）。 不可思议啊，全球最屌的云服务商竟然没有提供简化的自动备份配置？这不科学！后来我找到一个github项目aws-missing-tools，我差点就要这么做了。不过这个项目文档里缺少了配置AWS CLI的内容，所以我又去AWS官方文档里找，突然让我发现了一个例子，尴尬了，这不是明明提供了自动化备份的功能么？只是设置的位置不是很直接而已嘛。 简单说，就是依靠AWS CLOUDWATCH提供的事件机制，结合你设置的规则来触发备份操作，反正你只要根据官网例子的步骤，1分钟之内就搞定了。我就不多啰嗦了。","tags":[{"name":"AWS","slug":"AWS","permalink":"https://blog.kazaff.me/tags/AWS/"},{"name":"EBS","slug":"EBS","permalink":"https://blog.kazaff.me/tags/EBS/"},{"name":"自动备份","slug":"自动备份","permalink":"https://blog.kazaff.me/tags/自动备份/"}]},{"title":"Casperjs无字体问题","date":"2017-09-20T09:37:12.000Z","path":"2017/09/20/casperjs无字体问题/","text":"今天在服务器（centOS）上测试casperjs，突然发现截图出来的页面都是无字天书： 一开始以为是字体文件加载未完成导致的，后来发现原来是系统没有字体。。。shit 查了一下： 1yum install urw-fonts 安装了这个字体，然后就搞定啦~~","tags":[{"name":"casperjs","slug":"casperjs","permalink":"https://blog.kazaff.me/tags/casperjs/"},{"name":"phantomjs","slug":"phantomjs","permalink":"https://blog.kazaff.me/tags/phantomjs/"},{"name":"centos","slug":"centos","permalink":"https://blog.kazaff.me/tags/centos/"}]},{"title":"关于过度设计的思考","date":"2017-09-15T09:37:12.000Z","path":"2017/09/15/关于过度设计的思考/","text":"最近的一些工作，让我突然想起了一个概念：过度设计。之前在看大牛分享的信息时，其实有反复看到关于过度设计的描述，不过给出的例子多数都是围绕着系统架构设计或编码阶段的一些具体实施来阐述“过度”的。 我之前有写过一篇文章，其实就提到了一种过度依赖软件的情况。虽然与今天要讲的内容并不相同，但却都与“过度”二字有关联。 最近我们团队在开发一个较为复杂的需求，这应算是这个项目目前为止复杂度数一数二的一个模块了。简单描述，该模块提供了公司职员工作安排功能： 部门经理对员工未来周进行排班计划时，要尽可能满足该部门的工作时数限制（避免不合理的加班） 该排班计划有灵活的多级审批流程，系统要识别出排班变更前后导致的差异并交给审批人批复 公司要可以统计出该部门某一周的排班变更频率，不同时段的排班变的性质不同 员工是跨部门的共享资源，多个部门排班对同一员工不能造成时间重叠 不同部门对员工的标准工作时数要求是存在差异的（例如A部门可能要求员工一周不能超过40小数，而B部门要求员工不能连续工作超过5天） ….. 你可能不信，如果我这样罗列下去，大概能有将近50条各种各样的规则，可能更让你不可思议的是，这些规则在某些极端情况下还存在语义上的冲突（欢迎来到真实世界）。正是这些冲突，导致了软件的使用者和开发者之间的距离，或者说对相同系统的不同理解，不同视角。 起初，我一直认为，大道至简，但现在认为，存在即合理。这应该是一种思想上的提升吧，毕竟你并非活在“任何逻辑必须合理且高效”的现实世界~~好啦，让我们继续话题。 我花了一些时间静下来思考，该模块目前为止的状况，都是哪些因素引起的。这么多规则，哪些是客户本质的需求，哪些是由本质需求推理出来的间接规则，哪些是由于软件设计的局限性产生的，哪些是由所选择的具体技术栈引起的。总结后发现，直接来自客户的本质需求只占总量的30% - 40%，而现有具体技术导致的大概占10%，剩下50%就都是在项目需求分析时细化推理出来的了。考虑到与客户沟通存在的理解偏差，加上开发人员的经验和能力，我认为这剩余50%的规则很值得寻味。 我拿其中的一类问题来给大家看一下： 如果部门的工作时数规则变更了，历史数据怎么处理？ 如果某个员工在不同部门出现排班，部门的限制又不相同，审批时该如何处理此员工？ 有的限制是以天为单位，有的是周，有的是连续性，那跨天排班，跨周连续性排班怎么计算？ 这些问题，在没有系统的时候，存在么？或者说，存在的那么明显么？换个角度思考，这些问题都一样的重要么？如果处理不当，整个系统会变得毫无意义么？如果我们的客户不是完美主义者，他接受一定程度的瑕疵，作为设计者，我们又该如何权衡？我相信，尽最大可能的了解客户所在领域的背景知识此时就显得极为重要，以此来度量出不同类型数据的价值，此外结合自身对软件人机交互的理解，“曲线救国”。 而，如果，追求极致的逻辑性，很可能使项目陷入停滞，而且设计出的方案一定是十分复杂的，实现起来，维护起来都是很困难的。最要命的是，如果没有对软件交互花足够的心思（一般程序员都会忽略），最终结果一定是惨烈的。花了那么多时间和精力，得到否定的结果，不仅影响团队的整体士气，客户也会对你失去信心。 如果我们换个做法，不要过度放大一些问题，不要为破坏了某个规则而感到过度不安（主要是强迫症），及时且尽早的将一些冲突问题与客户讨论，观察分析他们对待此类问题的反映，以此来自我消化同类问题（如果所有问题都与客户讨论，不仅时间成本很大，也很可能遭到客户的反感）。 最终你会在合理的时间交出一份90分的考卷，剩下的那10分你有足够的时间在未来搞定（也可能交给别人了）。","tags":[{"name":"过度","slug":"过度","permalink":"https://blog.kazaff.me/tags/过度/"}]},{"title":"阿里云EC2-Centos6内核升级","date":"2017-09-07T09:37:12.000Z","path":"2017/09/07/阿里云EC2-Centos6内核升级/","text":"这几天收到了阿里云发来的短信，内容十分惊悚，吓得我直抽抽~~于是立刻登录控制台看看到底咋了！ 原来是阿里云的云盾系统检测出我们公司的EC2上存在系统和软件漏洞。不过现在它提供的安骑士开始收费了，所以漏洞的详细信息无法查看，幸好给了7天的免费使用时间。 解决系统软件漏洞的方式简单粗暴，就是先来一波升级： 1yum update 这样，基本上常见的软件版本过时导致的漏洞都可以修复。不过如果涉及到系统内核升级，我觉得还是要慎重一些，查到一个博主的文章，建议还是根据自己的实际情况酌情对待。 如果你觉得可以升级内核，那么执行完上面的命令后，不要忘记reboot系统哦~ 服务器重启完毕后，在阿里云控制台对漏洞信息进行验证即可~ 不过，我发现，还是存在一些无法修复的漏洞，提示如下： 软件: kernel-devel 2.6.32-431.23.3.el6命中: kernel-devel version less than 0:2.6.32-573.22.1.el6 查了一下，原来是因为系统存在多个kernel-devel导致的，可以执行: 1yum info kernel-devel 把不需要的版本直接卸载掉： 1yum remove kernel-devel-&#123;version&#125;-&#123;release&#125; 不需要重启哟，再去阿里云后台验证一次就可以了。","tags":[{"name":"项目管理","slug":"项目管理","permalink":"https://blog.kazaff.me/tags/项目管理/"}]},{"title":"论项目成败","date":"2017-06-27T09:37:12.000Z","path":"2017/06/27/论项目成败/","text":"俗话说：常在河边儿走，哪儿能不尿炕。 毕业到现在，web开发行业也已经干了快十年了。参与过，负责过也不少项目了，有大有小，有自研性质的，有外包类型的。我一直觉得，一个系统到底用何种方式来开发，取决于这个项目在公司所处的位置和重要程度来衡量。即便是一些处于中间位置的项目，选择外包团队来做的话，至少也要在需求层面把握的足够细致清楚，不要指望外包团队来帮你发现需求问题，甚至是逻辑问题。不过，最近我就碰到了一个相当奇葩的项目，我认为几乎涵盖了所有失败的必然条件，我一度怀疑客户是故意的，项目就是他用来打发时间的一场游戏。现在，就由我来罗列一下其中的几宗罪吧，让你们也开开眼。 需求提供方素质太低，沟通起来完全就是在耍流氓关于这一点，你可能会说：客户一般都不是专业人士，你不能期望对方给你提供完美的沟通保证。这一点，完全正确。不过，首先我要明确的是，甲方是日本一家专门做IT项目开发的公司，就是那种将自己公司IT人员外派到客户企业做定制开发的公司。当然，我们也不能因为这个，就假设他们的老板就一定是个程序员。这没问题，但问题在于，在前期项目接洽环节，这个老板的态度是细节完全不管，只负责扯一些官话，什么大家要用心做啊，什么大家辛苦了，一定要把项目做好啊一类的话，我不知道这些话是他用来骗自己的，还是他的口头禅，完全没有卵用啊。一旦涉及到具体项目问题，他立刻就会说：我不管这些细节，你找XXXX来沟通。 听起来是一个甩手掌柜，很懂得放权嘛~~可是，到了验收阶段，他冒出来，对着系统又开始云里雾里： 我感觉咱们的页面看起来有点怪怪的，和我们公司现用的一些系统不太一样。 那请问您是看到了哪个具体地方有问题了吗？ 这些细节我不管，你还是找设计人员沟通一下吧，反正我感觉有点不太舒服。 这些细节可能还不足够让你无语，更悲催的是，在项目开发到中期，他突然空降了一个秘书作为项目的负责人，还在项目会议上说：大家以后都要听XXX的指令，我完全信任他的判断，你们要知道，我公司的很多财务转账我都是交给他来做的，可见我对他的信任了吧。 而这个所谓的秘书，对当前的项目完全不清楚需求，自诩也是开发人员出身，但提的很多问题都让人哭笑不得。最可气的是，他完全不理会设计人员提供的设计书，按照他自己的想法后期提了很多需求变更问题。但是他拒绝提供标准的设计书，按照自己的方式提供变更描述，还狐假虎威的勒令开发人员赶进度。 到此为止，就已经犯了软件开发的无数大忌。不过这才刚开始。 在项目验收阶段，我突然收到了这个秘书的一条信息，大致内容是他离职了。此时，甲方老板则一改之前的态度，会议上说：这个人（秘书）对我们项目的需求把握的达不到我的要求，而且由于不是开发人员出身，你们沟通起来可能不顺畅，所以把他撤出项目组了。 这些由于甲方公司内部组织架构变更而导致的风险，难道乙方也要承担么？而在过程中多次要求提供标准设计书和需求变更文档，被当做是无理取闹，最终由于一个人的个人喜好导致的错误的需求变化，老板简简单单的一句“撤出项目”就可以把问题解决了吗？ 设计人员参与度不够由于甲方公司在日本，沟通本来就只能依赖网络，再加上中日文沟通障碍，所以甲方委托了大连的一个设计团队来把控需求。看起来也很合理，但问题在于这些设计人员都是兼职，他们只有晚上的时间才可以提供QA支持，这一点就已经严重了影响到了开发效率。而这些设计人员提供的设计书又十分的粗糙，很多低级错误，如错别字，数据库表字段类型与设计书不匹配，等等。 设计人员对该项目的理解和甲方的存在不一致，导致很多问题两方的回答不一致。本来这也不是解决不了的问题，三方对峙就可以了。但，实际情况是根本就没有组织这种会议的可能性，在每周例会中，开发团队提出的任何问题，都会被甲方当作是借口，不管是设计问题，还是开发问题，都需要开发团队来承担。在甲方眼里，设计人员不存在任何问题。这种不尊重事实的假设，不知道理由到底是什么？ 角色定位混乱，没有责任感在这个项目中，除了开发团队，似乎其它每个参与进来的角色都在提意见。而且，甲方对设计人员的过分信任，对开发团队的质疑，这种厚此薄彼的态度，对于这种三方合作非常的不利。 设计人员初期拥有较多的话语权，中后期被秘书夺走，秘书离职后设计人员又开始颠覆秘书提出的要求。不过，不管是谁拥有话语权的时期，都不愿为自己说过的话来负责，这是核心问题。 不遵守合同在合同中，明确提到的责任划分，在实际项目实施过程中完全没有被尊重。开发进度中原本只包含的工作量与实际工作量相比，发生了成倍的变化，但工期并没有合理的安排，这一点使得开发团队整体情绪低落，间接影响了各个项目的各个方面，如代码质量，沟通等。 整个项目，我们作为开发方，无疑存在自身的问题。但，尊重事实的讲，我们自身的问题占的比重非常的少。所以，这篇文章并不是反省，而是总结。以后，此类型的项目，最佳的处理方式，就是果断的终止合作。任何一秒的投入，都可以算是浪费资源。","tags":[{"name":"项目管理","slug":"项目管理","permalink":"https://blog.kazaff.me/tags/项目管理/"}]},{"title":"浏览器Tab页刷新和关闭操作的甄别","date":"2017-05-31T09:37:12.000Z","path":"2017/05/31/浏览器Tab页刷新和关闭操作的甄别/","text":"最近工作压力太大，感觉身体被掏空，每天都疲惫不堪，终于体会到心累的滋味了。原因仅仅是一个来自日本的外包项目，具体就不展开说了，我们今天就盯着一个技术问题来扯。 一般的Web项目，对于帐号重复登录的逻辑上，常见的解决方案是后登陆的把前登录的用户踢掉。这么做很大一定程度上是受限于http协议的本质的。当然，如果你的项目允许同一个帐号多次登录，那更幸福。 然后，世界并不总是美好的，至少日本这个项目不是。对方要求的是若账户已经被登录，再次登录时需要提示该账号正在使用中。其实单纯讲业务，这个要求一点也不过分，很多软件也确实是这么做的。不过，在Web上想这么搞，就很恶心人了。 受限于http的先天条件，项目的服务端session很难毫无时差的和浏览器端进行同步，换句话讲，这是一个分布式的问题。因为客户端和服务端分别保存了会话的一部分，并且还要维护数据状态的一致性。 维护这种一致性，就要求浏览器端在用户出现关闭页面，关闭浏览器，甚至关闭电脑的情况下，能通知服务端以清理session。当然，这里面还没包含电脑死机等非正常情况。 可能有人会说，这也不是做不到哇，浏览器上不是存在了一些事件，用来监听用户的操作行为么？例如：onunload，onbeforeunload等。 一开始我们团队也是这么想的，结果在开发的时候发现，这种类型的事件是存在浏览器兼容性问题的，在chrome下，你是无法甄别出用户是刷新还是关闭页面的。展开来说的话，即便是兼容性最好的onbeforeunload事件，chrome下你也无法得知用户是点击的确定还是取消按钮。 这尼玛就有点悲剧了！~我拿着问题的详细描述去找客户去解释，得到的结论是“我不管那么多，我只看结果”，这个观点让我想CTMLGB。你可能觉得我的问题更多一些，我也不打算详细解释（如果有心情，我会花时间另开一个文章专门介绍一下这个奇葩客户）。 现在，问题已经描述清楚了，如果是你，该怎么办？ 老朽中午吃饭的时候想到了一个变通的方案，在这里分享出来，希望能帮助到大家。 问题的核心在于无法区分刷新和关闭Tab。针对这两种情况，前者不应该将session清除，后者应该立刻清除session。而两种截然不同的处理方式由于chrome下是相同的事件监听，我们只能从其它维度下手了。 正常情况下，刷新页面会引发数据重新加载，一定会和服务端进行交互。而关闭tab，则不应该会在几秒钟内再发送任何请求了。基于这个常理，我们 只需要在前端的onbeforeunload事件回调中将该用户的session设置成5秒（延迟时间可以根据实际情况来调整）后到期即可 。 这样做，在刷新触发前，设置了延迟到期，刷新后由于获取数据而立刻在服务端将该session正常续约。而关闭Tab后由于不会获取数据，该session在5秒中自动过期。 这样就几乎完美的解决了这个问题，别跟我扯淡说5秒中内用户再登录会提示帐号正在使用中，你手速那么快你小弟弟受得了么？！ 唉，虽然解决了，但宝宝一点也不开心~~","tags":[{"name":"session","slug":"session","permalink":"https://blog.kazaff.me/tags/session/"},{"name":"chrome","slug":"chrome","permalink":"https://blog.kazaff.me/tags/chrome/"},{"name":"beforeunload","slug":"beforeunload","permalink":"https://blog.kazaff.me/tags/beforeunload/"}]},{"title":"Gmail模拟登录失败处理","date":"2017-05-22T09:37:12.000Z","path":"2017/05/22/gmail模拟登录失败处理/","text":"不管任何开发语言，收发邮件相关的库都是有的，而且用起来都非常简单。不过在实际项目中，还是会因为所使用的邮箱平台而出现一些问题，比较常见的就是安全认证相关的问题。 拿Gmail为例，它会提醒你一系列的安全问题，例如： 123Error: Please log in via your web browser: https://support.google.com/mail/accounts/answer/78754 (Failure)&#123;&quot;message&quot;:&quot;connect ECONNREFUSED&quot;,&quot;name&quot;:&quot;Error&quot;,&quot;stack&quot;:&quot;Error: connect ECONNREFUSED\\n at exports._errnoException (util.js:742:11)\\n at Object.afterConnect [as oncomplete] (net.js:982:19)&quot;,&quot;code&quot;:&quot;ECONNREFUSED&quot;&#125; 按照网上提供的解决方案，我们可以先在chrome中登录我们的目标gmail帐号，然后访问https://myaccount.google.com/lesssecureapps，开启该设置后，很多人这个问题就搞定了。 但，我们并没有，可能是google更新了安全方面的限制，当我把项目部署上线后（放到了海外的vpn上），就又开始无法登录了。 随后我们还需要访问https://accounts.google.com/DisplayUnlockCaptcha, 授权通过后应该就搞定了。 但，我并没有，还需要在https://myaccount.google.com/notifications找到对应ip地址的请求拦截信息，点击“通过”，才算最终完成所有需要的配置。 赠送知识点jQuery如何操作页面iframe中的dom节点： 父窗口操作IFRAME: window.frames[“iframeSon”].document IFRAME操作父窗口: window.parent.document","tags":[{"name":"gmail","slug":"gmail","permalink":"https://blog.kazaff.me/tags/gmail/"},{"name":"安全认证","slug":"安全认证","permalink":"https://blog.kazaff.me/tags/安全认证/"}]},{"title":"Flux的作用","date":"2017-04-30T09:37:12.000Z","path":"2017/04/30/flux的作用/","text":"很久很久之前，应该是fb刚提出flux的初期，我就花时间在学习如何使用它了。不过断断续续的工作经验，很多东西的记忆已经模糊了，亦或者是从来没有悟出什么深层次的东西，导致前段时间另外一个团队的同事在聊到vuex时，我脑子里除了文档中教条式的使用方法，一片空白。问题大了啊~~ 今天无意间逛到了一篇文章，其中扯到了redux的意义，使我受益匪浅。为了避免再次忘记，我赶紧记录下了这篇文章。 关于flux、redux和vuex，我们不谈它们之间具体的用法，这篇文章的关注点在于这种思想的意义（比较空，但很重要）。对这种思想的一种粗浅理解是，通过增加这一中间层，让整个项目对数据的操作（CURD）由原先的混乱与随意改善为统一和明确，做大的好处应该就是增加了代码维护性。 而仅仅这一种维度的解释，实在无法说服别人来引入这个玩意儿，毕竟如果仅仅想做到这样，自己封装一个数据操作类即可啊。 另外一个模糊的认识，是来自于早期官方的一个示例，通过这个中间层，我们可以很轻易的就让应用实现“后退”功能。不过这依然可以简单的将历史版本数据保存在localstorage中而轻易实现。那么，flux还有什么深意么？ 直到看到了这篇文章，我们来总结一下文章中提到的几个要点： 如果数据出现在多个不同的地方，势必存在重复和嵌套层次，这会让更新数据的逻辑变得复杂 过深的嵌套和严重的重复会加剧操作数据的复杂性，降低处理性能 伴随着引入不可变数据类型，这种嵌套和重复还会丧失不可变数据类型的优势，进一步降低渲染性能 这样，flux的作用就基本上已经变得不可或缺，不再是可有可无了。 那么，具体flux提出了哪些概念来解决问题呢？ 文章中提到了“data table”，其实就是数据库概念，由于种种原因，数据库往往都被我们设计成尽量扁平的（关系型数据库只有行列概念），除了范式外，上面提到的要点也都自然而言的考虑在内了。 按照flux的最佳实践，我们需要： 为每一个概念实体都创建独立的存储单元 实体之间依赖id来进行关联 多对多关系也抽象成一种概念实体（和关系型数据库处理多对多关系完全一致） 其实完全是借助数据库思想来解决问题的，只是这种想法在当时还是很超前的。 实例嵌套结构： 12345678910111213141516171819202122232425262728293031323334353637383940414243const blogPosts = [ &#123; id : \"post1\", author : &#123;username : \"user1\", name : \"User 1\"&#125;, body : \"......\", comments : [ &#123; id : \"comment1\", author : &#123;username : \"user2\", name : \"User 2\"&#125;, comment : \".....\", &#125;, &#123; id : \"comment2\", author : &#123;username : \"user3\", name : \"User 3\"&#125;, comment : \".....\", &#125; ] &#125;, &#123; id : \"post2\", author : &#123;username : \"user2\", name : \"User 2\"&#125;, body : \"......\", comments : [ &#123; id : \"comment3\", author : &#123;username : \"user3\", name : \"User 3\"&#125;, comment : \".....\", &#125;, &#123; id : \"comment4\", author : &#123;username : \"user1\", name : \"User 1\"&#125;, comment : \".....\", &#125;, &#123; id : \"comment5\", author : &#123;username : \"user3\", name : \"User 3\"&#125;, comment : \".....\", &#125; ] &#125; // and repeat many times] 扁平化结构： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&#123; posts : &#123; byId : &#123; \"post1\" : &#123; id : \"post1\", author : \"user1\", body : \"......\", comments : [\"comment1\", \"comment2\"] &#125;, \"post2\" : &#123; id : \"post2\", author : \"user2\", body : \"......\", comments : [\"comment3\", \"comment4\", \"comment5\"] &#125; &#125; allIds : [\"post1\", \"post2\"] &#125;, comments : &#123; byId : &#123; \"comment1\" : &#123; id : \"comment1\", author : \"user2\", comment : \".....\", &#125;, \"comment2\" : &#123; id : \"comment2\", author : \"user3\", comment : \".....\", &#125;, \"comment3\" : &#123; id : \"comment3\", author : \"user3\", comment : \".....\", &#125;, \"comment4\" : &#123; id : \"comment4\", author : \"user1\", comment : \".....\", &#125;, \"comment5\" : &#123; id : \"comment5\", author : \"user3\", comment : \".....\", &#125;, &#125;, allIds : [\"comment1\", \"comment2\", \"comment3\", \"commment4\", \"comment5\"] &#125;, users : &#123; byId : &#123; \"user1\" : &#123; username : \"user1\", name : \"User 1\", &#125; \"user2\" : &#123; username : \"user2\", name : \"User 2\", &#125; \"user3\" : &#123; username : \"user3\", name : \"User 3\", &#125; &#125;, allIds : [\"user1\", \"user2\", \"user3\"] &#125;&#125; 实体关系处理： 12345678910111213141516171819202122232425262728&#123; entities: &#123; authors : &#123; byId : &#123;&#125;, allIds : [] &#125;, books : &#123; byId : &#123;&#125;, allIds : [] &#125;, authorBook : &#123; byId : &#123; 1 : &#123; id : 1, authorId : 5, bookId : 22 &#125;, 2 : &#123; id : 2, authorId : 5, bookId : 15, &#125; 3 : &#123; id : 3, authorId : 42, bookId : 12 &#125; &#125;, allIds : [1, 2, 3] &#125; &#125;&#125; 工具思想有了，工具也不能落后，normalizr就是来帮我们来快速实现不同结构之间的转化的。","tags":[{"name":"redux","slug":"redux","permalink":"https://blog.kazaff.me/tags/redux/"},{"name":"vuex","slug":"vuex","permalink":"https://blog.kazaff.me/tags/vuex/"},{"name":"扁平化","slug":"扁平化","permalink":"https://blog.kazaff.me/tags/扁平化/"},{"name":"不可变","slug":"不可变","permalink":"https://blog.kazaff.me/tags/不可变/"},{"name":"数据表概念","slug":"数据表概念","permalink":"https://blog.kazaff.me/tags/数据表概念/"}]},{"title":"拥有这些Casperjs技巧理论上就能采集全世界了","date":"2017-04-17T09:37:12.000Z","path":"2017/04/17/拥有这些Casperjs技巧理论上就能采集全世界了/","text":"熟悉我的人都知道，我还算是一个比较务实的人，所以我很少用类似本文这种有些哗众取宠的标题的。但，偶尔用用还是挺爽滴~ 场景描述基本上当今做web开发的，不可能没有自己写过或接触过 采集目标web页面 这个领域的相关工具。总之，你可以在搜索引擎中找到大量的关于“采集”主题的技术分享。采集，也叫“偷”，被采集的数据往往对采集者来说是有价值的，而无偿使用这些数据，我个人感觉和偷没有啥本质区别。我们这里从纯技术角度来讨论一下这种偷的技巧。我还是很支持知识付费的，只不过一些老一点的平台往往不提供接口供第三方获取数据，这个时候，就只能用技术手段来采集数据了。 如果你采集淘宝这种网站的商品信息，其实还是比较容易的，因为不需要必须登录系统嘛。所以，你可以简单的模拟首页请求，然后解析html，根据得到的目标链接进行迭代请求（这里没有特别强调目标系统的防御措施）。当然，至于是深度还是广度遍历优先，需要根据你自己的需要来选择了，当然还有一些其它的技术细节，如并发数控制，请求失败处理，连接池，甚至是代理IP库等等。这些都可以在网上找到对应的功能库或资料文献。 如果你采集的系统并不是公开的网站，而是管理系统类型的站点，那就意味着你要先想办法模拟登录到目标系统中。这里我们假设系统不要求输入任何类型的验证码（验证码识别并非我的强项，一般遇到这种情况我都是直接放弃，谁知道好的验证码识别方案，不妨留言给我哟~），而且我们又拥有对应的合法帐号。 接下来就可以开始捋起袖子开始采集了。 所需环境正如我们标题所说，这里我们使用casperjs，之所以没有使用简单的模拟请求方案，是因为被采集的WEB系统往往存在很多前端javascript的逻辑。如果你直接发起一个http请求，拿到的很可能是一些无用的html数据而已（目前大量的使用了前端框架的网站都这样）。所以，我们需要一个支持编程接口的虚拟浏览器，由于之前我做过前端端到端测试的尝试，所以自然就想到了casperjs。 实战这篇文章并不打算事无巨细的讲解casperjs要怎么用，毕竟它的官方文档非常的简单，而且我之前的文章也有比较详细的介绍过这些内容。这篇文章的目标主要是讨论几个关键问题点，并给出我的一个解决方案。 模拟点击其实这个方面，之前文章也有提到过。在模拟访问某些系统的时候，casperjs提供的click方法并不总是有效。我不能确定具体原因，但貌似像一些div, span等这样的元素上进行模拟点击会存在失败的可能。 这个时候，我会尝试使用casper提供的mouse库，不要问我为什么，总之是会管用的。 模拟滚动针对上面的模拟点击话题，有这么一种情况：如果当前页面中该区域并不处于可视区内，那么对它的模拟点击可能是无效的。这是什么意思呢？举个例子，下拉菜单里往往会有很多项，一般情况下如果选择项太多，菜单展开后都会有滚动条，对吧？你如果直接尝试模拟选择菜单中最下面的项目（需要滚动才能肉眼看到的选项），你可能会失败。这个时候就需要模拟滚动了。 了解到这一点后，剩下的就是如何模拟滚动了。casper提供的scroll***方法都是滚动整个页面区域，而我们想滚动特定区域的话，就得自己想办法了。当然，前面提到的mouse模块值得一试，我在之前的文章中也是这么做的，只需要模拟鼠标按下（down），移动（move），松开鼠标（up）行为即可。不过，还有更简单的方案可以考虑，就是直接使用casper提供的[evaluate方法(http://docs.casperjs.org/en/latest/modules/casper.html#evaluate)。 该方法本身并不是用来做这件事儿的，不过该方法允许我们注入我们的逻辑到目标页面中去，所以，我们可以借助其它js库的帮助来完成滚动行为，例如jquery.scrollTop。那如果目标网站自身并没有使用jquery怎么办？我们一样可以将目标库注入到页面中。 模拟下载文件本文的重点到了，如果我们想在目标网站上下载特定文件，该怎么办？简单点的模拟下载，只需要直接使用casper.download方法即可。 不过，如果下载链接无法直接获得的话，就需要我们耍点手段了。既然目标页面中附件的下载链接不是静态写在html中的，那么必然使用了js处理。如果这些js代码可读性比较好，我们依然可以通过分析来理清楚这些js的业务逻辑，最终自行手动拿到附件的目标链接地址。 但是，往往目标网站的js代码都是压缩混淆过的，基本上没有可读性。再复杂点的话，这些js逻辑还需要借助ajax请求来计算数据。如果我们拿不到附件的链接，我们就无法使用casper.download方法。而直接模拟点击下载链接的话。casper是不会帮你将附件下载到你的目标位置的，这里我不能确定是否下载到里某个casper的默认位置，知道的童鞋请告诉我哟~ 接下来我们该怎么办？我相信很多人又遇到过这个问题，解决方案多种多样，我这里分享一个我的思路。我遇到的实际情况是，前端js压缩混淆了，而且我经过分析发现，它会发送一个ajax去获取目标文件的id用做生成下载链接。 而我们需要的，就是模拟这个ajax请求去拿到对应的返回结果。不过这个ajax请求还携带了相当多的参数，这就增加了模拟请求的难度。拿到这些参数，我们可以使用casper提供的resource.requested事件HOOK。 按说我们可以拿到请求携带的参数，也应该同时可以拿到请求的响应结果才对，不过奇葩的是，casper并不提供获取响应结果的接口。我实在搞不明白这是因为什么理由，毕竟你已经提供了大量的接口来让用户模拟浏览器操作，而且提供的事件也围绕着请求的各个环节，也确实发送了请求。但怎么就不老老实实的把响应数据也提供出来呢？实在让人无语啊。 既然无法通过casper提供的现有方法来拿响应结果，我们只能再耍一些手段了。通过获取到的请求参数，我们再次使用evaluate方法，在页面中注入jQuery.ajax逻辑，并将ajax的返回结果写入到自己创建的Dom节点中。 最后通过casper.waitForSelector方法拿到我们想要的响应结果。到这一步，基本上就搞定了下载文件的需求了。 选择器casper提供的很多方法都接受“选择器”参数，用起来非常方便。不过有的网站并没有提供可供使用的id或class属性，甚至我还碰到过整个html中所有id和class都是随机生成的网站。 这个时候，我们就需要用一种麻烦的方法来搞定它了。使用evaluate方法注入jquery逻辑来定位目标Dom，并获取它的动态id或class属性值，并返回给casper。可能有更聪明的方法，等待高手赐教。 结束语这篇文章有点标题党了，不过还是有点价值的，不是么？","tags":[{"name":"Casperjs","slug":"Casperjs","permalink":"https://blog.kazaff.me/tags/Casperjs/"},{"name":"网站采集","slug":"网站采集","permalink":"https://blog.kazaff.me/tags/网站采集/"},{"name":"模拟请求","slug":"模拟请求","permalink":"https://blog.kazaff.me/tags/模拟请求/"},{"name":"模拟登录","slug":"模拟登录","permalink":"https://blog.kazaff.me/tags/模拟登录/"},{"name":"模拟鼠标点击","slug":"模拟鼠标点击","permalink":"https://blog.kazaff.me/tags/模拟鼠标点击/"},{"name":"模拟滚动","slug":"模拟滚动","permalink":"https://blog.kazaff.me/tags/模拟滚动/"},{"name":"下载文件","slug":"下载文件","permalink":"https://blog.kazaff.me/tags/下载文件/"}]},{"title":"前端异常上报","date":"2017-04-06T09:37:12.000Z","path":"2017/04/06/前端异常上报/","text":"问题描述我们的团队现在负责的是一个海外公司的ERP项目，一直困扰我们团队的一个问题是，用户在浏览器上碰到痛点我们很难拿到足够的异常数据供开发人员分析问题。由于异地和用户的非专业型，让获取异常数据难上加难。 不过，这方面的资料和实践已经非常的成熟，而且我们自身的场景也足够简单（由于是内部系统，所以浏览器版本可控），所以投入一些精力会得到相当丰厚的回报（非常值得）。 就在今天，哥们儿我就要直面这个问题！ 解决方案调研和思考花了一些时间的资料阅读，梳理出下面的大纲（a little bit of mess）： 监控的目标资源及场景 JS文件 Css文件 Html文件：DOM合法性检查 404 跨域 本地cache 代码压缩问题 数据少选条件 ip 异常级别 采样率 开发环境/线上环境 主动埋点上报 &amp; 全局异常收集 收集的信息 ip UserAgent 错误信息：错误文件名称，页面url，错误行号，错误详情，错误堆栈 错误发生时间 错误用户的帐号信息 performance信息：内存，cpu等 日志持久化方案 错误信息后台管理界面 触发方式：机器自动监控/真实用户行为 消息通知提醒：邮箱，短信等 可以看出，其实想要实现一个完美的前端异常监控工具，还是一个相当有含量的工作。不过其实可以分阶段来一点一点完善功能，最基本的功能我认为至少应该包含： js异常全局监听（暂时不考虑跨域，不需要很多埋点） 线上环境和开发环境的开关 收集错误详细信息，浏览器的基本信息（UserAgent，访问地址），可能的话应包含登录帐号的相关信息 数据上报和数据持久化可以简单的采用请求一个图片，并借助web server的log日志存储数据 我觉得这应该已经缩减到最低限度了，至少在我们的场景中是这样的。 注意事项 将 JavaScript异常监控脚本放置文档最前面（至少是其他 ‘script’ 块之前）； JavaScript 异常监控脚本独立在一个 ‘script’ 块中（建议是外部脚本文件，亦可使用缓存）； 每次上报请求避免缓存，需要增加随机数参数 URL参数不同的浏览器有不同的长度限制，考虑哪些参数需要放在URL参数上，哪些放在其它位置（header，cookie） 最低功能实现的话，基本上一个前端人员就可以轻松搞定。当然，如果你想要更简单，也不是不可能，目前许多第三方流量分析平台也提供了异常信息追踪功能，例如google analytics，好处就是提供了比较好用的管理界面。如果想迅速搭建一个可用的环境的话，是一个不错的选择。 参考前端代码异常日志收集与监控 JSTracker：前端异常数据采集 如何做前端异常监控？ 一种生产环境中高效定位JS异常的方案 前端异常监控 - BadJS 前端相关数据监控 使用Google Analytics跟踪捕获JavaScript，AngularJS，jQuery的在线错误和异常 异常跟踪 前端代码异常监控 sentry","tags":[{"name":"javascript","slug":"javascript","permalink":"https://blog.kazaff.me/tags/javascript/"},{"name":"error","slug":"error","permalink":"https://blog.kazaff.me/tags/error/"},{"name":"trace","slug":"trace","permalink":"https://blog.kazaff.me/tags/trace/"}]},{"title":"新UI新气象","date":"2017-03-28T09:37:12.000Z","path":"2017/03/28/新UI新气象/","text":"用了一年多的hexo皮肤终于还是审美疲劳了，换了个google的material风格的新UI，客官是不是觉得焕然一新呢？本来只是因为多说评论服务关闭了打算切换disqus这么简单而已，结果没想到最终连UI都更了。娃哈哈~ 其实更新过程并不是那么顺利的，因为新的这款UI编译需要node6+，而我travis配置文件中配置的是node4，所以出现了个灵异事件：编译完后没有任何html文件。花了好久才发现原因，travis编译结果显示的是正常，但只有在编译过程中查看log才能看到错误提示，真是醉了。 不知道这个UI会用多久，希望大家都喜欢吧。 PS：明天出发去香港，好紧张。不过一个人的旅途，很孤单啊~~","tags":[{"name":"travis","slug":"travis","permalink":"https://blog.kazaff.me/tags/travis/"},{"name":"theme","slug":"theme","permalink":"https://blog.kazaff.me/tags/theme/"},{"name":"disqus","slug":"disqus","permalink":"https://blog.kazaff.me/tags/disqus/"}]},{"title":"关于蚊香狗","date":"2017-03-16T09:37:12.000Z","path":"2017/03/16/关于蚊香狗/","text":"真的是long time no see了，一直没有更新博客最近。难道我挂了？然而并没有！ 今天要安利的是一个基于nodejs的api网关中间件：蚊香狗（下文称：MCDog）。 没错，是我最近做的一个模块。虽然还处于初期，但还是忍不住给大家分享一下。 其实关于网关模块，很久前就已经开始关注了，由于种种原因（拖延症）一直没有下手。不过，现在公司开始需要这样一个中间件，刚好我又有时间有兴趣，就着手随便搞了搞。 之所以叫蚊香狗，并没有什么特殊含义，就是觉得足够猎奇，创意来自于下面的这张图： 说回正题，其实目前MCDog提供的功能简陋的不要不要的，我也强烈推荐不要真的部署在生产环境上。不过，这并不表示它完全没有价值。你完全可以把项目在本地跑起来，然后将计划合并的api创建出来，然后把MCDog生成的文件放到线上来提供服务。 而一个称职的网关系统，对可用性和安全性要求都是很高的，同时还要保证灵活可扩展，对于我这样能力的人来说确实有些难以实现。两周前无意间看到了一篇文章，感觉其思路很好，就立马着手开发了这个扩展版。 在开发MCDog过程中，除了已知的难题外，还遇到了一些计划外的问题，小有收获。例如，在校验和解析使用者提交的DSL时，要求要尽可能覆盖到所有异常情况，生成的代码一定要捕获所有可能的异常。 尽管MCDog目前的DSL校验和解析的完善度还不够，采用的解决方案和实现的代码可能也很土鳖，未来也许会选择完全不同的路线。不过在编码的过程中我还是充满了乐趣的。 这篇文章并不打算详细介绍MCDog的使用方式和开发思路，因为我已经在github上提交了较为完整的文档，有兴趣的童鞋可以看一眼去，也欢迎大家把想法和问题在github上反馈给我。 好了，不说了，回家《地平线：黎明》去~~~","tags":[{"name":"rest","slug":"rest","permalink":"https://blog.kazaff.me/tags/rest/"},{"name":"网关","slug":"网关","permalink":"https://blog.kazaff.me/tags/网关/"},{"name":"api合并","slug":"api合并","permalink":"https://blog.kazaff.me/tags/api合并/"},{"name":"dsl","slug":"dsl","permalink":"https://blog.kazaff.me/tags/dsl/"}]},{"title":"如何避免遭受Wordpress的xmlrpc攻击","date":"2017-02-28T09:37:12.000Z","path":"2017/02/28/如何避免遭受wordpress的xmlrpc攻击/","text":"早上收到领导简讯，说公司官网无法访问了。omg，这怎么可能？第一反应是为啥360监控没有第一时间给我邮件提醒类？ 然后立刻登录到服务器上，发现所有的配置都没有变更过，运行了几个月就突然挂了吗？而且从表象上来看，nginx依然坚挺，只是返回403错误而已。这里需要注意的是，打开配置文件之前一定要先查看一下核心配置文件的最后修改日期，这样可以知道是否有其他人登录服务器修改过。 我毫无头绪的尝试了一些配置的修改后放弃了，因为一切都是那么的自然，毫无理由的就403了，nginx在，php-fpm也在。不过，nginx只能提供静态资源的请求，所有对php的请求都直接403了。 最终我只能去翻看一下nginx的日志文件，发现了线索： … connect() to unix:/var/run/php5-fpm.sock failed (11: Resource temporarily unavailable) while connecting to upstream, client… 日志中还显示出了一个ip地址，查了一下是一个芬兰的地址，不停的请求xmlrpc.php，引起了我的关注。 将该地址加入黑名单后，瞬间网站就恢复了。 由于服务器配置不高，所有我之前给php-fpm分配的配额很低，所以大量的请求一下子就让php-fpm饱和了。所以就有了前面的403返回。 有趣的是，对方使用了一个固定的ip不停的发来请求，如果是大量不同的ip，我可能到现在都无法定位问题。这里还是要提醒大家：线上系统突然出现问题，一定要第一时间去翻看日志。这样可以省去大量的时间来猜疑，像我，一开始就认为是https配置的问题。 接下来我查了一下wordpress的xmlrpc攻击相关的文章，发现这个问题对于使用wp的网站来说似乎普遍存在。这篇文章 有非常细致的剖析问题和提供了多种的解决方案，非常推荐看一下。","tags":[{"name":"php-fpm","slug":"php-fpm","permalink":"https://blog.kazaff.me/tags/php-fpm/"},{"name":"nginx","slug":"nginx","permalink":"https://blog.kazaff.me/tags/nginx/"},{"name":"wordpress","slug":"wordpress","permalink":"https://blog.kazaff.me/tags/wordpress/"},{"name":"xmlrpc","slug":"xmlrpc","permalink":"https://blog.kazaff.me/tags/xmlrpc/"}]},{"title":"Psd设计稿网工神器","date":"2017-02-22T09:37:12.000Z","path":"2017/02/22/psd设计稿网工神器/","text":"年后第一篇文章，就是安利一款神器。 前前后后试了好几款类似的工具，最终还是被它给征服！废话不多说，直接上链接：像素大厨。 试问谁能想到一款中文名字听起来以为是游戏的工具竟然能让我热血沸腾！啥也不说了，我要去切图了！","tags":[{"name":"psd","slug":"psd","permalink":"https://blog.kazaff.me/tags/psd/"},{"name":"设计稿","slug":"设计稿","permalink":"https://blog.kazaff.me/tags/设计稿/"},{"name":"标线","slug":"标线","permalink":"https://blog.kazaff.me/tags/标线/"},{"name":"取色","slug":"取色","permalink":"https://blog.kazaff.me/tags/取色/"}]},{"title":"安利Ios的web远程调试神器","date":"2017-01-25T09:37:12.000Z","path":"2017/01/25/安利ios的web远程调试神器/","text":"今天是农历春节前的最后一班岗，站好！ 前几天我们组的项目上线了一个手机端页面，一切都是那么自然，只是到发现问题后调试的时候很慌张。 andriod还好，很容易就找到了远程调试的方法，一条数据线即可，直接开启强大的chrome开发者工具，十八般兵器任君挑选。 可ios下就有点辣鸡了，基本上只能使用fiddle来抓包。我们不是专门的移动端开发团队，所以公司并没有给配mac机。黑苹果也不赶趟啊~ 今天突然发现一个方案，实测有效，高兴坏我了。 它的名字叫: weinre。 现在，我想说的就只有这些，祝大家新年快乐，随便扫得敬业福！","tags":[{"name":"debug","slug":"debug","permalink":"https://blog.kazaff.me/tags/debug/"}]},{"title":"2016不完全感悟","date":"2017-01-20T09:37:12.000Z","path":"2017/01/20/2016不完全感悟/","text":"这是一个对我来说变革的一年，上一次类似的变革发生在大概3年多前，那一次侧重于技术认知上的变革，这一次偏向于为人处事上的！ 程序生涯已有9年，写代码对我来说已经成了一种生存本能，和呼吸一样。虽然岗位变迁了几次，title也不停的变更，但对我来说初心未变，我依然清晰的记得第一次自己写的程序成功运行时的那刻喜悦，每每脑中浮现那一幕，嘴角依然会不由自主的上扬！ 我是个没有长性的类型，所以做一件事儿做了九年，这对我来说真的是一个奇迹。我此时此刻依然在庆幸选择了程序人生。记得刚毕业时，一个小小的伪愤青，骂公司，骂领导，骂同事，基本上恨不得拿着一把标尺去评价每一个人。那个阶段，活的虽然很有干劲儿，但却不顺利，自己内心清楚，那种格格不入的感觉无时无刻不让自己感到很无助。 曾经觉得，自己是千里马，自己是主人公，自己有主角光环，虽然这和祖国的教育脱不了干系，但总不能全都责怪整个世界，愚钝的自己不懂得自我反思，活的累也是活该。思维一旦跳出来，一切也就跟着不同了。 跳出固有的思维模式，虽然很多心灵鸡汤都有强调，但真的让我有所顿悟的还是我的老朋友：代码。曾经以为程序世界，非黑即白，对就是对错就是错，不存在丝毫妥协。在开头提到的我的第一次技术变革后，我明白了，即便是一个完美的技术方案，其中也无处不在权衡利弊。一个绝对正确的方案只可能存在于空气中，世界上任何一个个人或团队都不可能将其落地。那是我的感悟是，毕竟架构包含方方面面，涉及到企业文化，组织结构，甚至参与者的个人修养。简单的一个函数应该就很纯粹了吧，总不至于还要有什么妥协了吧？ 我不清楚如何详细的描述我想要表达的场景，但我深刻的认识到，即便是一个功能简单的函数，也有可能存在权衡，一味的追求极致有时候真的是自取灭亡。“调的一手好参数”就能得到最佳结果真的不是什么玄学。虽然这里面包含枯燥的数学原理，但大道至简，平衡各方才能达到最大效能。 15年中的时候，我离开了上一家公司，16年初到现在的公司，这就是我开头提到的我个人的第二次变革。虽然我担心你误解我的本意，但我依然想用“垃圾”来形容上一家公司。它让我看到了人性的阴暗面，虽然里面也有我尊重的人，但带给我更多的是一种精神污染。现在回想起，依然有反胃感。人与人之间，信任可以不谈，尊重可以没有，但虚伪真的让我受不了，上至领导，下至员工，随处可见的虚伪和欺骗，让我如履薄冰。离开它曾经让我误以为是错过了一个人生高峰，但事实却证明对我来讲，它只是一个深渊，爬的出来就是一种成长。敬还在公司的我尊敬的那几个人，祝福你们衷心的。 整整一个16年，我在新的公司，新的岗位继续成长着。虽然有些新的同事我不欣赏，但总体是幸福美好的，上级给予我足够的信任和尊重，同事相处也很和谐。自己也不再过于固执，很多存在妥协空间的事儿也都懂得妥协，求同存异嘛。虽然仍然会因为一些讨论控制不住自己的情绪，但总体还是处于稳态。 17年，依然很多事儿等着我来面对，不过相比年初时的我，少了彷徨和失落。我仅有的阅历，让我明白如何努力，才能完成目标，足够我识人辨事，与人相处。路还很远，路在脚下，路没错，走起~~~","tags":[{"name":"权衡","slug":"权衡","permalink":"https://blog.kazaff.me/tags/权衡/"},{"name":"平衡","slug":"平衡","permalink":"https://blog.kazaff.me/tags/平衡/"},{"name":"妥协","slug":"妥协","permalink":"https://blog.kazaff.me/tags/妥协/"}]},{"title":"看我如何搞定Nodejs内存泄漏问题","date":"2017-01-04T09:37:12.000Z","path":"2017/01/04/看我如何搞定nodejs内存泄露问题/","text":"最近又用node写了一个小工具，需要常驻进程，经过几天的观察，发现内存占用有持续增加的趋势（虽然不明显，但还是让我察觉到了，我真屌）。突然发现，我竟然不知道怎么排查nodejs的内存泄漏，吓死宝宝了！花时间看了一下相关资料（google真好，外果仁真屌），看来这部分也已经有比较完善的方法论+工具了。所以这篇文章记录一下自己从不懂到入门的经历~~我希望这篇文章不仅能提供具体的工具供大家使用，还提供足够的理论知识来辅助大家思考，当然，也可能是我自己想多了~~哇哈 发现问题由于没有太多运维经验，也不知道啥逆天的工具来帮我一键式监控所需要的指标，如果你和我情况一样，那我们只能手动来造个简陋的但够用的监控脚本了。 别告诉我你和我一样shell也不熟，直接就node吧。少废话~ 先装上pm2，然后写一个脚本，来定时打印目标应用的内存使用率，当然，前提是目标应用也都放在pm2中管理。 12345678910111213141516171819const exec = require('child_process').exec;var Later = require('later');var schedule = Later.parse.text('every 5 mins'); // 每5分钟正点触发Later.setInterval(function()&#123; exec('pm2 jlist', &#123; // 打印出pm2中应用的基本状态信息，输出是json字符串 timeout: 2000 &#125;, (err, data, stderr)=&gt;&#123; if (err) &#123; console.error(err, err.stack); // an error occurred return; &#125; //将结果写入日志 data = JSON.parse(data); if(data[0])&#123; // 这里取0是因为我希望监控的应用在pm2中的顺序是第一位 console.log(data[0].monit.memory/(1024*1024)); // 直接输出到pm2的log中 &#125; &#125;);&#125;, schedule); 然后就等一段时间，就会在对应的log文件中拿到相关的内存数据，然后只需要用电子表格生成一个图标即可，我推荐使用google drive的spreadsheet： 上面的图是我收集了大概2天的内存数据绘制成的图标，可以看出内存使用量成上升趋势。没错，就是泄漏了！！ 友情提醒： 修复内存泄漏可能会耗时很久，你最好先找一个临时方案来维持生计，例如定期重启程序。 搭建环境本着实战为主的策略，我们先从搭建内存泄漏监控环境开始。刚开始参考node-memory-leak-tutorial，以为会很顺利搭建好的，不过碰到了这个error。看Issus应该是个很常见的错误，按照别人的解决方案，尝试切换成nodejs 6.3.1版本进行了测试，确实可以绕过那个错误： 12// 在项目目录下node-debug leak.js 然后终端会启动你的chrome，并停在代码的断点位置，深吸一口气你就可以点击执行了。 备注：若遇到无法创建快照问题，需要多刷新几次哟~ 其它工具我也顺便试了一下： node-memwatch node-webkit-agent node-heapdump 因为它们都需要根据操作系统进行编译，我的本地环境是 win7 64bit，这并不是一个理想的nodejs环境，至少我这么认为，否则也不会碰到恶心的“.net framework”问题。我劝大家千万别学我轻易的就删除了 .net framework 3.5 这个安装包，因为这是win7自带的，删了以后就装不上了，而装更新的4.0+版本的话我这边很重要的一个软件就无法运行了（翻墙你懂的）。在Windows 7系统上安装.NET Framework 3.5框架很不容易的！建议可以用上docker来搭建一个专门用来分析用的容器，这里我就不折腾下去了，its your turn~~ nodejs内存分析的理论姿势在开始听我正儿八经胡说八道之前，推荐你先看几个文档： Memory Terminology How to Record Heap Snapshots Debugging Memory Leaks in Node.js Applications Simple Guide to Finding a JavaScript Memory Leak in Node.js Understanding Garbage Collection and hunting Memory Leaks in Node.js writing fast memory efficient javascript Error Handling in Node.js 一次性看完这些，可能要花很久，如此贴心的我已经帮你看过了，根据我的理解，总结如下： javascript的v8内存管理和java jvm类似，都有新生代（To-Space and From-Space），老年代等； 排查内存泄漏需要分析内存快照，可以使用已有的工具以devtool的profile面板或代码的方式创建snapshot； 创建的快照文件可以导入devtool的profile进行分析； 快照生成的最佳实践是：先保证程序已经预热，然后进行快照1（先触发GC），然后对程序进行一些交互（例如：对于web服务即http请求），再次创建快照2，如此循环来生成多个版本的快照； 合理的利用devtool的profile提供的功能，正确的选择视图； 理解profie中的字段含义： 对象上的黄色标识表示的是javascript直接引用，红色表示间接依赖引用，不太需要关注的是无底色对象，其代表被其它资源引用（如：natvie code）； profile会根据对象的构造方法对对象进行分组归类，每个组对应的“Shallow Size”表示的是该组对象的直接内存占用大小（例如：该类对象自身的原始类型数据的内存占用），对应的“Retained Size”表示的是该组对象依赖的其它对象而造成的内存占用总数（等于自身的Shallow Size + 依赖对象的Shallow Size [ + 依赖对象的依赖对象的Shallow Size [ + 递归下去]]）; 由于性能原因，profile中不会显示对象的整型类型的属性，但是它们并没有丢失，仅仅是工具没有显示出来而已。 应该警惕“distance”比较大或比较小的对象，总之和其它同类型对象的distance不一样就意味着可能有问题； 尽量不要用匿名函数，函数有名字会让分析更容易，其实更推荐的是使用OOP，这样会最容易定位需要追踪的变量，毕竟都是构造器创建出来的嘛； 闭包（匿名函数，定时器等）创建的上下文引用很容易造成不易察觉的内存泄漏； console的相关函数(log, error等)在实际分析中发现其引用的变量无法释放，可以参考#1741，所以你可以在测试代码中替换掉console的相关函数（这样你就不需要改动被测代码逻辑了）； 对象上的事件监听器的闭包最容易造成泄漏，即便是使用once，也可能一次都没有触发而导致该回调函数无限期引用数据。 ok，一大堆姿势足够你花很久时间阅读了。不过并不是说你看了这些内容，就可以轻松战胜困难了。还有一个环节我们没有讨论：若你的项目足够复杂（大），那要怎么搭建项目的测试环境呢？ 这里我认为，大概需要按照下面的步骤来做： 将完整的项目拆解成独立的不同块，并为每个拆解后的小模块写测试代码 针对定时器相关的逻辑，最好改成手动触发，或利用测试库（sinonjs）模拟时间片段 初期可以先尽可能排除依赖的第三方库，最后酌情去测试它们（如果你怀疑是它们的问题的话） 低级别异常伪造（例如socket，file等）要靠伪造对应方法（不推荐使用sinonjs.stubs，因为它保存每次调用时的参数数据，影响你观察，不妨试试mockery） 最终还是有必要放在线上环境实测一段时间来观测问题是否真的修复了 我们主要来说一下第5条，其意味着你要在线上环境想办法导出快照到本地来分析。下面来看看怎么做： 首先，你给线上环境中安装v8-profiler库，它用来提供创建快照的功能。 然后，看一下下面的这段样板代码，其意义在于在你的项目中加载v8-profiler库，并提供一个对外指令用来通知它创建快照文件。 123456789101112131415161718192021222324252627282930313233var fs = require('fs');var profiler = require('v8-profiler');// ---------------// 测试目标function LeakingClass()&#123;&#125;var leaks = [];setInterval(function()&#123; for(var i = 0; i &lt; 100; i++)&#123; leaks.push(new LeakingClass); &#125; console.error('Leaks: %d', leaks.length);&#125;, 1000);// ---------------// 指令服务var koa = require('koa');var route = require('koa-route');var service = koa();var snapshotNum = 1; // 用于为生成的快照进行编号service.use(route.get('/snapshot', function *()&#123; var response = this; var snapshot = profiler.takeSnapshot(); snapshot.export(function(error, result) &#123; fs.writeFileSync((snapshotNum++) + '.heapsnapshot', result); snapshot.delete(); response.body = 'done'; &#125;);&#125;));service.listen(2333, '127.0.0.1'); // 推荐绑定内网ip，不要允许外网访问该服务 每次请求http://127.0.0.1:2333/snapshot，你都会在项目根目录生成一个快照文件，然后把它下载到本地磁盘就可以在chrome里随时进行分析了。 总结在实际排查过程中，发现最难测试的还是依赖的第三方库的泄漏问题。毕竟你无法理解它们的实现。但不可能所有逻辑都自己来完成，所以面对各种各样的第三方类库，还是建议选择尽可能权威的，主流的。剩下那些很小的功能模块，就只能花时间研读其实现代码了。 如果你的业务采用了生产消费者模式，你的测试脚本一定要保证任务的生产和消费的速率保持同速率（或者干脆确保消费者处理完一批次的任务耗时一定要小于批次创建的间隔时间），不然由于任务得不到处理，必然会产生任务积累，看起来就好像有内存泄漏一样，但其实这种情况其实是合理的，只是说明你的消费者太少了而已。 另外，一定要最大频度的，尽可能长时间的运行测试代码，才能明显的暴露出问题，例如： 123setInterval(function memoryleakBlock()&#123; // 待测试的代码块&#125;, 100); 注意在上面memoryleakBlock中避免引用全局变量哟，这样你运行一夜，第二天上班来看结果（如果它还跑着的话）。","tags":[{"name":"内存泄漏","slug":"内存泄漏","permalink":"https://blog.kazaff.me/tags/内存泄漏/"},{"name":"memory leak","slug":"memory-leak","permalink":"https://blog.kazaff.me/tags/memory-leak/"}]},{"title":"Request使用proxy导致的socket连接数泄漏问题","date":"2016-12-30T09:37:12.000Z","path":"2016/12/30/Request使用proxy导致的socket连接数泄漏问题/","text":"这是一篇快餐文章，源于前天接到了服务器预警邮件，提示：连接数超过阀值。 快速定位问题，发现是因为之前跑的一个nodejs开发的小程序，该程序用于定时去指定网站采集相关数据的。 由于要采集的网站需要设置翻墙代理，很麻烦的。之前直接使用Request来发http请求也没碰到这个问题。所以归结为是由于使用proxy不稳定，导致大量的socket异常。 在进入主题之前，先吐槽一下Request这个库，它的文档看似详实，但如果你仔细阅读就会发现很简陋。很多地方感觉欲言又止，很是让人烦躁。 尝试去看它的源码，发现它返回异常的方式采用了2种不同的方法（一般异步方法都如此处理）： 沿用nodejs的回调返回异常风格 发布异常事件 这也是一开始总是被无法捕获的异常导致程序crash的原因。 去github上的issues翻到别人的异常捕获异常方法，也是醉了： 1234567Request.get(&#123;/*some config*/&#125;, function(err, response)&#123;/*回调逻辑，第一个参数为error*/&#125;).on('error', function(error)&#123; // 再次绑定error事件 console.error('Request on error'); console.error(error.stack);&#125;); 按道理说，这两种方式同时只需要用一种，同时使用两种则会发生重复调用问题。但如果我们没有绑定error事件（只采用回调风格），socket级别的异常是无法捕获得到的，这也就导致了上面的方案。不过经过绑定了这个error事件后，发现几乎Request触发的所有异常都被其捕获到了，不会再因为无法捕获的异常而导致程序crash了（还存在一些异常，我建议process.on(&#39;uncaughtException&#39;)还是不能省）。 接下来主题，先看一下一个issue，题主已经详细描述了问题也给了解决方案。实测了一天，感觉确实解决了，至少从图标中观测正常许多了： 如果去看源码，会发现很绕，毕竟到处可见的回调和事件流。不过从解决方案上来看，很直观。思路就是在: 确保在链接出现异常后也触发回调，在回调中来处理善后工作，最终会回收相关资源，而不是直接清除掉必要的事件监听器。 去看源码吧，你将会收获更多。","tags":[{"name":"socket","slug":"socket","permalink":"https://blog.kazaff.me/tags/socket/"},{"name":"proxy","slug":"proxy","permalink":"https://blog.kazaff.me/tags/proxy/"},{"name":"Request","slug":"Request","permalink":"https://blog.kazaff.me/tags/Request/"}]},{"title":"去年的今天就是你的Deadline","date":"2016-12-11T09:37:12.000Z","path":"2016/12/11/去年的今天就是你的deadline/","text":"我最近有个感悟：时间，最复杂的存在。 像我这样一个从没有跨过时区的孩子，是感受不到时间算法的复杂度的。直到有一天，我们的系统需要面对多时区的场景，我才恍然大悟。简简单单的一个时区问题都已经让系统很多基于时间的逻辑变得复杂，更不要提分布式时序问题了（我们今天并不讨论这个问题，我也讲不好）。时区什么的并不是我们今天的讨论重点，其实我想说的是“去年同日”这个问题。本来觉得这个不是啥问题，只需要将给定的日期减去一年即可得呀： 123console.log(moment('2016-12-11').subtract(1, 'years').format('YYYY-MM-DD'));// orconsole.log(moment('2016-12-11').subtract(366, 'days').format('YYYY-MM-DD')); That’s All! 这里要安利一个很强大很主流的时间库：Moment，本文重度依赖该库。 可现实总是残酷的，从业务角度来思考的话，这种“去年的今天”意义是不大的。举个例子，通常运营人员希望得到的时间关系是：今天是12月的第一个星期三，那去年12月的第一个星期三呢？ 这种对应关系中，线索包含：年份，月份，星期几，第几个。打开日历，我们观察一下每个月，会发现每月包含的周个数是不同的，有包含5周的，有包含六周的，极端情况下也会有包含4周的（2026-02）。这就导致一个问题，如果今年的当月包含五周，而去年同月只有四周，那么回答“去年X月的最后一个星期三是几号？”就需要特殊对待了，因为描述中使用的“最后一个”，很口语化，但却直接影响了排序方向。 而涉及到第几个星期几，就更复杂了。这里面还有一个隐藏的条件：每周起始天是星期几？我们国家都是星期一是每周的第一天，而美国是按照星期天是第一天哟~~ 小弟我写了一个Moment的扩展，方便来计算下面几个问题： 指定月份包含的总周数，例如：2015年8月份总共包含6周 指定日期属于当月的第几周，例如：当日属于9月份的第三个周 指定日期在当月中是第几个礼拜几，例如：当日为9月份的第二个礼拜三 指定月份中指定序号的指定星期的日期，例如：9月份第二个礼拜三的日期 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586(function () &#123; var moment = (typeof require !== 'undefined' &amp;&amp; require !== null) &amp;&amp; !require.amd ? require('moment') : this.moment; var firstDay = 0; // 设置每周起始日是星期几，num范围：0~6，0代表星期日，1代表星期一，...，6代表星期六 moment.setFirstDay = function (num)&#123; firstDay = num; &#125;; // 返回指定月份包含的总周数，例如：2015年8月份总共包含6周 moment.totalOfWeekOfMonth = function (date) &#123; var origin = moment(date); var dd = moment(date); dd.date(1); var totalOfWeekOfMonth = 0; var step = 1; var plus = dd.day() !== firstDay ? 2 : 1; // 若1号不是周起始日，相当于第一周，碰见第一个礼拜日时，就需要+2，之后再碰见周日只需要+1 while(dd.month() === origin.month())&#123; if(dd.day() === firstDay)&#123; totalOfWeekOfMonth += plus; plus = 1; step = 7; &#125; dd.add(step, 'd'); &#125; return totalOfWeekOfMonth; &#125;; // 返回指定日期属于当月的第几周，例如：当日属于9月份的第三个周 moment.weekOfMonth = function (date)&#123; var origin = moment(date); var dd = moment(date); dd.date(1); var weekOfMonth = dd.day() === firstDay ? 0 : 1; // 若该月第一天是周起始日，为了避免下面循环中重复累加，初始值应该为0 while(dd.date() &lt;= origin.date() &amp;&amp; dd.month() === origin.month())&#123; if(dd.day() === firstDay)&#123; weekOfMonth++; &#125; dd.add(1, 'd'); &#125; return weekOfMonth; &#125; // 返回指定日期在当月中是第几个礼拜几，例如：当日为9月份的第二个礼拜三 moment.numOfDayOfWeek = function (date)&#123; var origin = moment(date); var dd = moment(date); dd.date(1); var numOfDayOfWeek = 0; while(dd.date() &lt;= origin.date() &amp;&amp; dd.month() === origin.month())&#123; if(dd.day() === origin.day())&#123; numOfDayOfWeek++; &#125; dd.add(1, 'd'); &#125; return numOfDayOfWeek; &#125; // 返回指定月份中指定序号的指定星期的日期，例如：9月份第二个礼拜三的日期 moment.dateOfNumOfDayOfWeek = function (date, dayOfWeek, num)&#123; var dd = moment(date); dd.date(1); var month = dd.month(); var times = 1; while(dd.month() === month)&#123; if(dd.day() === dayOfWeek)&#123; if(times === num)&#123; return dd.format('YYYY-MM-DD'); &#125; times++; &#125; dd.add(1, 'd'); &#125; return undefined; &#125; if (typeof module !== \"undefined\" &amp;&amp; module !== null) &#123; module.exports = moment; &#125; else &#123; this.moment = moment; &#125;&#125;).call(this); 123456789101112131415161718&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;script src=\"//cdn.bootcss.com/moment.js/2.17.1/moment.min.js\"&gt;&lt;/script&gt; &lt;script src=\"./moment-sameday.js\" charset=\"utf-8\"&gt;&lt;/script&gt; &lt;title&gt;&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;script type=\"text/javascript\"&gt; moment.setFirstDay(1); console.log(\"当月总周数：\", moment.totalOfWeekOfMonth('2026-02-01')); console.log(\"当日为第几周：\", moment.weekOfMonth('2026-02-02')); console.log(\"当日为第几个星期几：\", moment.numOfDayOfWeek('2026-02-02')); console.log(\"当月第几个星期几是几号：\", moment.dateOfNumOfDayOfWeek('2026-02', 0, 1)); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 至于上面提到的“最后一个”问题，在你的业务中到底如何计算，我相信你可以通过上面4个方法的组合应用来得到你要的答案！我能帮的就这么多了，好自为之哟~~ 我的项目，获取去年同日的最终逻辑： 123456789101112131415161718192021222324252627// 假设dd为当日，targetDate为去年同日// ...var targetDate = null;// 计算当日是本月内的第N个星期Xvar day = dd.day(); //星期几var num = moment.numOfDayOfWeek(dd); //第几个// 计算去年当月的第N-1, 第N和第N+1个星期X的日期var tmpDd = moment(dd).subtract(1, 'years');var dates = [];dates.push(num &gt; 1 ? moment.dateOfNumOfDayOfWeek (tmpDd, day, num - 1): undefined);dates.push(moment.dateOfNumOfDayOfWeek (tmpDd, day, num));dates.push(dates[1] ? moment.dateOfNumOfDayOfWeek (tmpDd, day, num + 1): undefined);// 计算去年当月三个目标日期与今年当日的距离（日期距离）for(var i = 0, max = dates.length, duration = 31; i &lt; max; i++)&#123; if(dates[i] !== undefined)&#123; var tmpDuration = Math.abs(moment(dates[i]).date() - dd.date()); if(tmpDuration &lt;= duration)&#123; targetDate = dates[i]; duration = tmpDuration; &#125; &#125;&#125;// targetDate已经计算出来了","tags":[{"name":"momentjs","slug":"momentjs","permalink":"https://blog.kazaff.me/tags/momentjs/"},{"name":"去年同日","slug":"去年同日","permalink":"https://blog.kazaff.me/tags/去年同日/"}]},{"title":"nodejs下DNS缓存问题","date":"2016-11-15T09:37:12.000Z","path":"2016/11/15/nodejs下DNS缓存问题/","text":"无意间看到一个文章，是关于nodejs下发送http请求不会缓存dns结果的。这意味着，如果你基于nodejs写了一个http采集程序，不提供dns缓存则会让每次请求都傻傻的重复解析域名为ip地址。听起来会非常影响性能不是么？我的项目中，发送http请求并不是使用的node原生的http库，而是依赖一个常用的Request库。我查阅了一下该库的相关文档和github issue，也发现了一些和dns相关的帖子。不过多数说的是，关于dns问题，本身并不是Request库的范畴，而归结于nodejs的内核问题。omg，感觉好深奥啊！ 幸好，上面提到的那篇文章中也提出了两个解决方案： 应用级别：dnscache 操作系统级别：Bind, dnsmasq 和 unbound 不论是哪个方案，看起来似乎都很简单，只是安装并初始化即可。但问题是，我们怎么来验证它们真实有效？由于我本地的开发机操作系统环境是win7 64bit，所以上文提到的操作系统级别的方案我无法测试。那我们就来看一下应用级别方案到底是否有效吧~~ 首先，我们需要让win能追踪dns请求，这里我找到了一个软件，下载后不需要安装直接运行即可。然后，我们还需要一个清除缓存的方法，可以看这里，简单说就是在终端中执行： ipconfig /flushdns 工具就准备完毕了，我们创建一个测试脚本： 123456789101112131415161718const Request = require('request');function fetch(url, callback)&#123; Request.head(&#123; url: url, timeout: 10000, tunnel: true, gzip: true, proxy: false, followRedirect: false &#125;, callback);&#125;let now = Date.now();fetch('http://blog.kazaff.me', function(err, response, body)&#123; console.log('lookup time without cache: ', Date.now() - now);&#125;); 好的，现在打开DNSQuerySniffer，然后先清理一下本地DNS缓存，一切就绪后执行我们的测试脚本node test.js。你会在DNSQuerySniffer中看到一次DNS请求及其相关信息。在一定的时间间隔内，反复运行我们的测试脚本你会发现并不会再次触发DNS请求，这说明什么？我的win7环境本身就自带操作系统级别的DNS缓存（只是缓存时间很短）。 修改我们的测试脚本如下： 12345678910111213141516171819202122232425262728const dnscache = require('dnscache')(&#123; \"enable\": true&#125;);const Request = require('request');function fetch(url, callback)&#123; Request.head(&#123; url: url, timeout: 10000, tunnel: true, gzip: true, proxy: false, followRedirect: false &#125;, callback);&#125;let now = Date.now();fetch('http://priceline.com', function(err, response, body)&#123; console.log('lookup time without cache: ', Date.now() - now); setTimeout(function()&#123; now = Date.now(); fetch('http://priceline.com', function(err, response, body)&#123; console.log('lookup time with cache: ', Date.now() - now); &#125;); &#125;, 2000);&#125;); 这次我们在执行测试脚本后，快速清空本地DNS缓存（如果你手速不快，可以适当延长setTimeout的触发间隔），你会发现，两秒后的http请求并没有重新查询DNS，这说明什么？很明显，我们的应用自己维护了DNS缓存，所以第二次请求根本就不会关心操作系统本地是否存在对应的DNS缓存记录。 SO，我们如果不想让自己的程序依赖操作系统环境，那现在就可以自己来维护DNS解析记录了，哇啦~","tags":[{"name":"dnscache","slug":"dnscache","permalink":"https://blog.kazaff.me/tags/dnscache/"},{"name":"request","slug":"request","permalink":"https://blog.kazaff.me/tags/request/"},{"name":"window","slug":"window","permalink":"https://blog.kazaff.me/tags/window/"}]},{"title":"Javascript项目单元测试小结","date":"2016-11-14T09:37:12.000Z","path":"2016/11/14/javascript项目单元测试小结/","text":"这几天在写一个采集程序，用nodejs。老实讲这是第二次（相对正式）写和采集有关的模块，第一次是因为要定时访问自身项目来触发缓存的，细节可以看这里。后来从上一家公司离职，也没有继续维护这个程序了。这次算是正宗的采集模块（确实是从目标网站中获取有价值的内容，我就不方便透露具体细节了~），而且这个项目是正式产品，不像之前公司那样扯~~这篇文章本身不是讲采集程序细节的，其实个人感觉开源的采集系统已经很成熟了，如果不是有特别特殊的需求，还是不要重复造轮子的好。我们回到主题，讲一下测试。但在这之前呢，我还是要啰嗦一点（毕竟是总结嘛~）：产品。如果你做的是练习项目（毕业设计），或内部使用的小工具，又或是一个垃圾网站外包（总价才1000块），你可能会选择快糙猛的直奔功能去。这没错，我也会这么做（虽然可能强迫症患者（或道德模范）会觉得这么做太丢份儿）。但正式的产品，还是需要认真对待的。那怎么才叫认真对待呢？那就是所有产品相关的事物都要有：监控和测试。 现在很多云厂商提供的所有服务都有配套的监控功能，国内外还有专门做监控服务的公司，此外还有很多开源的监控系统（Open-Falcon初探）供我们选择，所以真的没有什么借口不去做监控。不过，还是推荐使用成熟的商用监控服务，因为监控平台自身的高可用和低性能损耗是非常重要的，也是技术含量较高的。 终于要说测试了，之前我写过几篇和测试相关的文章： 小团队玩不转的测试 casperjs+mocha+chai搭建E2E测试环境 译-Sinon入门:利用Mocks，Spies和Stubs完成javascript测试 前两篇说的都是“端到端测试”，这种测试一般都是在项目对应功能很稳定后，才比较适合进行。之前我们团队在推行这种测试时，由于对应功能出现了比较大的变更，导致开发人员的测试脚本失效，很是影响气势。假如项目文档，这种“端到端测试”还是非常有效的，而且也很有意义。 我们这次要说的是单元测试，应该上面第三篇文章的实战总结篇~~ 单元测试可以在项目初期，甚至没有开始写正式代码之间就开始（TDD），测试颗粒度是函数或方法。你需要把测试目标方法看作是一个黑盒子，根据传递已知的输入参数，来对输出结果进行断言。单元测试无法代替集成测试和端到端测试。 很容易，不是么？但对于正式产品而言，单元测试相当重要，毕竟人不可靠（俗话说好记星不如烂笔头，测试脚本就是烂笔头）。试想一下，你自己3个月前写的代码你确定还记得么（我的记忆只有3天）？一个1000行项目，某个函数你有胆量在几周后随意更改其输入输出结构？即便是你记忆超群，你能确保别人不修改你的代码么？ 我们如何一次性解决这类问题？单元测试，没错。有了测试脚本，下一步就是自动化测试流程（尝试持续集成），这里我们就不展开了。接下来我们主要讲javascript项目的单元测试。 这里用“javascript项目”的目的是：浏览器端项目和nodejs项目都适用。但这并不表示任何javascript代码都可以很容易被单元测试，你依然需要在代码结构上下大功夫，这考验你的代码驾驭能力和代码架构能力。这部分的知识靠的是日积月累，幸运的是大量资料都提供了相当多的最佳实践和准则~~ 在这次的项目测试中，Sinon提供了相当大的便利，但是由于第一次使用，也碰到了一些问题，下面就列出我在写测试项时的一些收获和采坑： 尽可能使用Sinon提供的断言，因为它携带了更多的错误信息供我们排查； 把方法（或函数）想象成黑盒子，不要试图测试方法体中的内部变量状态，关注返回值； 方法中依赖的第三方资源都要尽可能模拟，不仅是为了避免测试环境复杂，也因为阻断它们的变更而导致的破坏； 没有返回值的方法一般都修改了对应模块（或类）的属性状态，尽可能避免开发时写这种方法（函数式编程理念），测试它们需要提供测试脚本访问这些属性的能力（Get方法或直接定义成public）； 测试脚本一定要关注方法输入参数的边缘情况，不要只围绕着正常系走，所有分支都要测试到（盯着方法体代码写测试项，很容易陷入思维困局~~）； 异步逻辑的测试，可以使用Sinon的stub，通过检查stub的调用次数、输入参数和callback来完成单元测试（对于callback不依赖传参的方法，参考第4条）； Sinon的Fake timers使用时记住要先创建fake timer，再导入目标方法的定义，否则sinon无法控制目标方法的时间轴（例如setTimeout）； 关注测试脚本需要的运行时间，尽可能编写运行效率高的测试项，毕竟测试脚本多数情况下都是交给自动化测试流程运行的，时效性意义很大（每次代码提交都可能会触发执行测试）; 由于Nodejs require的缓存机制，如果不同的测试项需要重新加载（参考第七条），则需要在测试项前后分别执行delete require.cache[require.resolve(&#39;目标模块&#39;)];。","tags":[{"name":"test","slug":"test","permalink":"https://blog.kazaff.me/tags/test/"},{"name":"unit","slug":"unit","permalink":"https://blog.kazaff.me/tags/unit/"},{"name":"Sinon","slug":"Sinon","permalink":"https://blog.kazaff.me/tags/Sinon/"}]},{"title":"译-Sinon入门:利用Mocks，Spies和Stubs完成javascript测试","date":"2016-11-11T09:37:12.000Z","path":"2016/11/11/译-Sinon入门：利用Mocks，Spies和Stubs完成javascript测试/","text":"最近写完一个基于nodejs的组件后的我打算为其写一下单元测试，本以为之前了解过相当多的关于测试的知识，应该可以很顺利的搞定，可真的去写测试项的时候才发现依然存在一些需要克服的困难。不过，接下来翻译的这篇文章就是专门针对测试神器的，应该可以帮到和我一样的新手。 原文地址：https://www.sitepoint.com/sinon-tutorial-javascript-testing-mocks-spies-stubs/ 译文当我们写单元测试时一个最大的绊脚石是当你面对的代码过于复杂。 在真实的项目中，我们的代码经常要做各种导致我们测试很难进行的事情。Ajax请求，timer，日期，跨浏览器特性…或者如果你使用Nodejs，则面对数据库，网络，文件操作等。 所有这些事情之所以不容易测试是因为你无法轻易用代码控制它们。如果你使用Ajax，你需要一个服务端来响应请求，这样才能让你的测试项通过。如果你使用setTimeout，你的测试项不得不等待它。如果是数据库或网络，也类似–你需要一个包含正确数据的数据库，或一个网络服务。 真实世界不像那些测试教程里看起来的那样简单。但你知道有一个解决方案么？ By using Sinon, we can make testing non-trivial code trivial! （译者：这个口号不太好翻译，non-trivial） 让我们看看该怎么做。 是什么让Sinon如此重要？简单的说，Sinon允许你去替换代码中复杂的部分，以此来简化你的测试代码。 当我们测试某部分代码时，你不希望受到其它部分的影响。如果有外部因素影响测试，那么测试项将变得非常复杂且不稳定。 如果你想测试一个使用了ajax的代码，你该怎么做？你需要跑一个服务端，并保证该服务端返回指定的响应数据来支撑你的测试项。这很难完成也让运行测试很麻烦。 那如果你的代码依赖时间呢？假如它需要等待一秒钟才执行。怎么办？你需要在你的测试项中使用setTimeout，但这会让测试变得缓慢。想像一下，如果间隔时间很久，例如五分钟。我想你不会希望每次跑测试项都等待五分钟吧。 如果使用Sinon，我们可以搞定这些问题（甚至更多），并减少复杂度。 Sinon是怎么工作的？Sinon通过允许我们简单的创建test-doubles从而帮助我们减少测试项编写的复杂度。 正如它名字一样，Test-doubles作用是在测试中替换某部分代码。上面提到的ajax的例子中，不需要创建服务端，我们可以使用test-doubles替换掉Ajax调用。在timer例子中，我们可以使用test-doubles来控制时间。 听起来可能很复杂，但基本思想很简单。基于javascript的动态性，我们可以替换任何函数。Test-doubles只是在这个思想的基础上走的更远了一些。使用Sinon，我们可以使用test-doubles替换任何javascript函数，并提供很多方便测试的配置。 Sinon中test-doubles分三类： Spies，提供了函数调用的信息，但不会改变其行为（译者注：类似动态代理） Stubs，类似Spies，但是是完全替换目标函数。这可以让你随心所欲的控制函数–抛异常，返回指定结果等 Mocks，提供了替换整个对象的能力 此外，Sinon还提供了其他的辅助功能，本文不包含下面的范围： Fake timers，用来控制时间，例如触发一个setTimeout Fake XMLHttpResquest and server，可以用来伪造Ajax请求和响应 基于这些功能，Sinon可以让你解决测试中遇到的由外部依赖带来的所有复杂问题。如果你学会了Sinon提供的这些技巧，你几乎不需要其它别的工具了。 安装Sinon开始之前，我们需要安装Sinon Nodejs 使用npm install sinon安装sinon 在测试项中引入sinon：var sinon = require(&#39;sinon&#39;); 浏览器 你可以选择npm install sinon，或使用CDN，也可以从官网下载到本地 在你的测试页面引入sinon.js 入门指南sinon包含许多功能，但它们多数都存在关系。你只需要掌握一部分，就会了解剩余部分。这让sinon很容易使用，只需要你了解了基本用法并知道它们之间的差别。 只要我们的代码调用了一个不容易控制的函数，我们通常就需要sinon。 对于Ajax，它可能是$.get或者XMLHttpResquest。对于timer，它可能是setTimeout。对于数据库，它可能是mongodb.findOne。 为了方便我们讨论，后面我将成这类函数为依赖方。我们测试的目标函数依赖其它函数的返回结果。 最常见的使用sinon方式是使用test-doubles替换掉问题依赖方。 当测试Ajax时，我们使用test-doubles替换XMLHttpResquest来伪造ajax请求 当测试timer时，我们伪造替换setTimeout 当测试数据库时，我们使用test-doubles来替换mongodb.findOne来直接返回伪造数据 让我们写点代码吧。 SpiesSpies很简单，但其它很多功能依赖它。 spies的主要用法是收集函数的调用信息。你可以用来验证一些事儿，例如函数是否被调用。 1234567var spy = sinon.spy();//我们可以像调用函数一样调用spyspy('Hello', 'World');//我们可以得到调用信息console.log(spy.firstCall.args); //output: ['Hello', 'World'] sinon.spy函数返回一个Spy对象，该对象可以像函数一样被调用，它记录每次被调用信息。在上面的例子里，firstCall属性包含了第一次调用的信息，例如firstCall.args表示调用时的参数列表。 虽然你可以像上面例子那样创建一个匿名spies，但通常情况下你需要使用spy替换一个其它函数。 12345678910111213141516171819var user = &#123; ... setName: function(name)&#123; this.name = name; &#125;&#125;//为user.setName创建一个spyvar setNameSpy = sinon.spy(user, 'setName');//现在，每次调用目标函数，spy都会记录相关信息user.setName('Darth Vader');//我们可以使用spy对象查看相关信息console.log(setNameSpy.callCount); //output: 1//非常重要的步骤--拆除spysetNameSpy.restore(); 上面例子展示了使用spy替换其它函数的写法，最重要的一点是：当你确定不再需要spy后，你记得恢复原始函数，参考例子中的最后一行。不然测试可能出现非预期行为。 Spies包含许多不同的属性，用来提供不同的信息。spy文档列出了完整的属性列表。 在实际场景中，你可能不会经常使用spies。你更多时候使用的是stub，但是spies用来检测函数是否被调用非常方便： 12345678910111213141516function myFunction(condition, callback)&#123; if(condition)&#123; callback(); &#125;&#125;describe('myFunction', function() &#123; it('should call the callback function', function() &#123; var callback = sinon.spy(); myFunction(true, callback); assert(callback.calledOnce); &#125;);&#125;); 在这个例子中，我们使用Mocha作为测试框架，使用Chai作为断言库。如果你想了解更多信息，可以参考我之前的文章：使用Mocha和Chai来单元测试你的javascript。 See the Pen Sinon Tutorial: JavaScript Testing with Mocks, Spies &amp; Stubs sinons断言在我们介绍stubs之前，我们快速看一下sinon断言。 大多数使用spies（和stubs）的测试方案中，你需要一些工具来校验测试结论。 我们可以使用任何断言来验证结论。前面的例子中，我们使用Chai的assert函数来验证值的真实性。 1assert(callback.calledOnce); 这样做的问题是错误信息并不清晰。你将得到“false was not true”，或类似信息。你可以想象的到，这对于定位错误并不是很有价值，你需要在测试代码中翻找才能最终找到。一点都不美。 解决这个问题，我们可恶意包含一个自定义的错误信息在断言中。 1assert(callback.calledOnce, 'Callback was not called once'); 但如果我们使用sinon的断言库呢？ 12345678910describe('myFunction', function() &#123; it('should call the callback function', function() &#123; var callback = sinon.spy(); myFunction(true, callback); sinon.assert.calledOnce(callback); &#125;);&#125;); 使用sinon断言我们可以得到更多有价值的错误信息。这在当你验证比较复杂的条件时非常有用，例如函数的参数。 下面列出一些sinon提供的其它强大断言的一些例子： sinon.assert.calledWith可以用来验证函数是否使用指定的参数（这可能是我用的最多的一个） sinon.assert.callOrder用来验证函数的调用顺序 sinon断言文档介绍了所有的内容。如果你喜欢使用Chai，有一个sinon-chai-plugin可以让你通过chai的expect和should接口来使用sinon断言。 Stubsstubs归类于test-doubles是因为它的灵活和方便性。它拥有spies的全部功能，此外它还彻底的替换掉了目标函数。换句话说，当你使用spy，原始的函数依然会被调用，但如果使用stub，原始函数就不会被执行了。 这个特性让stub可以胜任许多任务，例如： 替换像ajax或其它外部函数等让测试变复杂或慢的调用 根据函数的响应来触发不同的代码流程 测试不寻常的条件，如抛出异常 我们可以像创建spies一样创建stubs： 123456var stub = sinon.stub();stub('hello');console.log(stub.firstCall.args); //output: ['hello'] 我们创建了一个匿名的stubs，但用stubs来替换存在的函数更有意义。 举个例子，如果你有一段代码调用了jquery的Ajax，测试它将变得麻烦。代码会发送请求到我们配置的服务端，所以我们需要保证服务端的有效性，或者给代码添加特定的分支来适配测试环境 – 这么做真的大错特错。你不应该在代码中编写任何测试特定逻辑。 我们可以使用sinon的stub来替换ajax调用。这会让测试变得简单。 下面的例子中，我们使用ajax向预定url发送一个携带参数的请求。 1234567function saveUser(user, callback) &#123; $.post('/users', &#123; first: user.firstname, last: user.lastname &#125;, callback);&#125; 通常，测试这个函数将变的很麻烦，但我们有了stub，一切变得美好。 假如我们想要确保传递给saveUser函数的回调方法在请求结束后正确的被执行了一次。 1234567891011121314151617describe('saveUser', function() &#123; it('should call callback after saving', function() &#123; //We'll stub $.post so a request is not sent var post = sinon.stub($, 'post'); post.yields(); //We can use a spy as the callback so it's easy to verify var callback = sinon.spy(); saveUser(&#123; firstname: 'Han', lastname: 'Solo' &#125;, callback); post.restore(); sinon.assert.calledOnce(callback); &#125;);&#125;); See the Pen Sinon Tutorial: JavaScript Testing with Mocks, Spies &amp; Stubs 这里，我们将ajax函数替换成了stub。这意味着请求不会被发送，我们不需要一个服务端 – 我们全权控制了我们的测试代码！ 介于我们想确认我们传给saveUser的回调会被执行，我们让stub立刻返回。这意味着stub将自动调用callback参数。这模仿了$.post在请求完成后的行为。 除了stub，我们还创建了一个spy。我们可以使用一个普通的函数作为回调，但使用spy会让sinon.assert.calledOnce更方便验证测试结论。 大多数需要stub的场景，都类似下面步骤： 确认是否包含问题函数，例如$.post 观察并掌握其行为 创建一个stub 让stub来模拟目标行为 stub不需要模拟所有的行为，只需要足够你的测试项使用即可，其它细节可以忽略。 另外一些stub的常用场景是验证一个函数是否使用特定的参数。 举个例子，在我们的ajax函数中，我们希望确定正确的数据被提交。因此，我们可能会这么做： 1234567891011121314151617181920212223242526describe('saveUser', function() &#123; it('should send correct parameters to the expected URL', function() &#123; //We'll stub $.post same as before var post = sinon.stub($, 'post'); //We'll set up some variables to contain the expected results var expectedUrl = '/users'; var expectedParams = &#123; first: 'Expected first name', last: 'Expected last name' &#125;; //We can also set up the user we'll save based on the expected data var user = &#123; firstname: expectedParams.first, lastname: expectedParams.last &#125; saveUser(user, function()&#123;&#125; ); post.restore(); sinon.assert.calledWith(post, expectedUrl, expectedParams); &#125;);&#125;); see the pen Sinon Tutorial: JavaScript Testing with Mocks, Spies &amp; Stubs 这次，我们有创建了一个$.post()的stub，但这回我们并没有让它直接返回。这次我们的测试目标不是回调，因此让它返回并不是必须的。 我们设置了一些变量来存期望的数据 - url和参数。这是一个好的实践，让我们很容易知道什么是测试必须的。也可以帮助我们减少重复代码。 这次我们使用sinon.assert.calledWith()断言。我们将stub传递进去，因为我们想确定stub包含了正确的参数。 使用sinon，还有其它的方法来测试ajax请求。例如使用sinon的伪造XMLHttpResquest功能。我们不会在这里去介绍细节，如果你想了解更多可以参考my article on Ajax testing with Sinon’s fake XMLHttpRequest。 MocksMocks不同于stubs。如果你之前听过mock object这个术语，那没错了 - sinon的mocks用来替换整个对象，并改变其行为。 如果你需要替换某个对象的多个方法，你就应该使用mocks。如果你只是希望替换某个单独的方法，stub更方便。 使用mocks时你需要小心！因为它太TM强大了，很容易让你的测试过于特定 - 测试的太细或太刻意 - 从而让你的测试太容易过期。 与spies和stubs不同，mocks包含内建的断言。当使用mock对象时，你可以定义你期望的结果，你期望的行为。 假设我们使用store.js来保存一些数据到localstorage，我们打算测试这个特性。我们可以使用mock来写测试： 12345678910111213describe('incrementStoredData', function() &#123; it('should increment stored value by one', function() &#123; var storeMock = sinon.mock(store); storeMock.expects('get').withArgs('data').returns(0); storeMock.expects('set').once().withArgs('data', 1); incrementStoredData(); storeMock.restore(); storeMock.verify(); &#125;);&#125;); See the Pen Sinon Tutorial: JavaScript Testing with Mocks, Spies &amp; Stubs。 使用mocks时，我们可以使用链式调用风格来定义期望的调用和结果。这和使用断言验证结果一样，除了我们需要提前定义，并在测试结束时校验它们storeMock.verify()。 调用mock对象的mock.expects(something)会创建一个期望值。意味着mock.something()方法期望被调用。Each expectation, in addition to mock-specific functionality, supports the same functions as spies and stubs.（译者注：只能意会无法言表啊） 你可能会觉得通常stub都比mock更简单 - 没错。Mocks要小心使用。 mock特定的特性，可以查看sinon的mock文档。 重要的最佳实践：使用sinon.test()这里有个使用sion的很重要的最佳实践，不管是使用spies，stubs还是mocks都应该牢记。 如果你用test-doubles替换了一个存在的函数，则使用sinon.test()。 前面的例子中，我们使用stub.restore()或mock.restore()来在我们使用完后清理它们。这很有必要，否则test-doubles将持续有效，这将可能影响其他的测试项并导致错误。 但是，直接使用restore()可能很难，有可能因为某个异常导致restore()没有被调用！ 我们有两种方法来解决这个问题：我们可以自己包装完整的try catch块。这允许我们将restore()放在finally块中调用来确保一切正常。 或者，一个更好的做法是我们可以将测试体写在sinon.test()中： 1234567it(&apos;should do something with stubs&apos;, sinon.test(function() &#123; var stub = this.stub($, &apos;post&apos;); doSomething(); sinon.assert.calledOnce(stub);&#125;); 上面的代码中，注意it()的第二个参数，它被sinon.test()包裹。此外注意我们使用this.stub()代替了sinon.stub()。 使用sinon.test()包裹测试体可以让我们使用sinon沙盒特性，其允许我们使用this.spy()，this.stub()和this.mock()来创建spies， stubs和mocks。任何你在沙盒中创建的test-doubles都会自动被清理。 我们上面的代码中并没有stub.restore() – 托沙盒的福它已经不再需要了。 请尽可能使用sinon.test()，你会避免由于前面的测试项没有清理test-doubles而导致的灵异问题。 Sinon并不是黑魔法Sinon很强大，而且某些时候很难理解它是如何工作的。让我们看一下Sion工作原理的原生javascript的例子，这样我们可以更好的理解其思想。 我们可以自己实现spies， stubs和mocks。使用Sinon只是因为它更方便 – 自己实现会非常复杂。 首先，spy本质上是一个函数wrapper： 123456789101112131415161718192021//A simple spy helperfunction createSpy(targetFunc) &#123; var spy = function() &#123; spy.args = arguments; spy.returnValue = targetFunc.apply(this, arguments); return spy.returnValue; &#125;; return spy;&#125;//Let's spy on a simple function:function sum(a, b) &#123; return a + b; &#125;var spiedSum = createSpy(sum);spiedSum(10, 5);console.log(spiedSum.args); //Output: [10, 5]console.log(spiedSum.returnValue); //Output: 15 我们可以很容易的使用自定义函数来实现spy的功能。但注意sinon的spies提供了非常多的特性 – 包括断言的支持。这让sinon更方便使用。 关于Stub Then？实现一个简单的stub， 你可以简单的替换成一个新的： 1234567var stub = function() &#123; &#125;;var original = thing.otherFunction;thing.otherFunction = stub;//Now any calls to thing.otherFunction will call our stub instead 但是，sinon的stub提供了许多更好用的功能： 它们拥有spy的全特性 你可以调用stub.restore()来恢复原始的行为 你可以结合sinon的断言 Mocks simply combine the behavior of spies and stubs, making it possible to use their features in different ways. 尽管有时候sinon看起来像个“黑魔法”，但它的大多数功能其实很容易自己实现。但比起自己来实现一套来说，sinon非常方便使用。 总结真实项目的测试有时非常的复杂，导致你可能彻底放弃。但是使用sinon，测试变得非常简单。 记住一个重要的准则：如果一个函数很难被测试，尝试使用test-doubles替换它。 想知道更多关于如何让你的代码使用sinon？当我的网站来，我会提供Sinon in the real-world guide给你，包含了sinon的最佳实践，和三个真实的例子来讲解如何在不同的测试方案中使用它。","tags":[{"name":"test","slug":"test","permalink":"https://blog.kazaff.me/tags/test/"},{"name":"ajax","slug":"ajax","permalink":"https://blog.kazaff.me/tags/ajax/"},{"name":"mock","slug":"mock","permalink":"https://blog.kazaff.me/tags/mock/"},{"name":"sinon","slug":"sinon","permalink":"https://blog.kazaff.me/tags/sinon/"}]},{"title":"一次被咨询后的感触","date":"2016-11-07T09:37:12.000Z","path":"2016/11/07/一次被咨询后的感触/","text":"上周末陪着朋友去谈项目，刚好派单方是我以前的同事（好吧，其实我就是传说中的介绍人~）。一进办公室，对方的程序员就开始直奔主题，让我感到些许的不舒服。别误会，我十分喜欢开门见山直奔主题，但项目背景肯定是不能省的，直接进入具体功能细节真的让我感觉不适应。不过考虑到我只是个旁听，所以也就耐着性子听了一会~不一会老板来了，我就被拉出来私聊了。和老板的一番交谈，刚好弥补了项目一些背景信息。原来之所以打算外包出去部分功能，是因为这个项目出现了不小的问题。这刚好解答了我本来的疑问：为什么明明自己有程序员，却还要找外包？ 梳理了一下，在这个老板的眼里，主要就一个问题：项目迟迟不能上线，他感觉被拖住了。这种感觉其实我的领导也有，我也因此被叫过去谈话好几次~~工作了七八年，凭借经验我第一直觉就是：需求变化问题。果不其然，这个项目的需求一直在不停的变化，大到需求小到界面，都随着老板日复一日的新想法而发生在或大或小的变动。当然，另一方面也和程序员有关，例如经验问题，能力问题，心态问题等。这部分我不想深谈，并非是因为我也是程序员而不想找程序员的问题，仅仅是因为我对他们的那个程序员不了解（后面通过和那个程序员接触我确实也发现了一些问题，后面再说~）。 我们先从需求本身来分析，问题如下： 需求变动太频繁 需求假设太严重 产品胜过使用者 很多我去过或见过的小公司的it项目都这样，老板突然有了个想法，自己其实也没想太透，更没有去验证和评估自己的设想，反正公司有开发人员，做呗。今天一个新想法，明天一个新创意，后天就能出来一个颠覆整个项目的大构想。身处这样领导的部门下面写代码，真TM的命苦啊！ 那该怎么办？严格控制单次迭代的需求变更次数，尽量做到不对当前迭代进行任何需求变更。说起来简单，但很难做到，毕竟这个世界是在时刻变化的。但，至少我们应该时刻记住这个约束，尽最大努力保证它的有效性，而不是无视它。无视它的后果就是项目的无限期延迟。（我所负责的项目，某个模块前后足足做了3个月，而最初预估用时是1个月，多出来的2个月都是因为需求不停的改动花费的~） 我这么提到这点后，老板则是这么回答的：那明明知道不改的话等于白做，也不提么？是个好问题，但这个情况值得深思~尽管我承认随着时间需求会产生变化，但是否每周都会有？是否每次变更都是颠覆性质的？我很好奇。 为什么会这样呢？我个人觉得，是因为成本预估没有有效的量化方法。身为老板，他决定是否启动一个项目时，首先要考虑的应该是资源的投入问题：时间成本，人力成本。当然，他需要一个程序员老司机（其实就是项目经理）来反馈一些数据来辅助他做决策。其实多数情况下，老板并不是最终的系统用户，他只是系统的间接受益方。而实际上呢，老板可能并不这么认为，所以他才会武断的提出所谓的创新和想法，他也固执的以为自己的这种想法真的可以有效的帮助到系统的真实用户。所以，在交谈时我有意无意的问他：这些设计是不是真的和用户确认过了？而他的答案呢，则莫能两可~~我内心只能呵呵。那应该怎么样呢？大公司往往这部分工作是有专门的产品经理来负责的，产品的定位，功能，甚至界面交互他们都会琢磨和思考。而且，职位的使命感和责任感会让他们不自觉的思考更多的维度，毕竟如果项目失败了，他们有可能面临被砍掉的风险。而这种无形的压力和约束力是老板不会有的（老板的压力来自于其他方向，我并不否认老板存在更大的多的压力）。 我私下和他们公司的一个部门经理聊（也是我的朋友），得到的数据确实证实了我的猜测，关于这个项目的很多想法真实的使用者并不买账，除了感觉复杂以外并无其它。我后来给他们提到了一个产品哲学问题：到底是人用产品，还是产品用人。听起来这是一个很搞笑的是选择题，对吧？！但我认为真的应该认真思考一下这个问题。 他们公司的产品，涉及到一个自定义订单处理流程的模块，他们参考一些现有OA系统的设计，弄出来一套非常灵活（有点bt）的工作流自定义方案。乍一听非常合理，也很强大，但我总觉得怪怪的~ 奇怪的地方在于，他们公司的规模：一个不超过100人的公司（包括保洁员），业务范围也相对专一（互联网营销），每年的订单量大概只有四位数（包括作废订单），办公场所不超过300平米（大厅，会议室，总经理办公室玻璃墙隔开）。这个项目他们一开始是自用，未来考虑卖给其他公司。通过询问，老板说这个项目的目标客户也都是此规模的小型公司为主。 ok，那么问题来了，公司真的需要这么灵活复杂的产品么？我们来仔细思考一下：订单处理流程。按照他们的需求，可以为指定类型的订单来设计每一个处理流程，每个流程对应系统里的部门及其中的负责人，包含处理所需时间，包含其它一些业务选项。流程也涉及到异常处理，状态提醒等（我也只听了这么多）。 这样的功能其实并不罕见，也很合理，经得起推敲。不过我的关注点则在于：考虑过使用者的感受么？其实订单处理流程一直都存在，我的意思是，即便是没有软件系统来定制，公司的规章制度也无时无刻在发挥作用。即便规章制度存在死角，但人是会变通的，公司没有倒闭就证明这不是问题。人类是社会性动物，意味着无时无刻都活在规则里，不存在绝对自由。同时，人又是自私的，怕担风险的。这一切的事实，就证明了，即便是系统不提供这样的明确要求，一个订单从开始到结束都会遵循合理的流程。 我给他们举了个例子：新员工小明有一个订单需要提交，他会怎么做？他一定会去问经理，经理一定会知道提交给谁，即便是这两个“一定”出现了错误，小明错误的把订单提交给了小美，但小美自己知道处理不了会怎么做？ 那按照我的逻辑，工作流就没有意义了么？当然不是，如果是跨国公司，或者公司人数上万，办公场所不在一起，业务范围繁多，都会需要一个相对明确的工作流程。但注意，即便如此，我依然不认为固化工作流程太细是好事儿（例如上面提到的“指定处理时间”或“指定处理人”）。 就拿“在工作流程安排中指定处理人”来说，人是可以请假的，对吧？所以，我的意思是，不要把软件设计成一座用户的监狱。是的，没错，我不太同意把用户当成傻瓜的设计哲学，这有悖于人的自我认知，毕竟没有人觉得自己真傻。 另一个层面，我们需要设计一套完善的数据分析逻辑，来让使用者查看系统中错误订单的错误原因，尤其是甄别出恶意操作行为。系统收集这些数据的方案很多，尽量保证不要影响用户的使用成本。这样似乎才能更好的打造一款产品吧（我似乎不确定一定可以！）。 上面我们一口气说了那么多，解释了我对那三个问题的看法。接下来花些时间来说说这个公司的程序员。 可能是作为旁观者更容易发现问题吧，我觉得这个程序员（或者说大多数程序员）有一个问题：不懂得化繁为简。更可怕的是，很多程序员还总化简为繁来寻找成就感（几年前我就这么干）。 当然，这和工种有关，这也证明一个稍微大一点的项目，都需要一个项目经理。直接交给程序员来分析功能只会加速这个项目的崩塌，因为他们思考问题容易陷入细节，没有大局观。因为他们的成就感来自于解决某个特定的技术问题，只有这样他们才会感觉幸福。如果让他们思考技术之外，人的问题，他们的思维就会变得慵懒，拖延症。但，正如之前我说的，产品是辅助人使用的。你可以不认可这个观点，但你总不能推翻“产品是人用的”这个论点吧，既然如此，你怎能忽略“人”呢？ 我不否认我推崇“技术至上”的价值观，但多年的工作经验让我明白：解决方案总比问题多。我也喜欢刨根问底，多年的刨底经验告诉我，如果一个业务问题复杂度超过预期，那一定是人的问题。这个时候，通常需要冷静，跳出来思考，另辟蹊径总能达到更好的效果。 说了这么多，你一定以为我成功的影响到了对方，辅助他们做出了更合适的选择，对吧？！其实，这次咨询经历是很失败的，至少对于他们来说，因为我并没有提供任何被采纳的建议，哇哈哈哈~他们的需求还是一如既往的复杂~如果让我分析原因，我只能归结于： 我并没有足够的权威来辅助我的观点 我并没有强大表达能力 对方并没有足够的失败经验来辅助消化我的建议 其实，尴尬的是，我自己负责的项目，依然存在上面说的所有问题。我连自己的客户都搞不定，还怎能奢望去搞定他人呢？哇哈哈哈~~","tags":[]},{"title":"十月份杂谈","date":"2016-10-31T09:37:12.000Z","path":"2016/10/31/十月份杂谈/","text":"Long time no write. 整整一个多月没有更新blog，原因有很多，但主要还是忙~ 现在工作岗位偏重需求分析，我需要花大量的精力在与需求方一起讨论工作流程和系统的落地方案上。纯粹的技术性质的问题一般都会交给其余同事负责跟进。所以一时不知道应该写什么在博客，毕竟之前博客都是偏向具体技术问题的主题。 需求分析既然提到了需求分析，那也来说说这方面的体会。其实，与客户沟通，以前也不是没做过，只是现在更偏重于这个方向，最明显的体会在于：你一定要做好充分的准备，因为你是业务人员和技术人员之间的桥梁。你必须既能够消化业务需求，又可以和技术人员共同确定实施方案，最好还可以独自将业务背后隐藏的一些要素给挖掘出来，这样程序员在理解你的方案时才能够充分的明白上下文环境。否则，你可能会处于一种两面夹击的境地之中：一面是业务人员给你提出各种莫能两可的需求，还不停的责怪你为何无法理解；一方面则是程序员对你提出的实施方案产生质疑，始终无法投入编码。 其实要规避这种境界也不难，嗯~~至少，说起来不难： 必须和双方都建立一个良好的人际关系：对于业务人员，你要让对方当你是朋友，这样会更有利于沟通；而对于程序员，则简单多了，你要让他信服你，只需要在关键时刻拿出自己的技术实力即可，例如：讨论方案时对某领域透彻的分析，或辩论时能在技术层面提出强有力的质疑并顺带拿出更好的方案。毕竟嘛，团队协作，以人为本，所以人际关系永远是最重要的。 最好可以直接和真实用户接触，如果和你提出需求的永远是间接用户，那你很难拿到需求真正的痛点，从而存在纸上谈兵的风险。 任何时刻都应有轻重主次划分，此时团队整体的事务永远大于个人。例如，我也会实际负责一个模块的开发，但我不能总是试图先把自己负责的模块弄好再管别人，这就大错特错了。 总之，要明白的是，公司不会因为我自己负责的模块做的多完美而表扬我，因为我的本职工作是确保项目整体交付。 云服务现在做一个web项目，早已不和以往一样了。大量成熟高效的基础设施和公共服务已经摆在你眼前，它们既廉价又强大，不仅省去了你自己运维的成本，而且还提供了非常高的可用性和安全性。可以说，现在的程序员，真是TM的幸福，更幸福的是：我就是现在的程序员！ 由于项目的原因，客户更信赖Amazon，但就我之前使用阿里云的经验告诉我，国内云服务也非常的棒！不论如何，如果你在拿到需求后第一时刻如果不是对照云服务来设计你的方案，那你真的是落伍了。 凭我的个人经验告诉我： 绝对不要自己来维护数据库，因为你的能力真的解决不了产品上线后的数据安全和稳定问题，放手使用RDS吧； 绝对不要搞vps，因为它太麻烦了，你可以选择云实例（阿里的ECS或AWS的EC2），至少它提供了更好的监控措施和便捷的状态管理。不过，更佳的策略是使用应用容器（AWS的Elastic Beanstalk）； 绝对不要自己来管理文件，还是交给文件云吧，阿里的OSS，AWS的s3，国内的七牛又拍等。 其它诸如消息队列啊，数据分析等，更是应该首选云服务，因为成熟嘛~不过需要解决的我认为最重要的问题是：说服老板！！！ 写代码其实，我的岗位工作内容中是没有被要求必须写代码的。相反，是我一直在争取写代码。一开始是搭建前端团队要用的框架，再后来是做一些相对独立的中间件服务。我认为，永远不能远离代码，前面提到的技术素养全靠写代码来培养。 如果你被团队当成外行来看，那你在团队里基本上任何工作都难以开展，毕竟程序员的世界里，真的容不下产品经理！！！ 总结和分享这也是这篇文章存在的理由~~ 写在最后《行尸走肉》第七季，一定要看！《生活大爆炸》第十季，不能错过！","tags":[]},{"title":"如何基于接口文档生成模拟数据","date":"2016-09-21T09:37:12.000Z","path":"2016/09/21/如何基于接口文档生成模拟数据/","text":"前后端分离是目前主流的团队开发方式，有效的隔离不同技术领域开发人员的依赖，包括开发进度上的依赖和代码文件上的依赖。而做到这一点，全靠一个规则：依赖接口而不依赖实现。 那么，web领域前后端分离，通常都会选择基于http+json的方式，也就是大家说的restful类型的接口。在进入代码编写之前，团队会定义好所有依赖的rest接口，前后端开发人员遵照接口文档各自完成自己的工作，完美！那么，在前后端开发进度不一致的情况下，可能会出现前端界面完成了但对应的后端服务还没有完成，怎么办？这个时候一般都是会让前端人员自己来提供一个模拟的响应数据，这种方案简单有效，瞬间前端人员就可以模拟出所需的服务接口响应。但是，问题在于，随着需求的变化，接口会发生变更，此时会影响前后端的代码，但往往会由于各种实际原因，之前的模拟接口响应的脚本并没有随着接口的变更而更新，对未来的测试留下了隐患~ 怎么做最理想？几年前我在之前的团队推广过阿里的一个开源项目：RAP。虽然最终并没有大规模使用，但通过调研，RAP真心是个不错的解决方案，尤其是对国内的项目，本地化支持度非常高。该项目也一直在持续更新，如果有兴趣，我首先推荐尝试RAP。 不过这里，我们并不会采用RAP，理由可能有些牵强，仅仅是因为团队已经存在大量的接口文档在swagger标准上，并且组员也已经相当熟悉这个规范，他们不会接受突然切换到RAP上而带来的学习成本。 不过没关系，swagger作为rest文档界的事实标准，自然应该存在大量的模拟数据的解决方案，所以我花了几天的时间查阅了相关的资料（其实就是不停地google），找到了几种方案。 不过目前我能找到的，swagger下的对应解决方案都不像RAP那样开箱即用，多数都是需要自己完成组合才能投入使用的。这里我罗列一下相关的库： swagger-express-middleware swagmock swagger-node swagger-cli 最终，我决定基于 swagger-node + mockjs 来完成我们的根据swagger接口文档生成模拟数据的目的。之所以选择swagger-node，是因为它自身就容纳了很多方便的工具，例如：swagger editor：swagger.io在线版的编辑器，你可以直接跑在自己的服务器上，并且它是实时持久化到文件中且同步到mock server的。 安装和配置swagger-node非常的简单，只需要按照官方的步骤完成即可。装好后按照推荐的用法，我们需要先swagger project create一个项目出来，记得选择express作为服务端框架。swagger-node会按照预设为我们创建好一个标准规范的项目文件结构。 直接执行swagger project start就可以运行一个解析基于swagger接口标准规范文档并自动化创建对应服务的后端服务。该服务可以校验每次接口的请求，根据接口文档的要求来返回匹配的结果数据。 基本上完美，就是我们想要的。不过，美中不足的是，目前swagger-node根据接口文档返回的模拟数据是静态的，例如，整型只会返回1，字符串类型只会傻傻的返回Sample text。这显然无法满足我们的要求，唉，真可惜！ 不过庆幸的是，我在该项目的issue中找到了一个很有价值的贴，提供了一个非常不错的解决方案。其实也很好理解，就是将原本swagger-node用来生成模拟数据的逻辑修改成符合我们要求的逻辑即可。同时该方案也让我关注到了mockjs项目，应该是个国人的项目，非常棒！其实国外也有不少同类型的项目，但说到本地化，还是我们自己国家的开发人员最贴心。并且看完文档后，发现它的扩展性也非常不错，简单直观，赞！ 好的，一切就绪，开始改造吧！ 先找到\\你的项目\\node_modules\\swagger-express-mw\\node_modules\\swagger-node-runner\\node_modules\\swagger-tools，目录结构比较深~ swagger-tools提供了我们需要修改的中间件middleware\\swagger-router.js。由于要使用到mockjs，所以需要将对应的文件放到lib文件夹下。 接下来就要修改代码了，使用你心爱的编辑器打开swagger-router.js： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151//找到getMockValue方法，大概在107行var Mock = require('../lib/mock');var getMockValue = function (version, schema) &#123; var type = _.isPlainObject(schema) ? schema.type : schema; var xmock = _.isPlainObject(schema) ? schema['x-mock'] : schema; var Random = Mock.Random; var value; var temp; if(xmock)&#123; var data = xmock.split(\"|\"); type =data[0]; if(data.length&gt;1) temp = data[1]; &#125; if (!type) &#123; type = 'object'; &#125; switch (type) &#123; case 'guid': value = Random.guid();break; case 'id': value = Random.id();break; case 'step': value = Random.increment(temp);break; case 'title': value = temp==='cn'?Random.ctitle():Random.title();break; case 'text': value = temp==='cn'?Random.cparagraph():Random.paragraph();break; case 'color': value = Random.hex();break; case 'ip': value = Random.ip();break; case 'email': value = Random.email(temp);break; case 'firstname': value = temp==='cn'?Random.cfirst():Random.first();break; case 'lastname': value = temp==='cn'?Random.clast():Random.last();break; case 'cname': value = Random.cname();break; case 'name': value = Random.name(temp === 'middle');break; case 'url': value = Random.url(temp);break; case 'date': value = Random.date(temp);break; case 'time': value = Random.time(temp);break; case 'datetime': value = Random.datetime(temp);break; case 'array': value = []; for(var i = 0; i &lt; temp &amp;&amp; i &lt; 20; i++)&#123; value.push(getMockValue(version, _.isArray(schema.items) ? schema.items[0] : schema.items)); &#125; break; case 'boolean': if (version === '1.2' &amp;&amp; !_.isUndefined(schema.defaultValue)) &#123; value = schema.defaultValue; &#125; else if (version === '2.0' &amp;&amp; !_.isUndefined(schema.default)) &#123; value = schema.default; &#125; else if (_.isArray(schema.enum)) &#123; value = schema.enum[0]; &#125; else &#123; value = Random.boolean(5, 5, true)||1;; &#125; // Convert value if necessary value = value === 'true' || value === true ? true : false; break; case 'file': case 'File': value = Random.image();break; case 'integer': if (version === '1.2' &amp;&amp; !_.isUndefined(schema.defaultValue)) &#123; value = schema.defaultValue; &#125; else if (version === '2.0' &amp;&amp; !_.isUndefined(schema.default)) &#123; value = schema.default; &#125; else if (_.isArray(schema.enum)) &#123; value = schema.enum[0]; &#125; else &#123; value = Random.integer(1,1000)||1; &#125; // Convert value if necessary if (!_.isNumber(value)) &#123; value = parseInt(value, 10); &#125; //TODO: Handle constraints and formats break; case 'object': value = &#123;&#125;; _.each(schema.allOf, function (parentSchema) &#123; _.each(parentSchema.properties, function (property, propName) &#123; value[propName] = getMockValue(version, property); &#125;); &#125;); _.each(schema.properties, function (property, propName) &#123; value[propName] = getMockValue(version, property); &#125;); break; case 'number': if (version === '1.2' &amp;&amp; !_.isUndefined(schema.defaultValue)) &#123; value = schema.defaultValue; &#125; else if (version === '2.0' &amp;&amp; !_.isUndefined(schema.default)) &#123; value = schema.default; &#125; else if (_.isArray(schema.enum)) &#123; value = schema.enum[0]; &#125; else &#123; value = Random.float(0, 100, 0, 4); &#125; // Convert value if necessary if (!_.isNumber(value)) &#123; value = parseFloat(value); &#125; // TODO: Handle constraints and formats break; case 'string': if (version === '1.2' &amp;&amp; !_.isUndefined(schema.defaultValue)) &#123; value = schema.defaultValue; &#125; else if (version === '2.0' &amp;&amp; !_.isUndefined(schema.default)) &#123; value = schema.default; &#125; else if (_.isArray(schema.enum)) &#123; value = schema.enum[0]; &#125; else &#123; if (schema.format === 'date') &#123; value = new Date().toISOString().split('T')[0]; &#125; else if (schema.format === 'date-time') &#123; value = new Date().toISOString(); &#125; else &#123; value = Random.string(0, 61); &#125; &#125; break; &#125; return value;&#125;; 放心保存吧~~ 接下来，我们在当前项目下直接运行swagger project start，哇哈，后台服务应该已经跑起来了。swagger-node会监控我们的接口定义文件（在你的项目\\api\\swagger下），每次变更，都会被实时映射到mock服务上。 上面的代码并不是非常难理解，我只是将mockjs官方提供的数据类型都简单的绑定到了swagger生成模拟数据的规则上。 使用起来也非常简单，特殊类型只需要增加对应的x-mock声明即可，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081swagger: \"2.0\"info: version: \"0.0.3\" title: Hello World App# during dev, should point to your local machinehost: localhost:10010# basePath prefixes all resource pathsbasePath: /#schemes: # tip: remove http to make production-grade - http - https# format of bodies a client can send (Content-Type)consumes: - application/json# format of the responses to the client (Accepts)produces: - application/jsonpaths: /hello: # binds a127 app logic to a route x-swagger-router-controller: hello_world get: description: Returns 'Hello' to the caller # used as the method name of the controller operationId: hello parameters: - name: name in: query description: The name of the person to whom to say hello required: false type: string responses: \"200\": description: Success schema: type: object required: - total - users properties: total: description: 用户总数 type: integer users: description: 用户列表数据 type: array x-mock: array|5 items: $ref: \"#/definitions/HelloWorldResponse\" # responses may fall through to errors default: description: Error schema: $ref: \"#/definitions/ErrorResponse\" /swagger: x-swagger-pipe: swagger_raw# complex objects have schema definitionsdefinitions: HelloWorldResponse: required: - id - name - message properties: id: type: integer x-mock: step|1 name: type: string x-mock: title|cn message: type: string ErrorResponse: required: - message properties: message: type: string 完工！ 警告这种结合mockjs和swagger-node的方法并不是很优雅，毕竟需要直接修改项目的依赖库。这会导致 每次创建新项目可能都会丢失之前的修改。不过我并没有找到swagger-tools这个项目的扩展方式，至少官方没有提供类似的hook供我们来使用。（逼急了哥，直接弄个docker镜像给你看 :-)） 补充： swagger-node目前只支持单个swagger.yaml，并不支持多个独立的接口文件。只能要求我们在使用的时候将多个文件合并到一起，或者搭建多个项目，前面放一个nginx做反向代理。","tags":[{"name":"mock","slug":"mock","permalink":"https://blog.kazaff.me/tags/mock/"},{"name":"接口","slug":"接口","permalink":"https://blog.kazaff.me/tags/接口/"},{"name":"swagger","slug":"swagger","permalink":"https://blog.kazaff.me/tags/swagger/"},{"name":"模拟","slug":"模拟","permalink":"https://blog.kazaff.me/tags/模拟/"}]},{"title":"译-在多个标签页之间共享sessionStorage","date":"2016-09-09T09:37:12.000Z","path":"2016/09/09/译-在多个标签页之间共享sessionStorage/","text":"原文：Sharing sessionStorage between tabs for secure multi-tab authentication 译者得er瑟 昨天，就在昨天，前端一同事提了一个问题：我们的系统，用户重新开一个标签页，就要重新登录。我当时觉得这怎么可能？结果现场一测，还真是，好尴尬！ 今天抽了点时间网上查了查，才发现原来一直以为很简单的sessionStorage，还真埋了这么一颗雷。不过国外前辈也提出了一个解决方案，不仅如此，文章还把浏览器端保存数据的场景分析的很透彻，所以斗胆翻译了一下。 原文翻译 tl;dr;我实现了一种机制可以利用浏览器提供的sessionStorage或memoryStorageStorage的固有的安全性来实现用户身份认证，并且可以保证用户不需要每次新开一个标签页都重新登录。 现有的浏览器存储机制 localStorage：~5MB，数据永久保存直到用户手动删除 sessionStorage：~5MB，数据只在当前标签页有效 cookie：~4KB，可以设置成永久有效 session cookie：~4KB，当用户关闭浏览器时删除（并非总能立即删除） 安全的认证token保存一些重要的系统会要求当用户关闭标签页时会话立刻到期。 为了达到这个目的，不仅绝对不应该使用cookies来保存任何敏感信息（例如认证token）。甚至session-cookies也无法满足要求，它在标签页关闭（甚至浏览器完全关闭）后还会持续存活一定时间。 （任何时刻我们都不应该只使用cookies，它还有其他很多问题需要讨论，例如CSRF） 这些问题就使得我们在保存认证token时应使用内存或sessionStorage。sessionStorage的好处是它允许跨多个页面保存数据，并且也支持浏览器刷新操作。这样用户就可以在多个页面之间跳转或刷新页面而保持登录状态。 Good。我们将token保存在sessionStorage，并在每次请求服务器时将token放在请求头中来完成用户的身份认证。当用户关闭标签页，token会立即过期。 但多标签页怎么办？即便是在单页面应用中也有一个很常见的情况，用户经常希望打开多个标签页。而此场景下将token保存在sessionStorage中将会带来很差的用户体验，每次开启一个标签页都会要求用户重新登录。没错，sessionStorage不支持跨标签页共享数据。 利用localStorage事件来跨标签页共享sessionStorage我利用localStorage事件提出了一种解决方案。 当用户新开一个标签页时，我们先来询问其它已经打开的标签页是不是有需要给我们共享的sessionStorage数据。如果有，现有的标签页会通过localStorage事件来传递数据到新打开的标签页中，我们只需要复制一份到本地sessionStorage即可。 传递过来的sessionStorage绝对不会保存在localStorage，从localStorage事件将数据中复制并保存到sessionStorage，这个流程是在同一个调用中完成，没有中间状态。而且数据是对应事件携带的，并不在localStorage中。（译者注：作者意图解释这个方案的安全性） 在线例子 点击“Set the sessionStorage”，然后打开多个标签页，你会发现sessionStorage共享了。 123456789101112131415161718192021222324252627// 为了简单明了删除了对IE的支持(function() &#123; if (!sessionStorage.length) &#123; // 这个调用能触发目标事件，从而达到共享数据的目的 localStorage.setItem('getSessionStorage', Date.now()); &#125;; // 该事件是核心 window.addEventListener('storage', function(event) &#123; if (event.key == 'getSessionStorage') &#123; // 已存在的标签页会收到这个事件 localStorage.setItem('sessionStorage', JSON.stringify(sessionStorage)); localStorage.removeItem('sessionStorage'); &#125; else if (event.key == 'sessionStorage' &amp;&amp; !sessionStorage.length) &#123; // 新开启的标签页会收到这个事件 var data = JSON.parse(event.newValue), value; for (key in data) &#123; sessionStorage.setItem(key, data[key]); &#125; &#125; &#125;);&#125;)(); （译者注：上面的代码是我从在线demo中截取的，原文中并无提到） 接近完美我们现在拥有了一个几乎非常安全的方案来保存会话token在浏览器里，并支持良好的多标签页用户体验。现在当用户关闭标签页后能确保会话立即过期。难道不是么？ chrome和firefox都支持当用户进行“重新打开关闭的标签页”或“撤销关闭标签页”时恢复sessionStorage。F**k！（译者注：作者原文用的是“Damn it!”，注意到那个叹号了吗？） safari在这个问题上处理是正确的，它并不会恢复sessionStorag（只测试了上述这三个浏览器）。 对用户而言，能够确定sessionStorag已经过期的方法是直接重新打开网站，而不是选择“重新打开关闭的标签页”。 除非chrome和firefox能够解决这个bug。（但我预感开发组会称其为“特性”） 即便存在这样的bug，使用sessionStorag依然要比session-cookies方案或其他方案要安全。如果我们希望得到一个更加完美的方案，我们就需要自己来实现一个内存的方案来代替sessionStorag。(onbeforeunload也能做到，但不是太可靠且每次刷新页面也会被清空。window.name也不错，但它太老了且也不支持跨域保护) 跨标签页共享memoryStorage这应该是唯一一个真正安全的实现浏览器端保存认证token的方法了，并且要保证用户打开多个标签页不需要重新登录。 关闭标签页，会话立即过期–这次是真真儿的。 这个方案的缺点是，当只有一个标签页时，浏览器刷新会导致用户重新登录。安全总是要付出点代价的，很明显这个缺点可能是致命的。 在线例子 设置一个memoryStorage，然后打开多个标签页，你会发现数据共享了。关闭所有标签页token会立即永久过期（memoryStorage其实就是一个javascript对象而已）。 12345678910111213141516171819202122232425262728293031(function() &#123; window.memoryStorage = &#123;&#125;; function isEmpty(o) &#123; for (var i in o) &#123; return false; &#125; return true; &#125;; if (isEmpty(memoryStorage)) &#123; localStorage.setItem('getSessionStorage', Date.now()); &#125;; window.addEventListener('storage', function(event) &#123; if (event.key == 'getSessionStorage') &#123; localStorage.setItem('sessionStorage', JSON.stringify(memoryStorage)); localStorage.removeItem('sessionStorage'); &#125; else if (event.key == 'sessionStorage' &amp;&amp; isEmpty(memoryStorage)) &#123; var data = JSON.parse(event.newValue), value; for (key in data) &#123; memoryStorage[key] = data[key]; &#125; &#125; &#125;);&#125;)();","tags":[{"name":"localStorage","slug":"localStorage","permalink":"https://blog.kazaff.me/tags/localStorage/"},{"name":"sessionStorage","slug":"sessionStorage","permalink":"https://blog.kazaff.me/tags/sessionStorage/"},{"name":"共享","slug":"共享","permalink":"https://blog.kazaff.me/tags/共享/"},{"name":"memoryStorage","slug":"memoryStorage","permalink":"https://blog.kazaff.me/tags/memoryStorage/"},{"name":"认证token","slug":"认证token","permalink":"https://blog.kazaff.me/tags/认证token/"}]},{"title":"casperjs+mocha+chai搭建E2E测试环境","date":"2016-08-24T09:37:12.000Z","path":"2016/08/24/casperjs+mocha+chai搭建E2E测试环境/","text":"紧接着上一篇《小团队玩不转的测试》文章，我们继续来看看如何搭建一个端到端测试环境。 废话不多说，直接开始动手，省的你说没干货:-) mocha-casperjs前一篇文章分别介绍过每个用到的插件，而此刻提到的这个组合，也是有前辈整合好了能开箱即用的：mocha-casperjs。 这里需要注意的是，根据官方提供的安装方法，推荐所有使用到的组件都使用-g全局安装，这样方便日后在任何项目中跑测试，而且如果有的采用全局安装有的采用非全局的话是跑不起来的，这一点官方也有说明： Note that mocha-casperjs has peer dependencies on casper and mocha, and will be installed ajacent to where you are installing mocha-casperjs (e.g. if you install mocha-casperjs globally, you’ll have mocha and casperjs also installed globally). 如果你和我一样使用的是window开发环境，那你可能还需要自己手动安装phantomjs，并将其添加到系统环境变量中，否则会被casperjs提示缺少依赖。 还有就是，win自带的终端对中文编码支持不够好，所以我推荐使用Git Bash，前提是你机器上安装了git环境，其实就是一个MINGW环境。这样就能方便的使用大部分shell命令了。 对了，如果你win环境下没有python也不行，所以安装一个吧，并且将其加到系统环境变量中，一切就绪就可以运行官方提供的测试代码了： mocha-casperjs 在你的测试项目路径下执行上面的这个命令，会自动查找test或tests文件夹下所有的测试脚本并自动执行，最终的测试报告默认会直接打印在终端中~ casper-chai看了官方提供的测试例子，是不是蒙圈了？我对比了一下chai的文档，其实大多数chai提供的api都没有用到，毕竟都是针对单元测试的。而这个组件将casper的一些测试api用chai的方式提供给我们，感觉写起来更舒服一些。 所以推荐你先来看一下官方手册，看看都有哪些断言api可以用来写测试项。 除此之外，你还应该看一下casperjs的api文档，这样才能玩得转端到端测试的各种场景。 测试用例接下来的内容我们就来尝试对一些有代表性的页面编写端到端测试用例。下面的测试项目中，json文件用来模拟接口响应，这里只是为了演示方便，正常情况下端到端测试应该是在公司内部测试服务器上进行的，要求测试服务器上的项目版本必须和线上完全一致，包括初始化数据在内。 表单页面登录表单该页面的特点是： 表单验证 登录后跳转 会话数据 复杂表单该页面的特点是： ajax输入校验 头像上传 复杂的输入类型 列表页面该类型页面的特点是： 弹窗表单 翻页 自动完成、日历等特殊功能输入框 测试链接如下： http://localhost/E2ETestDemo/pages/list/list.html?page=1 注意，这里之所以要提一下访问链接格式，是因为现有前端逻辑需要依赖地址栏中的?page=1参数。这一点正是我想说的，我们在写测试项时，必须要完全理解代码的含义和场景所需要的输入输出数据结构。有时候可能是因为我们模拟的测试环境丢失了一些数据导致测试不通过，而在线上产品中并没有重现该bug，这是最头疼，也是最无奈的。 拖拽功能页面该类型页面的特点是： 可拖拽区块 拖拽区限制 上面的各个被测场景我已经放在github了，大家可以部署在自己本机的web环境里看一下页面的模样。对应页面的测试用例放在tests中，端到端测试用例脚本推荐以e2e.js结尾，这样未来当你还有单元测试等其他类型的测试脚本时容易区分。同时建议每个测试脚本对应一个页面，这样日后对应页面产生修改后也可以快速定位需要同步修改的测试脚本。 截图 坑页面的逻辑js没有被执行这个坑是真的坑爹，坑的我差点崩溃！从表象上来看，你会发现你根据官方提供的接口无论如何都无法触发目标页面的js执行，拿我们上面那个登录页面来说，你会发现不论是使用sendKeys，还是evaluate里使用__utils__都无济于事！你可以通过capture生成截屏来验证！ 哥哥我当时就蒙圈了，说好的幸福呢？这尼玛让我如何完成对领导的承诺？！就在我万念俱灰准备离职之际，突然看到了一个歪果仁的解答： I find that this happens when casperJS is too fast for the browser and a casper.wait() will solve the issue. 突然就醍醐灌顶了，一下子就好像打通了任督二脉！回味一下确实如此，内置的浏览器加载html所依赖的资源需要时间，而我们的测试脚本在html加载完毕后就会立即执行，所以就给我们感觉仿佛世界观崩塌了！ 解决方案很简单，只需要让测试脚本执行前wait几秒钟即可。 页面默认是手机尺寸如果你的被测网站是响应式的，你会发现capture生成截屏默认是适配的手机样式。我起初以为和UserAgent有关： 1casper.userAgent(&apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36&apos;); 事实证明没鸟用，后来我又试图修改截图的尺寸，结果也是毫无意义。后来才发现官方提供了viewport接口，而且也明确表明，默认phantomjs采用的是400 X 300的设置。 坑爹！ 提示”0 passing”不知道为何，当我们的测试脚本存在语法错误时，执行测试命令后就会看到0 passing提示，这对我们调试造成了非常大的困难，尤其是当测试脚本行数非常大时！ 灵异问题在实际使用时，由于ui所使用的一些js组件，或其他一些不明真相的原因，会导致你写不出正确的测试用例，我在demo中有注解，仔细看的话会发现不少小问题，虽然都最终解决了，但不知道为何如此，不甘心啊~ 心得总结经历了上述的几种常见场景，我觉得基本上只要有足够精力和时间，大部分项目都可以完成端到端测试。不过再次提醒，维护测试脚本也是要花时间和精力的，建议针对项目中相对稳定的界面和功能进行端到端测试，而处于待定阶段的页面还是人肉先行吧。 测试用例之间最好不要有顺序上的依赖，除非你能保证日后该流程会被整体执行，否则当你想单独执行某个子测试用例时会非常尴尬！还有就是，你的项目一定会使用很多第三方成熟的ui组件，而针对这些组件你不需要写太多测试用例，你的测试关注点应该更多的集中在自己实现的逻辑中。 推荐优先使用thenClick而非click接口，因为如果你连续调用（和两次调用代码位置无关），很可能会造成casperjs执行太快而导致早于你的业务逻辑js代码的执行，截屏上会让你感觉你的click调用未生效，但其实是太快了~~~ 最终，想要写出好的测试用例，排除测试代码自身质量因素外，还需要编写者对被测页面有足够的了解，不仅仅是页面上的显示元素，还包括该页面的加载细节（最头疼的网络延迟问题，不过可以使用casper.waitXXX相关api来优雅的解决这个问题），初始化细节等等。这样看来，其实最合适写测试用例的还是这个页面的实现者本尊，但同时这就要求你必须同时掌握测试相关的背景知识。现在来看，“测试先行”就显得非常合理了，根据设计书，开发人员完全可以先完成测试项（这里指的并非测试脚本，因为端到端测试不同于单元测试，它重度依赖UI，所以提前写端到端测试脚本不太现实，我这里指的测试项指的是文字描述的测试条目，有助于开发人员更好的分析页面逻辑），只不过这种做法的效率目前没办法评估，还要扭转开发人员现有的思维模式，是一个不小的挑战！ 这里我们也回避了一个较为重要的问题：到底一个页面需要多少个测试项才合理？前一篇文章和其参考文献中也有对该问题的相关描述，在考虑到诸多因素后如何权衡才能取得到一个平衡点，这是一门科学，等我日后有感悟了再回来表。 这篇比上一篇干货要多不少吧，希望大家满意:-) 参考手册http://docs.casperjs.org/en/latest/modules/index.html https://github.com/brianmhunt/casper-chai/blob/master/docs/casper-chai.md http://chaijs.com/api/bdd/","tags":[{"name":"端到端测试","slug":"端到端测试","permalink":"https://blog.kazaff.me/tags/端到端测试/"},{"name":"casperjs","slug":"casperjs","permalink":"https://blog.kazaff.me/tags/casperjs/"},{"name":"mocha","slug":"mocha","permalink":"https://blog.kazaff.me/tags/mocha/"},{"name":"chai","slug":"chai","permalink":"https://blog.kazaff.me/tags/chai/"}]},{"title":"小团队玩不转的测试","date":"2016-08-18T09:37:12.000Z","path":"2016/08/18/小团队玩不转的测试/","text":"早在上一家公司，就为测试问题头疼过，那时候测试全靠人肉，还整出了黑盒白盒测试文档，还要对代码进行打点，还要人工去匹配打点数据是否执行…都是泪，都是泪，都是泪啊！ 那个时候团队人数最多时有十二个，项目现在想想也不算大，按道理是可以分出来一部分人来专做测试工作的，只是当时无法说服领导成立测试小组，毕竟自己也没有自动化测试的经验。最后强迫别的部门的同事来帮我们测，除了心不甘请不愿外，测试的结果也不是特别的理想。 换了一家公司，依然是四人小团队，现在我开始琢磨如何自动化测试了。毕竟之前的不愉快精力，再加上我现在的岗位更多是解决团队的开发效率问题，所以必须得正视这个头疼的问题。 痛点小团队自动化测试的困难到底有哪些？ 人手不足 时间不足（和团队大小无关） 经验不足 前两点都归结于资源不足，正常的项目排期满满当当的都是需求文档和功能开发，不过仔细分析了一下团队的工作时间分布，发现其实还是有三分之一左右的时间是留给测试环节的。 按照我们的开发节奏，平均两周一个版本，三分之一相当于3.5天，相当可观的开销。如果这三天半的时间来花在写测试项上，应该也可以拿到一个比较好的结果，至少面对日后的回归测试会有较大的帮助，这里有一个公式： 自动化的收益 = 迭代次数 X 全手动执行成本 - 首次自动化成本 - 维护次数 * 维护成本 至于经验不足，也是一个比较头疼的问题。要知道，写好一个测试本身就不是件容易的事儿，而编写高可测试代码更要求开发人员的能力，这可不是短时间就能获得的技能啊。 测试关注点 哪些代码适合写测试 简而言之，就是那些重要的，复杂的，不易变的逻辑部分适合最适合写测试。 怎么写出好的测试项 覆盖率 如果没有定好测试覆盖率，就无法评估测试的程度，你也就不能安心的认为跑完测试的代码就一定没有问题，好尴尬啊~ 定位问题 我们的测试项报错后，应该尽可能的辅助开发人员定位问题的位置，而不是笼统的报错。 低耦合 尽量的保证业务代码的耦合较低，有助于编写和维护测试项。 高性能 尽可能保证测试的性能，谁都不愿意等待。 怎么维护测试项 这就是个权衡问题了，前期测试项写的越多，一旦代码变更了，需要同步修改的测试项也会很多。上一张经典的测试金字塔图： 图中： UI：端到端测试 Service：集成测试 Unit：单元测试 该图表达了不同测试类型对应的测试项比重，这是前辈们的经验得出的科学分配法，值得我们遵守。 测试套件KarmaKarma作为测试套件的执行器（test runner），为我们的测试框架提供测试环境的，它的安装和配置简单的我都想哭了。不过好像它并无法直接进行端到端测试，github上有一个扩展：karma-e2e-dsl是专门做这件事儿的，有兴趣的可以看看。 AtomusAtomus给自己定位很准确，就是轻量级UI测试套件，重点提到了它可以支持局部界面测试，其实就是利用了jsdom提供的能力来灵活的做到了js+html联调，思路不错，不过github上的关注度和活跃度不算高。 Casperjs类似Atomus，但是Casperjs更加的强大，它是基于PhantomJS来实现浏览器模拟的，并提供了强大的测试API，该项目的关注度也很高，值得好好看看。 google上可以找到比较多的使用casperjs配合mocha和chai来做端到端测试的资料，这也是目前我比较中意的组合。 chrome上还有一个casperjs的插件：Resurrectio，用来帮助我们通过直接在页面上操作来录制测试脚本，虽然作者好像已经不再维护这个项目，但经测试依然可以满足一些简单的场景。 Mocha + Chai + Sinon这套黄金搭档在面对单元测试时基本上所向无敌，不过如果是做端到端测试，我们几乎不需要做mock或stub，所以Sinon就可以先放一边。Mocha配合Chai可以提供标准的测试所需功能，是目前最新的测试框架之一。所以之后我在搭建具体测试环境的时候也会优先选择这俩工具。 PhantomFlow如果你想要一份屌炸天的测试报告，那PhantomFlow应该是一个开箱即用的工具了，它基于我们上面提到的一些工具，并提供了各种漂亮的展示模版，很适合装逼用。 Page-Monitor上面说的都是功能测试为主，最后我们来说一下界面测试，国人大牛提供了Page-Monitor工具，可以帮我们对比界面的差异，使用方法非常简单，具体可以根据官方步骤来做，目前这不是我的关注点。 总结这篇文章走马观花的看了一下目前能发现的一些测试套件，科普的成分多一些，没有啥干货。之后我会根据自己选择的相关工具集合把搭建环境和编写测试脚本的步骤和流程写出来，请给予关注哟~ 虽然这篇自身没有太多干货，但下面的参考文献提供的文章却非常有料啊，希望大家能看过瘾。 参考文献 JavaScript客户端测试之旅 前端自动化测试探索 Client-side testing with Mocha and Karma How to set up Mocha + Chai + Sinon + Karma + Browserify +Istanbul + Codecov unit-test-e2e-environment End to end testing with zombie.js, mocha.js and should.js Simpler UI Testing With CasperJS FRONTEND TESTING WITH PHANTOMJS CASPERJS MOCHA AND CHAI 回归测试:人工测试还是自动化? 使用node.js进行API自动化回归测试","tags":[{"name":"端到端测试","slug":"端到端测试","permalink":"https://blog.kazaff.me/tags/端到端测试/"},{"name":"单元测试","slug":"单元测试","permalink":"https://blog.kazaff.me/tags/单元测试/"},{"name":"回归测试","slug":"回归测试","permalink":"https://blog.kazaff.me/tags/回归测试/"}]},{"title":"Git Hook帮你维护前端代码规范","date":"2016-08-17T09:37:12.000Z","path":"2016/08/17/git hook帮你维护前端代码规范/","text":"只要不是一个人在战斗，你都一定会碰到很多工程问题。我们今天来说的，就是代码格式问题。这不是个什么有意思的话题，这个话题讲的就是条条框框，就是枯燥，就是没意思。但是，如果你的团队缺少代码格式规范的话，当你review组员的代码时，你就会感觉在吃屎，没错，不夸张！ 我不怀疑团队组员的积极性，因为条条框框本来就不是程序员的调调，而且人类和机器的最大差别就是遵守规范的程度。所以，你不能要求你口头上说代码格式要怎样怎么，所有同事就会立刻写出符合要求的完美代码。我也从来没活在真空中，项目的期限和需求变动累加在一起，但凡是个活人，都尼玛会被折腾的精疲力尽，谁有能包票自己无时无刻都会遵守代码规范？反正我做不到！既然我自己都做不到，就更不应该要求别人。 不过，如果可以让机器来帮我们检查代码格式问题，那无疑是一个不错的注意。这里的哲学思想是：习惯成自然！你可以一开始记不住规范，但随着你在项目组内待的时间，慢慢你就会养成习惯，最终所有人的代码都会符合既定规范。 ok，扯了这么多废话，我们开始搭建代码检测环境吧。 git hook用git的人，肯定知道这个东西，但可能没怎么用过，例如我。这可不是个小玩意儿，它厉害着呢！大家可以从这里大概了解一下相关的知识。而我们这次主要目标是：pre-commit。 pre-commit钩子是在git本地提交前会执行的一个回调，通过我们自己定义的脚本可以达到前面我们提到的代码格式检查的目的，但凡不符合规范的提交，一律驳回。 nodejs如果你shell和我一样弱鸡，那你也可以和我一样考虑使用nodejs作为脚本引擎。庆幸的是不少前辈已经为我们铺平道路，老省心了： husky node-hooks 今天我要介绍的，是我们国内百度提供的一套解决方案：fecs，之前从没说过，今天也是在搜索相关主题的时候发现的好东西。我们主要使用它的插件：fecs-git-hooks，有了它，我们只需要一行命令就可以创建好所需的pre-commit：12//项目根目录下执行npm i fecs-git-hooks 执行完后，你会在项目根目录下的.git/hooks文件夹下看到配置好的pre-commit，一切就已经完成了。你可以创建一个测试项目来测试一下，现在不满足fecs定义好的规范的提交都会被驳回。 那么，到底有哪些规范会被检查呢？ html js css 规则自定义每个公司甚至每个团队都可能有自己的代码风格规范，这个不能强求，毕竟没有最好的规范，只有最合适的。 那如果fecs默认的规范和我们的不匹配呢？也很简单，fecs提供了可配置文件来让我们设置是否开启某一项检查。 那我的场景来说，我修复了所有的ERROR级别的错误，但是WARN级别的我个人觉得就无所谓了，但机器不这么认为，所以只要还存在一条不满足规范的代码就无法提交！ 根据官方文档所述，我们可以在项目根目录添加一个.fecsrc配置文件。但官方并没有解释清楚配置项要怎么写，故意的吗？ 其实并不难，只需要根据git提交时报错的日志，每一行的最后一个()内就是这个配置项的名字，只需要分别出它到底是html，css还是js的配置项即可。 然后在.fecsrc中这么写：12345678910111213141516171819202122232425262728&#123; \"htmlcs\": &#123; \"max-len\": false, \"asset-type\": false, \"style-disabled\": false, \"lowercase-id-with-hyphen\": false, \"lowercase-class-with-hyphen\": false, \"indent-char\": false, \"self-close\": false, \"img-width-height\": false, \"attr-lowercase\": false, \"img-src\": false, \"label-for-input\": false, \"bool-attribute-value\": false, \"attr-no-duplication\": false &#125;, \"csshint\": &#123; \"disallow-important\": false &#125;, \"eslint\": &#123; \"env\": &#123; \"es6\": true &#125;, \"rules\": &#123; \"no-console\": 0 &#125; &#125;&#125; 你也可以查看都有哪些可配置的项目： csshint htmlcs eslint 不足这种方案相当于是本地化git hooks，要求项目组员必须在各自的开发环境下统一配置一致的fecs环境，日后如果有调整，也需要所有人同步更新。 git hook其实也有服务器端的，日后我们再学习一下~ 注意一旦在组内推广，初期肯定会碰到大量的代码格式报错，所以应该选择一个合适的时间段进行推广，不然会带来额外的繁重工作影响团队气氛，甚至是项目进度。","tags":[{"name":"FECS","slug":"FECS","permalink":"https://blog.kazaff.me/tags/FECS/"},{"name":"代码格式","slug":"代码格式","permalink":"https://blog.kazaff.me/tags/代码格式/"},{"name":"git","slug":"git","permalink":"https://blog.kazaff.me/tags/git/"},{"name":"hook","slug":"hook","permalink":"https://blog.kazaff.me/tags/hook/"},{"name":"nodejs","slug":"nodejs","permalink":"https://blog.kazaff.me/tags/nodejs/"},{"name":"pre-commit","slug":"pre-commit","permalink":"https://blog.kazaff.me/tags/pre-commit/"},{"name":"review","slug":"review","permalink":"https://blog.kazaff.me/tags/review/"}]},{"title":"Open-Falcon初探","date":"2016-08-10T09:37:12.000Z","path":"2016/08/10/Open-Falcon初探/","text":"随着项目的开发一点一点的完成，离初版上线日期已经越来越近了，这样就涉及到各种运维问题，监控的意义就体现出来了。虽然项目最终会部署在云平台，而云平台自身会带监控套件，不过不够灵活，一些想要的指标和报警方式还是需要自己来实现。大概看了几款监控解决方案，对Open-Falcon特别有好感，虽然我不懂GO语言~ 环境搭建废话不多说，先来快速搭建一套Open-Falcon环境吧，官方提供了docker镜像来让我们这种新手快速尝鲜，不过是放在docker.hub的仓库中，速度不太理想。幸好有前辈将镜像clone到灵雀的仓库了，你懂的。 成功把镜像下载到本地后，我们还需要根据官方提示，用自带的run.sh来实例化镜像。不过在那之前，我们要对该脚本做一下修改： 123456#!/bin/shHOST_DIR=/home/kazaff/open-falcon/dataDOCKER_DIR=/home/work/open-falcondocker run -td --name open-falcon -v $HOST_DIR/conf:$DOCKER_DIR/conf -v $HOST_DIR/data:$DOCKER_DIR/data -v $HOST_DIR/logs:$DOCKER_DIR/logs -v $HOST_DIR/mysql:$DOCKER_DIR/mysql -p 8433:8433 -p 6030:6030 -p 5050:5050 -p 8088:8080 -p 8081:8081 -p 6060:6060 -p 5090:5090 -p 6081:6081 index.alauda.cn/lixiaowei/open-falcon:latest 一共修改了3个地方： 将HOST_DIR地址修改到了我指定的位置，为了方便后面的测试，不用来回切换路径 增加了6081端口的绑定，用于查看judge的状态，参考这里 修改了镜像的名字，毕竟我们使用的灵雀云 剩下的步骤就按照官方提示来做就ok了。 Agent安装docker镜像是不自带agent的，agent是安装在被监控机器的，我们可以从官方提供的二进制安装包中获取到编译好的agent。 由于我们是将docker的宿主机当作是被监控机器，所以基本上我们不需要修改agent默认的配置文件，直接根据文档提示的步骤运行即可。 目前为止，agent已经开始每分钟上报一次监控数据了。 架构图 推荐大家还是先好好读一下官方介绍，对后面可能遇到的使用问题帮助。 端口监控Open-Falcon默认就会采集目标机器的200多项指标，基本上可以覆盖目标机器的OS运行状态。不过，对我们自己的应用来说，可能还不够，我们需要实时掌握应用是否还存活，所以我们需要对指定的端口号进行监控。 默认Open-Falcon是不会监控端口的，毕竟它也不能知道我们目标端口的配置。所以我们需要自己定义监控策略。 在后台中配置好用户组和用户关系后，还需要配置HostGroup，并为其分配目标host和策略模版才能完成我们的目标，操作流程官方已描述。 一切就绪后，请求judge的http接口来查看一下配置好的策略是否生效了： http://judge的ip:6081/strategy/endpoint名字/net.port.listen 其中，endpoint名字根据上面的agent配置，你需要在自己docker的宿主机上执行hostname命令来获取。 坑监控表达式不起作用我们可以在后台看到，不仅能根据HostGroups来配置策略达到监控的目的，Open-Falcon还提供了“expressions”模块，我们可以直接来进行监控配置，如下图： 我这边实测情况是，只成功了一次，然后就再也无法被judge获取到。不知道为啥？我重新安装环境都没解决~ 重启docker容器后hostname不识别这个问题指的是，在Dashboard后台的endpoint搜索，无法搜索出重启容器之前的hostname节点。 只有手动编辑agent的配置文件中hostname的值，而且还不能和之前的名字一致，使用设置的名字才可以检索到结果。 而且更奇葩的是，重启前的hostname数据肯定还在，因为监控报警中还能看到针对之前名字的警告在实时更新。 报警邮件无法收到默认是没有提供邮件server的，所以我们必须自己来实现一个提供http调用的邮件服务，官方提供了一个GO的实现，但并没有给二进制版本，所以我们需要自己编译。没有GO环境的，可以直接在这里下载哟~ mail-provider的配置文件cfg.json的配置：12345678910111213&#123; &quot;debug&quot;: true, &quot;http&quot;: &#123; &quot;listen&quot;: &quot;0.0.0.0:4000&quot;, &quot;token&quot;: &quot;&quot; &#125;, &quot;smtp&quot;: &#123; &quot;addr&quot;: &quot;xxxx:25&quot;, &quot;username&quot;: &quot;xxxxx&quot;, &quot;password&quot;: &quot;xxxxx&quot;, &quot;from&quot;: &quot;xxxxx@xxx.xx&quot; &#125;&#125; 需要注意的是，我这里端口号用的是25，而不是465，因为实测中我使用的AWS的SMS服务，是提供465端口发送ssl邮件的，不过这个插件貌似不行，最终使用25端口才能跑正常。此外，AWS的SMS服务还需要对from设置进行邮箱校验，注意哟~ 最后记得要修改一下sender的配置文件sender.cfg：1234&quot;api&quot;: &#123; ...... &quot;mail&quot;: &quot;http://172.17.0.1:4000/sender/mail&quot; &#125; 其中api.mail的值是用的我本地的ip。修改后执行：1docker exec open-falcon supervisorctl restart sender 监控趋势图js报错查看监控趋势图是需要选择Endpoint的，不过当你一开始跑Agent后立刻来查看趋势图，是会发现图表绘制报错的，根据我多年的js经验应该是由于没有数据导致的。 所以，稍等几分钟，待agent上报几次数据后就可以正常看到监控趋势图了。 灵异在不停地重启不停的配置不停重装环境的折腾下，还碰到了各种灵异问题，例如OpsPlatform的链接突然变成了默认设置，Agent进程莫名其妙的消失了等等。 总之感觉如果不是特别熟悉GO的话，想玩转Open-Falcon难度还是有的，风险还是大的。","tags":[{"name":"监控","slug":"监控","permalink":"https://blog.kazaff.me/tags/监控/"},{"name":"docker","slug":"docker","permalink":"https://blog.kazaff.me/tags/docker/"}]},{"title":"关于功能依赖数据的思考","date":"2016-08-05T09:37:12.000Z","path":"2016/08/05/关于功能依赖数据的思考/","text":"严格意义上来说，这并不适合称之为架构问题，或者说应该加个范围：业务架构。我不知道这是不是一个通用性问题，至少我在多个项目中不止一次的遇见过这个问题。先来让我简单描述一下问题的具体表现吧~目前我司正在做的是一个企业内部的ERP系统，在需求分析阶段我们根据企业内部实际的工作流程和角色，在软件系统中引入了预知对应的元素： 部门 &gt; 用户 这非常的常规，部门和用户自然也是允许管理员根据企业需要动态的增删改的。目前为止都是美好的！接下来，我们需要根据需求继续将实际工作流程搬到软件中来。假设，我们有N个部门，不同的部门自然对应不同的工种，例如经理，财务，行政，销售等。每个工种都有自己的工作流程和要解决的实际问题。 问题来了，我们要为每个部门开发对应的软件功能，意味着在开发阶段，我们的功能就要确定针对的是哪个部门，这里就产生了依赖。由于“部门”是允许在软件开发完成上线后使用者动态操作的，如何能确保使用者的行为不会打破这种依赖呢？我想这样类似的问题你一定也碰到过，对吧？！ 功能依赖数据这种依赖只要不解决掉上面提到的那种干扰，就一定是不健全的。但，简单的将“部门”编辑权限关闭，以牺牲用户使用灵活度为代价的方案肯定也是耍流氓，那这中间的权衡应该是什么呢？ 让我们撒开欢来思考，如果使用老套路，增加额外的一层来解决被依赖数据的稳定性，是不是一个好的方案呢？ 比方说，引入“角色”概念进来，将功能与角色进行关联而不是关联部门，而这里提到的角色是使用者不可变的（注：可能这里叫“角色”容易让大家和权限对应的角色产生关联，但我实在想不出别的好名字，见谅）。 在开发阶段，我们根据需求实现某个业务流程，围绕该流程我们预置所需的角色。换句话说只要该业务流程还在，对应的角色就不会消失。 听起来是个不错的方案，但缺点是，使用者是否能很好的理解“部门”和“角色”的区别和用法？你能确保使用者在创建系统用户的时候很容易的就选择合适的部门和角色么？毕竟现实世界中，部门就足够用了，多出来的角色会让使用者十分的困惑，对他们来说这是个多余的玩意儿！ 没错，角色的出发点本身就不是来自于真实需求，更多存在的理由是来自于我们的技术方案，或者说的更直接点，是实际需求和技术方案不匹配导致的。 这个依赖，也是真实的会影响代码的，回想一下以往我们的做法是什么？对于这种功能依赖数据的场景，更多的是使用配置文件，对吧？那现在一般都是前后端分离，意味着相同的配置项，前后端都要各自保存一份（或者提供对应的服务接口）。 总之，前后端最终都会对该配置文件产生强依赖。OMG，越说越可怕，我都不敢往下想了。 绕回刚才的方案，如果我们不引入角色，而是对部门提供额外的约束，例如分成预设部门和普通部门两种类型，预设部门不能编辑。这么做至少我们让使用者不会迷茫了，但依赖的问题还在。 依赖“倒置”如果我们将这种依赖关系放手交给使用者来维护的话是否合适呢？思路类似很多框架常用的一种模式，我们提供对应的接口和界面来供使用者完成功能和部门之间的依赖维护，这么做无疑会增加使用者的操作负担，不过也不失为一种灵活可行的方案。但别着急高兴，继续分析。 我们的例子里，若在运行期间修改了功能和部门的关系，那该功能中之前产生的历史数据就有可能不能正常使用，例如由A部门切换到B部门后，原先历史数据中都是A部门下的用户数据，现在都作废了。抛开是否删除这些数据不谈，我们主要关注于这些数据的价值，多数场景下它们都是有价值的。至少，它们曾经代表了系统的一种状态，但它们现在却无法提供太多有意义的信息，甚至会对使用者造成一定程度的视觉干扰。 我们需要根据需求来对待它们，但问题的本质还是依赖改变导致的。如果我们只提供给用户仅一次的依赖绑定操作，之后不允许修改依赖，甚至连被依赖的部门都不能再删除，这又是一种需要权衡的妥协。 结论这种依赖之所以讨厌，是因为牵扯的面太多，不仅在开发阶段，也在使用阶段。如何最终提供给使用者一个尽可能还原现实世界流程的，不引入陌生概念的方案，该方案还同时可以让开发者能尽可能高效灵活的组织代码，不暴露过多给接口调用方。 可惜，目前，我没有这么样的一个方案。你有么？","tags":[{"name":"依赖","slug":"依赖","permalink":"https://blog.kazaff.me/tags/依赖/"}]},{"title":"AH01071 Got Error 'Primary Script Unknown'","date":"2016-08-04T09:37:12.000Z","path":"2016/08/04/AH01071 Got error 'Primary script unknown'/","text":"这几天有点神经衰弱了，被apache搞的！之前的一篇日志完成了ec2上的apache+php-fpm的环境搭建，一切都是那么的自然。 可谁知道，环境相当的稳定，间歇性的会导致apache或php-fpm服务挂掉，真是哔了狗了！在apache的error日志文件中看到了大量的报错： AH01071 Got error ‘Primary script unknown’ GG了一大圈，感觉没有一个对口的解决方案，只能自己来猜！最后发现可能导致该报错的原因是：由于我们把php请求交给php-fpm进程来处理，那apache就不应该再管这些请求了，所以在尝试把默认的apache对应的php模块都关闭后，就再也看不到该报错了。 怎么关？我的环境下，apache的相关配置文件在/etc/httpd/，只需要就将下面两个文件备份删除，重启服务即可： /etc/httpd/conf.d/php.conf /etc/httpd/conf.modules.d/10-php.conf 目前已经感觉不会再报该错了，留院观察几天吧~ 补充根据本人的观察，发现还是会有上述的报错，证明我们的思路是错的！不过奇怪的是上面的设置确实会导致这个报错大量减少，是我的错觉么？为了避免大量的错误信息淹没我的磁盘，我默默的设置了清理日志文件的定时任务，这算是什么？掩耳盗铃么？娃哈哈~ 推荐一下360的网站监控服务，很贴心，免费套餐的配额也是很给力的。根据两天的观察，确实已经稳定了。但不知道是为啥会随机的服务挂掉。 继续留院观察吧~","tags":[{"name":"php-fpm","slug":"php-fpm","permalink":"https://blog.kazaff.me/tags/php-fpm/"},{"name":"apache","slug":"apache","permalink":"https://blog.kazaff.me/tags/apache/"}]},{"title":"RedHat7的apache+php-Fpm环境搭建小结","date":"2016-08-01T09:37:12.000Z","path":"2016/08/01/RedHat7的apache+php-fpm环境搭建小结/","text":"之前写过一篇《EC2下RedHat的nginx+php-fpm环境搭建小结》。 由于种种原因，这次要把nginx替换成apache，所以再总结一篇~~要在RedHat上安装我们的目标环境，先得找到合适的源： 12rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmrpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm 安装php环境，由于php依赖httpd，所以下面的命令会安装好apache2： 1yum install php56w php56w-fpm php56w-opcache php56w-gd php56w-pdo php56w-mysql php56w-common 最后，我们还需要让apache感知到php-fpm，新增/etc/httpd/conf.d/fpm.conf文件： 1ProxyPassMatch ^(.*\\.php)$ fcgi://127.0.0.1:9000/var/www/html/ 启动对应服务即可：12service httpd restartservice php-fpm restart PS:如果你也是在用AWS的话，建议初始化完EC2后创建个快照，以便日后快速恢复系统。 参考： https://webtatic.com/packages/php56/ http://developers.redhat.com/blog/2014/04/08/apache-with-various-php-versions-using-scl/ https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Confined_Services/chap-Managing_Confined_Services-The_Apache_HTTP_Server.html","tags":[{"name":"php-fpm","slug":"php-fpm","permalink":"https://blog.kazaff.me/tags/php-fpm/"},{"name":"apache","slug":"apache","permalink":"https://blog.kazaff.me/tags/apache/"}]},{"title":"传统Web项目代码变更引起的浏览器缓存问题和解决思路","date":"2016-07-26T16:37:12.000Z","path":"2016/07/26/传统web项目代码变更引起的浏览器缓存问题和解决思路/","text":"不确定文章的标题描述的是否已经足够明了，至少用类似的描述在gg中并没有定位到相关的文章。讨论这个话题的更多是围绕这前端工程化套件的用法的（例如webpack、grunt、gulp等），而这些工具对单入口页面SPA应用支持度非常的好。但现实往往不尽如意，面对传统的多入口web项目，前端又应该如何解决浏览器缓存旧版本代码的问题呢？让我们先来详细描述一下问题的环境域： 多入口项目，意味着包含非常多的html页面 大量的内嵌js和css 我们的关注点不在加载性能上，本次我们仅关注：如何实时影响浏览器端及时访问更新后的代码版本。这应该属于工程问题，而非开发问题。意味着我们不应该试图让开发人员来解决这个问题，而应该在工具上来寻找突破口。毕竟，开发人员来解决的话，他们会简单粗暴的靠代码来禁用浏览器缓存，这虽然确实“治本”，但杀伤力太大，朕很不喜欢。那么到底应该怎么来做才完美呢？ 首先我们来看看到底有哪些内容被缓存了： html，以及内嵌其中的js和css 外联的js和css 其它静态资源文件，如图片等 回忆之前写的spa项目，不存在内嵌js，所有项目的前端应用js脚本都会最终编译打包成一个独立的文件，并且会根据合并后的内容做签名，有效的避免了浏览器使用旧版本缓存的问题。而html这部分也都会使用js模版方案，最终也是会编译成js打包起来的。css也会合并成一个带内容签名的独立文件的。换句话说，SPA项目已经有了成熟的解决方案和工具来帮我们解决旧代码版本的浏览器缓存问题。 但，传统的多html文件入口的web项目呢？就没有人管它们的死活了吗？好像真的是啊！唉~至少我在google上找了一圈，都没有相关资料啊（baidu就不试了）。看来只能挽起袖子自己动手丰衣足食了啊！ 其实也并不需要从零做起，依然是可以根据现有工具来实现我们所需的功能的。这里我打算基于webpack来完成这个挑战。不过在具体动手之前，得先仔细分析一下要解决的问题范围： html的新旧版本问题 外联js和css的新旧版本问题 第二个问题其实webpack已经可以解决的很好了。关键是第一个问题很难搞，为什么呢？如果我们参照js文件的处理方式来修改html文件的名字来依靠签名做版本控制，这会导致我们还需要同步维护应用逻辑中所有的跳转链接，让其自动的可以绑定最新的文件名。 如果我们的前端项目中要求所有使用链接的地方都调用一个特定的函数来获取目标链接，这是可以让我们有机会在前端脚本执行时根据额外的一个路由表来动态绑定正确的链接。但这个方案的问题是，要求代码修改量较大，且会导致用户的历史收藏夹保存的链接失效。 再考虑考虑，如果我们不修改html的文件名本身，而是在后面增加请求参数呢？例如： test.html?v=1&amp; 经测试，完美。那我们可以保证html版本的更新不会导致用户历史收藏夹失效，但依然需要大量的人为修改代码的工作，有没有进阶的办法呢？其实可以将这部分的人工操作自动化处理。只需要匹配出html中所有的链接，并根据生成的路由表来自动替换即可。 注意上面例子中结束的”&amp;”，这样若项目需要其它的地址栏参数，则可以直接拼接。 目前能想到的需要匹配出的链接上下文场景有： html中a标签的href属性 内嵌js脚本的相关跳转地址 可能还会有form标签的action属性 外联js脚本的相关跳转地址 注意一个细节，由于我们是先根据文件内容创建出路由表，再去替换文件内容中的链接地址，这会再触发一次内容变更。目前我不确定这是否会成为一个问题，不过先标注出来吧。 思路大致有了，感觉也不是太复杂。剩下的就是看看如何使用webpack来完成了。 ###补充 由于我们当前的项目是基于这个框架来组织代码的，所以前面提到的那个会导致手工修改工作量的问题在当前项目中其实不存在。而且，我们的思路中提到的路由表甚至都不需要生成，因为本身这个框架中就提供了每个模块的配置文件，我们只需要修改配置文件中对应项即可完成目标，简直了！ 根据具体情况，我在原有编译流程中增加了这些逻辑，轻松完成需求。","tags":[{"name":"webpack","slug":"webpack","permalink":"https://blog.kazaff.me/tags/webpack/"},{"name":"SPA","slug":"SPA","permalink":"https://blog.kazaff.me/tags/SPA/"},{"name":"多入口","slug":"多入口","permalink":"https://blog.kazaff.me/tags/多入口/"}]},{"title":"Jquery的ajax下载blob文件","date":"2016-07-20T16:37:12.000Z","path":"2016/07/20/jquery的ajax下载blob文件/","text":"好久没有解决前端问题了，下午同事问了我一个问题：jquery的ajax怎么下载文件？乍一听有点蒙，之前用ng和react时也写过类似的功能，但是很顺利（所以忘记具体细节了）。jquery为啥会不行呢？看了一下具体场景，发现原来jq的ajax回调已经把response的数据傻瓜式的以字符串的方式解析了。 查了一下gg，发现国内的解决方案就是在该场景下不实用jq，而是自己手动创建XMLHttpRequest。虽然这个方法很可靠，但之前封装的jq的ajax就不能使用了。 查了查jq的文档，本打算自己根据jq提供的jQuery.ajaxSetup()接口来拓展数据类型，但怎么都搞不定。后来，在github上找到了一个大牛封装好的jq插件。 然后我们就可以这么写了：123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;blob demo&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;img id=\"img\" src=\"\" /&gt; &lt;script src=\"//cdn.bootcss.com/jquery/2.2.1/jquery.js\" charset=\"utf-8\"&gt;&lt;/script&gt; &lt;script src=\"jquery-ajax-blob-arraybuffer.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt; $.ajax(&#123; url: \"./face.jpg\", type: \"get\", dataType: \"blob\", //扩展出了blob类型 &#125;).done(function(data, status, jqXHR)&#123; var reader = new window.FileReader(); reader.readAsDataURL(data); reader.onloadend = function() &#123; document.getElementById(\"img\").src=reader.result; &#125; &#125;).fail(function(jqXHR, textStatus) &#123; console.warn(textStatus); &#125;); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 不过，从该插件的源码上来看，它也是手动构建了一个XMLHttpRequest对象来发送ajax，不过兼容性可能会成为问题。想深究的可以看这里。","tags":[{"name":"jquery","slug":"jquery","permalink":"https://blog.kazaff.me/tags/jquery/"},{"name":"ajax","slug":"ajax","permalink":"https://blog.kazaff.me/tags/ajax/"},{"name":"blob","slug":"blob","permalink":"https://blog.kazaff.me/tags/blob/"}]},{"title":"RedHat的cron配置BAD FILE MODE问题","date":"2016-07-20T09:37:12.000Z","path":"2016/07/20/RedHat的cron配置BAD FILE MODE问题/","text":"最近在AWS提供的免费EC2上跑了个Wordpress，用的是nginx+php-fpm。可能是由于免费ec2的配额是在无法容纳庞大的wp，也可能是别的原因，总之一个几乎没有流量的网站也会出现周期性的卡顿。由于没有思路进行代码方面的排查，所以准备写个计划任务来定期重启php-fpm的连接池来简单粗暴的规避上述问题。不过，竟然发现红帽子linux默认的crontab状态竟然提示： 1crond[510]: (root) BAD FILE MODE (/etc/crontab) 其实原本不需要写这篇文章，因为上述错误信息你放在gg或bd上搜会找到大量的解决方案（其实中文帖子都是源自一个源，一个字都没改动）。我照着试了试，还是不行，看了一下一个英文论坛上的描述，其实差别只是一个0，但是对于我这种半吊子来说，也是很致命的： 12chmod 0644 /etc/crontabservice crond restart 644和0644的差别： 先来看一个提问，下面跟的答案其实说的不是太明确，但应该意思没错。 我们来看一个国产贴。这篇文章写的很屌，而且举了一个很好的例子。虽然我看的还是有点晕，但相信有经验的运维人员已经搞明白了。 不负责任的说，那个0表示的就是关闭掉setUID权限设置（避免过多的开放权限给普通用户），这应该是crond对配置文件的要求吧~","tags":[{"name":"RedHat","slug":"RedHat","permalink":"https://blog.kazaff.me/tags/RedHat/"},{"name":"计划任务","slug":"计划任务","permalink":"https://blog.kazaff.me/tags/计划任务/"},{"name":"crontab","slug":"crontab","permalink":"https://blog.kazaff.me/tags/crontab/"}]},{"title":"Disconf+spring Boot+mybatis","date":"2016-07-15T09:37:12.000Z","path":"2016/07/15/disconf+spring boot+mybatis/","text":"最近在调研“配置中心”这一块儿，用于解决大量子系统的部署问题。目前系统一多，每次一个配置项目的变更老麻烦了，流程是： 修改项目的配置文件 –&gt; 重新打包 –&gt; FTP上传服务器 –&gt; 解压替换现有项目 –&gt; 重启项目 由于目前项目处于开发阶段，更多的场景是变更代码，所以上述流程并没有显得特别的笨重，但每次线上调试时就显得有些不合理了。再加上日后打算做基于配置的功能开关，所以说配置中心是一个不错的解决方案。 没有花太多的时间去调研各个开源配置中心解决方案，有兴趣的推荐看看这篇博客，这里我选择的是百度的disconf。 disconf环境搭建这里我们直接走个捷径，使用docker-disconf来完成环境搭建。 关于docker的用法，可以参考我前面的一些文章，我就不啰嗦了。 disconf测试环境在仔细阅读完官方wiki后，我们就可以clone其提供的demo代码到本地，进行测试了。 这部分基本上只要文档读的仔细，根据自己环境来调整几个参数，基本上就都可以顺利跑起来。 而demo中不仅提供了spring boot的用法，连dubbo都有，果然是国产良心。而我们可以直接使用它的spring boot例子来继续我们的任务。 例子其实比较简单，可能出于某种目的，并没有根据配置文件创建对应的redis连接，只是通过一个定时器不停的打印配置项，供我们来测试配置中心的实时同步和自动更新功能。 而我们接下来要使用disconf的配置变更来做到自动更新mybatis使用的数据源。 与mybatis结合老实说，我并不是java高手，对spring boot的研究也很浅薄。基于我的同事搭建好的一个项目基础代码来完成我们的目标。 要完成我们的目标，我们可能需要做到以下几点： disconf和spring boot完美结合（官方demo已经给了方法） spring boot和mybatis结合（我同事的代码已经做好） disconf和mybatis结合（我们要做的事儿） 代码我最终会上传到我的github上，下面我们来详细说说这个第三步。 要做到disconf和mybatis结合，其实说白了就是两件事儿： 让mybatis使用的datasource配置项走disconf 当disconf发布相关配置项的变更事件后，mytabais能感知到 第一步比较好搞： 12345678910111213141516171819202122232425262728293031323334@Bean(name = \"dataSource\") public DataSource dataSource(DBConfig dbConfig)&#123; return DataSourceBuilder.create() .url(dbConfig.getUrl()) .username(dbConfig.getUsername()) .password(dbConfig.getPassword()) .driverClassName(dbConfig.getDriverClassName()) .build(); &#125;@Bean(name = \"sqlSessionFactory\") public SqlSessionFactoryBean sqlSessionFactory(DataSource dataSource) &#123; SqlSessionFactoryBean factoryBean = new SqlSessionFactoryBean(); factoryBean.setDataSource(dataSource); factoryBean.setTypeAliasesPackage(\"xyz.uutech.www.opencartservice.model\"); PageHelper pageHelperPlugin = new PageHelper(); Properties properties = new Properties(); properties.setProperty(\"dialect\", \"mysql\"); pageHelperPlugin.setProperties(properties); Interceptor[] plugins = new Interceptor[] &#123;pageHelperPlugin&#125;; factoryBean.setPlugins(plugins); return factoryBean; &#125;@Bean public MapperScannerConfigurer mapperScannerConfigurer() &#123; MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer(); mapperScannerConfigurer.setBasePackage(\"xyz.uutech.www.opencartservice.repository\"); mapperScannerConfigurer.setSqlSessionFactoryBeanName(\"sqlSessionFactory\"); return mapperScannerConfigurer; &#125; 就是自己创建datasource覆盖spring boot默认的即可，上面的例子中使用到的DBConfig类正是与disconf结合的点： 12345678910111213141516171819202122232425262728293031323334353637383940414243@DisconfFile(filename = \"db.properties\")public class DBConfig &#123; private String url; private String username; private String password; private String driverClassName; @DisconfFileItem(name = \"url\") public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; @DisconfFileItem(name = \"username\") public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; @DisconfFileItem(name = \"password\") public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; @DisconfFileItem(name = \"driver-class-name\") public String getDriverClassName() &#123; return driverClassName; &#125; public void setDriverClassName(String driverClassName) &#123; this.driverClassName = driverClassName; &#125;&#125; 和官方demo基本上一样。 这样我们运行项目，即可看到会先和disconf通信并下载我们需要的配置文件到本地。 关键就是第二步，对我这种半吊子选手就有点麻烦了。我们要想做到动态变更datasource，需要借助AbstractRoutingDataSource这个类，网上有不少讨论spring boot下为mybatis配置多个数据源的文章，都是推荐使用这个抽象类来搞的。 依葫芦画瓢，我们也这么做： 123456public class DisconfDataSource extends AbstractRoutingDataSource &#123; @Override protected Object determineCurrentLookupKey()&#123; return \"TARGET\"; &#125;&#125; 注意，我们的场景其实并非是多个数据源，我们的目的是替换旧的数据源，so，这里我们直接在determineCurrentLookupKey方法中固定返回一个标识位。 然后我们将第一步中定义的dataSource修改一下： 1234567891011121314151617@Bean(name = \"dataSource\")public DataSource dataSource(DBConfig dbConfig)&#123; DataSource ds = DataSourceBuilder.create() .url(dbConfig.getUrl()) .username(dbConfig.getUsername()) .password(dbConfig.getPassword()) .driverClassName(dbConfig.getDriverClassName()) .build(); Map&lt;Object, Object&gt; dss = new HashMap&lt;&gt;(); dss.put(\"TARGET\", ds); DisconfDataSource dds = new DisconfDataSource(); dds.setTargetDataSources(dss); return dds;&#125; 注意这里我们要保持那个自定义的标识位一致。做到这里，我们其实已经让mybatis使用我们指定的数据源了，根据disconf官方的Tutorial 14 ，我们还需要为对应的变更事件绑定回调： 123456789101112131415161718192021222324252627@Service@DisconfUpdateService(classes = &#123;DBConfig.class&#125;)public class sqlSessionFactoryUpdateCallback implements IDisconfUpdate &#123; @Autowired private DataSource dds; @Autowired private DBConfig dbConfig; @Override public void reload() throws Exception&#123; DisconfDataSource targetDds =((DisconfDataSource) dds); //根据更新后的配置重建数据源 DataSource dataSource = DataSourceBuilder.create() .url(dbConfig.getUrl()) .username(dbConfig.getUsername()) .password(dbConfig.getPassword()) .driverClassName(dbConfig.getDriverClassName()) .build(); Map&lt;Object, Object&gt; dss = new HashMap&lt;&gt;(); dss.put(\"TARGET\", dataSource); targetDds.setTargetDataSources(dss); targetDds.afterPropertiesSet(); &#125;&#125; 注意reload方法的最后一行，由于我们的目的是替换旧的数据源（而非在多个数据源之间切换），所以我们必须避免使用AbstractRoutingDataSource为我们缓存起来的数据源，我们可以看一下这个afterPropertiesSet方法的实现细节： 1234567891011121314public void afterPropertiesSet() &#123; if (this.targetDataSources == null) &#123; throw new IllegalArgumentException(\"Property 'targetDataSources' is required\"); &#125; this.resolvedDataSources = new HashMap&lt;Object, DataSource&gt;(this.targetDataSources.size()); for (Map.Entry&lt;Object, Object&gt; entry : this.targetDataSources.entrySet()) &#123; Object lookupKey = resolveSpecifiedLookupKey(entry.getKey()); DataSource dataSource = resolveSpecifiedDataSource(entry.getValue()); this.resolvedDataSources.put(lookupKey, dataSource); &#125; if (this.defaultTargetDataSource != null) &#123; this.resolvedDefaultDataSource = resolveSpecifiedDataSource(this.defaultTargetDataSource); &#125; &#125; 所以，这么做，我们才可以让项目立刻切换到新的数据源配置上。 遗留问题我们虽然快草猛的做到了我们想要的效果，但是这里面有个疑问，由于spring boot默认会使用数据库连接池来提升性能，我们目前的这种切换datasource的方式，是否会造成一些无法察觉的bug或性能问题，希望有这方面研究的朋友可以给我留言解答。","tags":[{"name":"配置中心","slug":"配置中心","permalink":"https://blog.kazaff.me/tags/配置中心/"},{"name":"datasource","slug":"datasource","permalink":"https://blog.kazaff.me/tags/datasource/"},{"name":"实时同步","slug":"实时同步","permalink":"https://blog.kazaff.me/tags/实时同步/"},{"name":"自动更新","slug":"自动更新","permalink":"https://blog.kazaff.me/tags/自动更新/"},{"name":"disconfig","slug":"disconfig","permalink":"https://blog.kazaff.me/tags/disconfig/"},{"name":"spring boot","slug":"spring-boot","permalink":"https://blog.kazaff.me/tags/spring-boot/"},{"name":"mybatis","slug":"mybatis","permalink":"https://blog.kazaff.me/tags/mybatis/"}]},{"title":"EC2下RedHat的nginx+php-Fpm环境搭建小结","date":"2016-07-05T09:37:12.000Z","path":"2016/07/05/EC2下RedHat的nginx+php-fpm环境搭建小结/","text":"这几天在折腾AWS上的环境，免费套餐由于配置太低，原本打算放的项目临时调整，所以现在需要搭建wordpress环境（nginx，php），原本so easy的事儿，结果折腾了一天半，恶心坏了啊~ nginx由于ubuntu下安装php-fpm死活失败，查了一下竟然说官方仓库中的版本有问题，原本我就对ubuntu不太熟，一看又来个官方问题，玩儿蛋去吧，果断更换成RedHat系统。 结果谁知道，安装nginx时，默认的AWS下的RedHat源里没有啊。使用这里提供的方法添加对应的源，就可以顺利安装nginx。 php-fpm记得安装之前，先执行：1yum remove php* 然后，我没有安装7.0版本的php，所以，上面给的那个文章中的php安装部分不适合我，1yum install php56w php56w-fpm php56w-mysql php56w-gd 然后按照之前的一篇文章来配置nginx和php-fpm即可，最后执行：12service nginx restartservice php-fpm restart 开启对应服务即可。 mysql这里我们不需要在EC2上安装mysql，直接连接RDS即可。不过这里还会碰到一个问题：数据库创建连接失败。 排除了rds地址，端口，用户名密码填写失败的问题后，我们只能把思路放在系统级别了，这里提供了解决方案：1setsebool -P httpd_can_network_connect=1 至少，我是靠这个办法搞定的，别问我为啥，不造啊~ 文件上传失败wordpress下上传文件，系统会提示下面这个报错： the uploaded file could not be moved to wp-content/uploads/2016/05. 就是这个问题，让我折腾了一整天，阿西吧！一直以为是因为nginx和php-fpm所使用的权限导致的，试了各种设置，完全不行啊~ 最后老思路，既然不是软件环境的事儿，那就来看看操作系统呗，这篇文章就是解决方案：1/usr/sbin/sestatus 若该命令返回的结果中提示SELinux enabled，你就需要:1vi /etc/selinux/config 将SELinux的值从enforcing改为disabled,然后重启EC2即可。 总结学艺不精啊，半吊子运维果然不给力啊~","tags":[{"name":"php-fpm","slug":"php-fpm","permalink":"https://blog.kazaff.me/tags/php-fpm/"},{"name":"aws","slug":"aws","permalink":"https://blog.kazaff.me/tags/aws/"},{"name":"ec2","slug":"ec2","permalink":"https://blog.kazaff.me/tags/ec2/"},{"name":"nginx","slug":"nginx","permalink":"https://blog.kazaff.me/tags/nginx/"}]},{"title":"AWS免费环境搭建","date":"2016-06-29T09:37:12.000Z","path":"2016/06/29/AWS免费环境搭建/","text":"告诉大家一个好消息，亚马逊云（AWS）提供免费套餐好久了~好吧，可能你们比我知道的都早！ 由于种种原因，我们部门打算将自身的一个ERP项目部署在AWS上，既然有免费套餐，那是测试的最佳条件了。开始弄吧！不过，注册AWS是需要你填写信用卡的，有点小不放心啊。网上搜了搜，不少人说AWS有些收费坑，所以建议使用前认真阅读文档从而避免不必要的开销。幸好是公司项目，所以开通帐号的事儿自然不需要我来操心。 注册完帐号后，我们就可以登录后台管理界面来开始搭建项目环境了。正常情况下，新帐号的后台中是没有运行任何实例的，需要我们来根据需要逐个创建。 EC2这玩意儿相当于web server，我们需要它来跑tomcat或nginx等web容器，所以它必不可少。创建EC2也很简单，直接根据新建指南完成流程即可，注意，根据自己的目标人群，选择合适的可用区哟~例如，我们的项目是给美国客户用的，所以自然会选择对应美国的可用区。 在新建的过程中，还会碰到VPC的设置，简单的说，VPC就是AWS给你提供的一个虚拟局域网络。既然大家使用AWS，肯定是离不开集群部署，所以一个局域网络环境是必须的。官方文档有很大的篇幅在讲VPC，而且我感觉也是最复杂的一个知识点。不过，幸好目前AWS会为所有的可用区提供默认的VPC配置，所以，如果你不是有特殊的组网需求，一般你可以选择默认的VPC即可。 安装步骤中，会需要你配置磁盘存储，默认只有一个8G的磁盘供EC2来使用，但免费套餐中提供了大小为30G的EBS供你使用，所以你可以在该步骤中添加一块满足免费配额的磁盘，注意，是SSD的哟~ 最后还会提示你需要下载一个私钥(xxx.pem)供SSH远程登录使用，要好好保存，只有一次下载的机会~后面我们会说如何使用它。 一切就绪，点击完成即可坐等实例初始化了。查了一些资料，有看到说默认情况下，每次EC2重启实例，外网ip地址可能会变更，所以我们可以顺手申请一个弹性IP，点击“分配新地址”，然后绑定到目前唯一的那个EC2实例上即可。 好了，实例状态应该已经成为“running”了，我们就可以用ssh工具连接了。我工作机是win7环境，使用的是MobaXterm Personal Edition工具，非常不错的SSH工具哟~吐血推荐！ 创建ssh连接之前，我们需要在使用该工具提供的MobaKeyGen小工具（Tools下），点击”load”按钮，选择前面得到的那个pem文件，然后点击”save private key”，就可以得到我们所需要的ppk文件了！ 然后在ssh连接之前，还需要在EC2的管理后台看一下“安全组”，确认EC2使用的安全组下route配置的”入站”规则，如果没有开启22端口，则手动开放。 一切就绪了，可以ssh了，我的EC2是ubuntu的，默认使用的用户名就是ubuntu，由于我们使用的是证书登录，所以不需要密码~ 登录以后执行df -h命令，你会发现磁盘空间并没有我们配置的30G磁盘，那是因为还需要我们手动挂载，具体方法可以看官方手册。 剩下的工作就是你自己搭建你的web server所需环境了。 RDS免费套餐中的EC2，怎么说好呢，其实配置很低的，况且之所有使用云平台，就是想尽可能做到零运维成本不是么？所以非常不推荐自己在EC2上安装MYSQL，而是使用RDS。 但是，一定要留意的是，创建RDS的时候，一定要明确指定可用区，最好是和EC2的保持同一个可用区，这样可以避免它们之间的流量费用。 RDS类型，自然是选择与你匹配的，例如MYSQL。然后呢，我们就坐等实例的初始化，期间我们同样需要在安全组中配置所需要的规则，不过其实啊，RDS一般应该是放在内网环境下的，所以为了安全考虑其实应该不提供对外的ip地址~ 不过，我们一开始很可能需要用客户端工具连接RDS的，所以还是需要相关配置的（和EC2的配置一样）。不过，还需要注意一点，那就是必须在EC2上的应用配置中使用RDS实例提供的内网ip，或者提供的那个实例域名（该域名是指向内网ip的）。这样，你就真正做到EC2和RDS通信不产生流量费用了。 目前该说的，就这么多，以后有内容再补充吧~再见！","tags":[{"name":"aws","slug":"aws","permalink":"https://blog.kazaff.me/tags/aws/"},{"name":"ec2","slug":"ec2","permalink":"https://blog.kazaff.me/tags/ec2/"},{"name":"rds","slug":"rds","permalink":"https://blog.kazaff.me/tags/rds/"},{"name":"vpc","slug":"vpc","permalink":"https://blog.kazaff.me/tags/vpc/"},{"name":"弹性ip","slug":"弹性ip","permalink":"https://blog.kazaff.me/tags/弹性ip/"}]},{"title":"给Docker registry加上SSL","date":"2016-06-20T09:37:12.000Z","path":"2016/06/20/给docker registry加上SSL/","text":"之前我们的registry是裸奔的，今天我们就给它穿点衣服~参考主流的做法，就是搭建一个nginx节点来做registry的反向代理，然后在nginx上配置ssl证书来达到安全校验的目的。基于docker的话，我们只需根据nginx的官方镜像创建一个容器即可：123docker run --name nginx --restart=always -p 443:443 -v /www:/www \\-v /www/nginx/nginx.conf:/etc/nginx/nginx.conf:ro -d --link registry:registry \\nginx:latest 先别着急执行上面这条命令，首先，我们注意两点： link了registry容器，该容器就是我们的裸奔registry 挂在了www文件夹，这里面就放了SSL使用的证书 跟随这篇文章的生成证书流程，完成了所需要的证书文件后，我们来配置一下nginx的配置文件：123456789101112131415161718192021222324252627282930313233...http &#123; sendfile on; keepalive_timeout 65; upstream registry &#123; server registry:5000; &#125; server &#123; listen 443; server_name registry.me; ssl on; ssl_certificate /www/nginx/private/registry.me.crt; ssl_certificate_key /www/nginx/private/registry.me.key; client_max_body_size 0; chunked_transfer_encoding on; #location /v2/ &#123; # #&#125; location / &#123; proxy_pass http://registry; proxy_set_header HOST $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_read_timeout 900; &#125; &#125;&#125; 我们这里给nginx配置的server_name和生成证书时输入的Common Name保持一致。这样就可以run了。 然后我们还需要给物理机映射对应的443端口，另外还有修改hosts文件，增加域名绑定。之后就可以试试了：1curl -vk https://registry.me/v2/_catalog 注意，由于我们的registry使用的是v2版本~ shipyard命令行下的docker使用，总不是那么省心，所以我们可以安装shipyard，它提供了web ui来帮助我们更好的使用docker。 注意，官方提供的deploy脚本中使用的都是dockerhub上的镜像，我们国内下载起来非常慢，外加sh脚本执行时进度提醒不及时，很容易白等一整天，推荐的方式是自己先将所需的镜像pull到本地，再执行deploy脚本。这里不推荐切换deploy中的镜像为国内镜像源，主流的几个国内镜像仓库都没有匹配的源，会导致版本不兼容的~ 安装好以后会暴露8080端口和一个admin帐号（密码是shipyard），你可以浏览器上直接访问了（记得做物理机端口映射）。 不幸的是，目前官方主版本并不支持registry的v2版本接口，所以我们无法在shipyard中添加私有仓库。虽然我在github上看到有支持v2的branch提供我们使用，但并没有提供足够详细的文档，我这个新手不知道如何部署啊~可悲~如果你知道，请留言赐教~不胜感激！","tags":[{"name":"docker","slug":"docker","permalink":"https://blog.kazaff.me/tags/docker/"},{"name":"registry","slug":"registry","permalink":"https://blog.kazaff.me/tags/registry/"},{"name":"SSL","slug":"SSL","permalink":"https://blog.kazaff.me/tags/SSL/"},{"name":"shipyard","slug":"shipyard","permalink":"https://blog.kazaff.me/tags/shipyard/"}]},{"title":"搭建本地私有Docker仓库","date":"2016-06-16T16:37:12.000Z","path":"2016/06/16/搭建本地私有docker仓库/","text":"在上一篇中，我们已经把gitlabCI生成的镜像推送到灵雀云上面。不过这还不够，毕竟真实项目的镜像还是需要放在内网环境下才能避免不必要的麻烦和确保良好的下载速度（说白了就是没钱买私有镜像仓库~~）。首先，我们先把之前的那个demo项目的.gitlab-ci.yml修改一下：1234567build:image: stage: build script: - docker build -t 172.17.0.1:5000/kazaff/hello-kazaff . - docker push 172.17.0.1:5000/kazaff/hello-kazaff tags: - shell 注意，这次我们push到了自己搭建的Registry中。本以为只要使用官方提供的registry直接run一个docker容器就搞定了，要真那么顺利也就写这篇日志的必要了！还有和上一篇文章中使用的Runner方式不同，眼尖的童鞋应该注意到上面新的.gitlab-ci.yml中我们把tags由原先的docker改成了shell，后面会说原因，至于如何创建一个executor是shell类型的Runner，上一篇我也介绍过了。 按照官方文档，有两种方法来搭建私有仓库，当然，第二种依赖亚马逊S3的路子我们pass了，憋说话，我是穷逼。 而直接执行第一种方法给的命令，等着你的就是悲剧。首先，从官方下方的留言区可以看出，貌似latest并不是指向最新的registry版本，而且我这边pull镜像的时候也总登录不上dockerhub，还是老办法，使用灵雀云，我并非在给它打广告，确实能找到最开放的国内镜像库了： 1docker pull index.alauda.cn/library/registry:2.4.1 注意，这次一定要带上tag号啊~ 然后我们直接执行：1docker run -d -p 5000:5000 -v /srv/docker/registry:/var/lib/registry --restart=always --name registry index.alauda.cn/library/registry:2.4.1 这次，就应该搭建好了，如果你想让物理机环境下也能访问，记得依然要做虚拟机的端口映射哦~ 然后我们只需要向我们自己的gitlab提交最新版本的项目文件，即可触发build了~ 你可能会碰见比我复杂的多的问题，这里有一篇文章总结的很全面，推荐阅读。 这样，我们就基本上搭建了一个自娱自乐的CI/CD环境，不过，想直接投入团队使用还差太多，后面我会继续改善和丰满该工作流。不过今天就先到这里~ 补充之前测试宿主机直接push镜像时，并没有碰到问题，可在Runner中push时肯定不能使用localhost，一旦使用registry所在机器的ip地址时，就会碰到下面的问题：1tls: oversized record received with length 20527 按照上面的帖子给的方法貌似没有搞定，不过换了另外一个办法：1vi /lib/systemd/system/docker.service 该文件中添加insecure-registry设置后的样子大概如下：1234567891011121314151617[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network.target docker.socketRequires=docker.socket[Service]Type=notifyExecStart=/usr/bin/docker daemon --insecure-registry=172.17.0.1:5000 -H fd://MountFlags=slaveLimitNOFILE=1048576LimitNPROC=1048576LimitCORE=infinityTimeoutStartSec=0[Install]WantedBy=multi-user.target 然后我们重启一下docker：12systemctl daemon-reloadservice docker restart 搞定，注意修改成你的registry的ip哦~ 此外，我也把项目中所有使用到的镜像都改成了私有镜像仓库地址，速度杠杠的。不过，在下载dockerhub镜像时碰到一个小插曲，总提示我无法登录帐号！查了不少帖子，官方只是说修复了此问题，但我还是无法登录咋整？ 后来发现，原来可以试试单独执行：1docker login 提示你输入username时，不要输入邮箱地址，而是输入你在dockerhub后台设置的username，就能登录了。。。 别高兴太早，我早就暗示过你们，过程是坎坷的！前面我提到过，我们新建了一个Runner，executor类型是shell，为什么呢？之前docker类型的好好的不是么？ 前面我们不是提到私有仓库证书的事儿么？虽然我们已经将宿主机的docker环境关闭了TLS，不过别忘了我们的Runner可是docker-in-docker啊~我找了好多地方，都没有提供如何使Runner启动的那个docker也关闭TLS，否则我们依然会碰到上面的那个问题！~恳请知道解决方案的童鞋留言帮助我升级！~ 当然，后面我也会为我们的私有镜像仓库搭建TLS环境，到时候自然会完美解决这个问题！暂时让大家失望了，抱歉~小哥我真的是尽力了，不信看图： 依旧别高兴太早，还记得我的docker环境么？虽然我们解决了宿主机下使用私有仓库的TLS问题，但我的物理机是window啊，我使用docker toolbox安装的docker依然无法使用我们的私有仓库，唉，这就是坎坷。不过知道了问题的本质就好办了，只需要看一下win环境下如何设置insecure-registry即可： 1vi ~/.docker/machine/machines/dev/config.json 找到文件中下面这段配置按需修改即可：12345&quot;EngineOptions&quot;: &#123; &quot;InsecureRegistry&quot;: [ &quot;192.168.1.23:5000&quot; ], &#125; 注意，这里我填写的是私有镜像仓库的宿主机（ubuntu）的端口通过virtualbox网络配置映射到物理机对应端口后，使用的物理机地址，原因你应该已经猜到了，我就不啰嗦了。 总算踉踉跄跄配好了个粗狂版~","tags":[{"name":"docker","slug":"docker","permalink":"https://blog.kazaff.me/tags/docker/"},{"name":"registry","slug":"registry","permalink":"https://blog.kazaff.me/tags/registry/"}]},{"title":"尝试持续集成--第一版","date":"2016-06-16T09:37:12.000Z","path":"2016/06/16/尝试持续集成--第一版/","text":"趁热打铁，根据之前我们自己搭建的gitlab-ci环境，我们来跑一个demo项目，爽一下~我们的目的是，程序员提交代码到gitlab，会触发自动创建镜像，并上传到公共的镜像仓库（目前先这么搞）。 根据官方的文档，我们先在gitlab上搭建一个项目，结构如下：1234567/.|-&gt;.dockerignore|-&gt;.gitignore|-&gt;.gitlab-ci.yml|-&gt;Dockerfile|-&gt;app.js|-&gt;package.json 其中，.dockerignore文件内容：123.git.gitlab-ci.ymlREADME.md .gitignore :1node_modules .gitlab-ci.yml:12345678910111213image: docker:latestservices:- docker:dindbuild:image: stage: build script: - docker build -t index.alauda.cn/kazaff/hello-kazaff . - docker login -u kazaff -p 我的密码 index.alauda.cn - docker push index.alauda.cn/kazaff/hello-kazaff tags: - docker Dockerfile:123456789FROM node:slimCOPY . /myapp/WORKDIR /myapp/RUN npm installCMD [&quot;node&quot;, &quot;app&quot;]EXPOSE 80 app.js:12345678var koa = require(&apos;koa&apos;);var app = koa();app.use(function *()&#123; this.body = &apos;Hello, Kazff&apos;;&#125;);app.listen(80); package.json:123456789101112131415161718&#123; &quot;name&quot;: &quot;docker-gitlab-node-demo&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;description&quot;: &quot;test for gitlab-ci&quot;, &quot;main&quot;: &quot;app.js&quot;, &quot;scripts&quot;: &#123; &quot;test&quot;: &quot;echo \\&quot;Error: no test specified\\&quot; &amp;&amp; exit 1&quot; &#125;, &quot;keywords&quot;: [ &quot;demo&quot;, &quot;gitlab-ci&quot; ], &quot;author&quot;: &quot;kazaff&quot;, &quot;license&quot;: &quot;MIT&quot;, &quot;dependencies&quot;: &#123; &quot;koa&quot;: &quot;^1.2.0&quot; &#125;&#125; 好了，把上面这个项目提交到我们的gitlab环境下，就会触发自动的“build”了（确保你的gitlab配置开启了”build”）！ 不过怎么可能顺利呢？第一次一般都是失败的，查看错误日志：12345678910gitlab-ci-multi-runner 1.1.2 (78b3f82)Using Docker executor with image node:argon ...Pulling docker image node:argon ...Running on runner-e02dcb0d-project-1-concurrent-0 via 21f65ebc58fa...Cloning repository...Cloning into '/builds/root/welcome'...fatal: unable to access 'http://gitlab-ci-token:xxxxxx@localhost:10080/root/nodeDemo.git/': Failed to connect to localhost port 10080: Connection refusedERROR: Build failed: exit code 1 你的错误信息可能和我的不完全一样，不过问题是一样的：Runner启动的docker容器里无法访问到localhost:10080这个地址（能访问到才怪）。这一般是由于我们的测试环境没有使用域名导致的，gitlab论坛里也不少人讨论这个问题，如果你是在部署正式的gitlab环境，那你自然会有一个域名来使用。不过我这里只是搭建测试环境，所以我使用了一种投机的方法： 修改Runner的/etc/gitlab-runner/config.toml文件，在其中的[runner.docker]下增加：1extra_hosts = [&quot;localhost:172.17.0.1&quot;] 意思是，让runner启动的容器中将”localhost”域映射到我的ubuntu宿主机，这样runner请求”localhost:10080”时就会被路由到正确的地址，不推荐正式环境下这么做~ 然后再次触发build，这次看看还会碰到什么问题么：1write /var/lib/docker/tmp/xxx: no space left on device 这次悲剧了，我的ubuntu虚拟主机分配的磁盘空间太小，结果现在提示硬盘控件不足……（这里我发现其实我分配的磁盘控件（8G）才使用了2G多一点，不明白VirtualBox为什么不给虚拟机扩展容量了！） 我只能删除掉一些不使用的镜像和容器来腾出一些地儿，再次build，好家伙，虚拟机直接死掉了。 [直播]对，目前还处于无响应状态，不知道为啥，我只能等。。。已经快二十分钟，妈蛋！我方了，咋办？…… 我强行重启虚拟机后一切都归零了，gitlab container无法正常启动了，娃哈哈哈哈~玩我呢是吧？也没有任何错误日志给我参考，得了，重新装一次环境得了~ 重新安装环境很快的其实，我们只需要停止所有的容器，然后删除重建一下/srv/docker/gitlab/这个目录即可，该目录就是之前我们搭建环境时往相关容器中加载的本地卷，重建它们以为这之前在gitlab上的操作都不要了，再次start相关容器，一个干净的gitlabCI环境就好了（docker的魔力）~~ 除此之外由于我们的gitlabCI重建后Runner的token也更新了，所以要重新register一下我们的Runner，一切又和好如初！ 不怕死的我再一次尝试build，这次会成功么？（有了这次教训，建议你再重新开始build之前先对虚拟机做一个快照，这样再失败就可以直接回滚了~） 由于之前碰到了磁盘空间不足的问题，所以这里就建议使用微容器，可以省下来一笔相当可观的空间和流量啊，Dockerfile的第一行改成：1FROM iron/node:dev 好了，漫长的等待后（全看网速）就可以看到灵雀云中已经成功创建好镜像了！ 这里需要 强烈提醒：在测试该镜像时：1docker run -it --name hello-kazaff -p 10081:80 index.alauda.cn/kazaff/hello-kazaff 一定要加-it，不然你就傻逼了，像我一样无法退出该docker容器了。。。。草！有知道如何解决这个问题的朋友请一定要给我留言啊！","tags":[{"name":"docker","slug":"docker","permalink":"https://blog.kazaff.me/tags/docker/"},{"name":"持续集成","slug":"持续集成","permalink":"https://blog.kazaff.me/tags/持续集成/"},{"name":"gitlab","slug":"gitlab","permalink":"https://blog.kazaff.me/tags/gitlab/"},{"name":"gitlab-ci","slug":"gitlab-ci","permalink":"https://blog.kazaff.me/tags/gitlab-ci/"},{"name":"gitlab-runner","slug":"gitlab-runner","permalink":"https://blog.kazaff.me/tags/gitlab-runner/"},{"name":"微容器","slug":"微容器","permalink":"https://blog.kazaff.me/tags/微容器/"}]},{"title":"基于docker+gitlabCI搭建私有集成环境","date":"2016-06-15T09:37:12.000Z","path":"2016/06/15/基于docker+gitlabCI搭建私有持续集成环境/","text":"看了几天的docker，感觉好极了。现在回到我一开始的目标：构建一个团队内部的持续集成（下文统称CI）环境，并梳理出适合我们自己的工作流。今天我们主要是来搭建依赖的环境： virtualBox ubuntu server 16 docker gitlab version8+(该版本以上自带CI模块) gitlab runner gitlab需要的其它组件（redis，postgresql） 之前学习docker时，一直都是基于自己的工作机，装的是win7 64bit，win下和docker相关的问题可以看我之前的文章。由于我们现在是要搭建一个自动化CI环境，提供web ui来供用户使用，所以环境搭建部分的操作完全不需要使用者参与，就不存在之前考虑的所谓的OS水土不服问题！介于网上多数资料都是使用ubuntu来作为docker的宿主系统，所以我这里就放弃了亲爱的centos（其实centos也是没问题的，我只是低调的炫耀自己通吃而已~见谅）。 老规矩，先来共享一下相关的文献资料： Gitlab CI 文档 使用Gitlab CI &amp; Docker搭建CI环境(主要借鉴该文章后半部分提到的“提升构建速度”) sameersbn/gitlab(gitlab docker镜像的使用文档) ubuntu下安装docker我们这次不使用win下的dockerToolbox来安装docker了，而是直接在虚拟机中安装unbuntu系统，然后直接在unbuntu中安装docker： 1curl -sSL https://get.docker.com/ | sudo sh 目前这样的安装方式，依然需要注意，后面运行gitlab容器时配置的端口映射只是将容器的端口映射到了ubuntu虚拟机系统上，如果想让物理机直接访问，还需要在virtualBox上再进行一次端口映射，具体步骤在这篇文章中我已经详细介绍过了。 镜像选择问题docker环境装好后，就需要开始下载所需要的镜像了： sameersbn/gitlab:latest sameersbn/postgresql:latest sameersbn/redis:latest sameersbn/gitlab-ci-multi-runner:latest 由于我们是在国内，所以直接使用hub.docker.com来下载无异于浪费生命，还好国内也有良心镜像库： sameersbn/gitlab:latest sameersbn/postgresql:latest sameersbn/redis:latest sameersbn/gitlab-ci-multi-runner:latest 这里需要提醒的是，尽管灵雀云上显示的sameersbn/gitlab镜像版本是”7.14.1”，不过别担心，只是md文件没有更新而已，可以切换到页面的“版本”选项卡来确认latest版本。 这样基本上20分钟就可以把三个镜像全部下载到本地啦~ 启动容器镜像下载完毕后，根据官方文档我们需要创建对应的docker容器来启动相关的服务，我并没有使用官方提供的第一种方法（docker-compose），主要是不熟悉~ 我们手动来完成三个容器的启动： 123456789101112131415161718192021#启动postgresql容器docker run --name gitlab-postgresql -d \\ --env &apos;DB_NAME=gitlabhq_production&apos; \\ --env &apos;DB_USER=gitlab&apos; --env &apos;DB_PASS=password&apos; \\ --env &apos;DB_EXTENSION=pg_trgm&apos; \\ --volume /srv/docker/gitlab/postgresql:/var/lib/postgresql \\ index.alauda.cn/sameersbn/postgresql#启动redis容器docker run --name gitlab-redis -d \\ --volume /srv/docker/gitlab/redis:/var/lib/redis \\ index.alauda.cn/sameersbn/redis#最后启动gitlab容器docker run --name gitlab -d \\ --link gitlab-postgresql:postgresql --link gitlab-redis:redisio \\ --publish 10022:22 --publish 10080:80 \\ --env &apos;GITLAB_PORT=10080&apos; --env &apos;GITLAB_SSH_PORT=10022&apos; \\ --env &apos;GITLAB_SECRETS_DB_KEY_BASE=kazaffisagoodcoder&apos; \\ --volume /srv/docker/gitlab/gitlab:/home/git/data \\ index.alauda.cn/sameersbn/gitlab 注意，上面的命令我已经改成使用我们本地镜像的名字了，还有在启动gitlab容器时我使用了一个自定义的字符串来赋值GITLAB_SECRETS_DB_KEY_BASE，由于是本地测试环境，外加我实在不知道如何通过在--env命令中使用shell变量来使用pwgen -Bsv1 64生成的随机字符串（手动输入简直等于自杀！）~知道如何做的朋友请留言赐教，不胜感激。 根据自己的硬件配置，可能需要稍等那么一分钟左右，在做好物理机端口映射配置后，我们就可以在本地浏览器中访问http://localhost:10080/来使用gitlab了！（可能由于gitlab首次启动初始化问题，你可能会看到gitlab提示的500错误，稍等一下再刷新即可） 现在你基本上就已经拥有了本地的gitlab环境，你可以使用本地的git客户端来创建一个gitlab测试项目，试试clone到本地，试试commit到gitlab，应该妥妥的~ Gitlab Runner这部分内容文章开头给的文档并没有汉化，不过没关系，我们都懂英文，对吧？ 出于性能考虑官方不推荐将Runner安装在和gitlab同一台物理机上，出于安全考虑也不推荐将其安装在独立的物理机上，靠，也只能用docker来跑了！ 这里还有两个概念： 特定Runner 共享Runner 前者适配有特殊需求的项目，同时也可以避免重要的项目runner资源被占用的问题。 而如果多个项目的优先级一样，并且有非常相似的依赖环境，那使用共享Runner是一个不错的选择。 既然我们决定使用docker容器来跑runner，那就先选个镜像吧，官方提供的是：gitlab-ci-multi-runner，我是用的灵雀云镜像了，注意，这里灵雀云镜像上的md介绍依然是错误的，镜像本身是和官方镜像一致的，小不完美啊 :———(~ 还要说的一点是，官方提供的这个镜像是纯净镜像，不包含任何其它的环境（例如java环境或node环境），而你需要根据自己的项目实际情况来创建满足的自定义镜像。官方提供了几个语言环境的例子~ 这种使用runner的思路相当于我们拿这个运行着runner的docker容器当作一个“物理机”，然后在这台“物理机”上做.gitlab-ci.yml指定要做的事儿。在此基础上，我们依然可以让Runner执行docker build模式，这里就涉及到“docker-in-docker”的思想了。具体细节官方提供了比较清晰的文档。（其实，如果你使用文档中提到的“docker-in-docker”方案的话，那实际上就是“docker-in-(docker-in-docker)”，好绕啊~） 提到Runner的executor的类型，文档里有提到不少（shell，docker，ssh，等等），差别我感觉都不是太大，主要还是shell和docker两大类。 理论姿势就这么多，接下来我们跑起我的runner容器： 1234567mkdir -p /srv/docker/gitlab-runnerdocker run --name gitlab-ci-multi-runner -d --restart=always \\ --volume /srv/docker/gitlab-runner:/home/gitlab_ci_multi_runner/data \\ --link gitlab:gitlab --env=&apos;CI_SERVER_URL=http://gitlab/ci&apos; \\ --env=&apos;RUNNER_TOKEN=你gitlab生成的token&apos; \\ --env=&apos;RUNNER_DESCRIPTION=myrunner&apos; --env=&apos;RUNNER_EXECUTOR=shell&apos; \\ index.alauda.cn/sameersbn/gitlab-ci-multi-runner 这里需要提醒的是两点： 你自己的gitlab生成的token可以在http://localhost:10080/admin/runners看到（我假设你和我一样要创建的是一个共享Runner） CI_SERVER_URL参数不要使用你物理机浏览器地址栏里的值，我们在上面的命令中已经将之前创建的gitlab容器link到了runner容器，so，我们只需要填写设置的主机名即可，否则无法注册成功！ 接下来刷新你的gitlab管理员后台，你将会看到注册成功的Runner信息。 还没完，由于我们使用的是shell模式，所以我们还需要进入到runner容器中来安装docker环境： 1234docker exec -it gitlab-ci-multi-runner bashcurl -sSL https://get.docker.com/ | sudo shsudo usermod -aG docker gitlab-runnersudo -u gitlab-runner -H docker info 如果可以看到正确的docker版本信息，那就说明一切顺利。但怎么可能那么顺利！！ 事实证明我太天真了，在docker容器中由于文件系统是只读的，所以无法安装docker环境~至少我们现在的做法还不行，如下图： 这里说一个插曲，当得到这个结论后的我失望的打算将这个悲剧的镜像删除，我在gitlab admin后台的runner页面点击“remove”按钮，然后又将对应的docker容器也删除掉。当我再次使用相同的命令打算启动一个干净的runner容器时，我发现gitlab的runner页面不在提示成功注册runner了！我彻底方了！一切都和第一次执行流程一致，为啥这次就不认了呢？ 于是我发现了一个秘密，上面的创建runner容器的命令中有一个参数：1--volume /srv/docker/gitlab-runner:/home/gitlab_ci_multi_runner/data 我在宿主机的/srv/docker/gitlab-runner目录下找到了答案，原来一直试图想找到的config.toml文件在这里，也是因为这个文件的缘故所以gitlab才不在接受我的runner注册的！删除并重建这个目录即可！ 走了弯路不可怕，可怕的是接下来不知道怎么办？没关系，我们现在先在宿主机上安装gitlab-ci-multi-runner环境，并使用官方提供的docker-in-docker方式来增加一台runner： 123456789curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-ci-multi-runner/script.deb.sh | sudo bashsudo apt-get install gitlab-ci-multi-runnersudo gitlab-ci-multi-runner register -n \\ --url http://localhost:10080/ci \\ --token 你gitlab生成的token \\ --executor docker \\ --description &quot;My Docker Runner&quot; \\ --docker-image &quot;index.alauda.cn/library/docker:latest&quot; \\ --docker-privileged 记住，我们这里要填写的gitlab-ci coordinator URL应该是：http://localhost:10080/ci。 这里还有个细节是官方文档没有提到的，那就是让注册好的Runner运行起来，不过其实默认安装好gitlab-ci-multi-runner后它就自己已经以服务的方式注册到系统里了（服务名：gitlab-runner），每次注册runner导致config.toml文件变更后都会自动触发该服务的reload。但了解一下这个细节还是对分析问题很有帮助的！ 这样我们就应该有两个runner了（一个是shell类型，一个是docker类型）： //todo ubuntu -&gt; docker-in-docker -&gt; runner -&gt; docker engine .gitlab-ci.yml这个文件应该放在我们的项目repo的根目录下，用来描述当发生目标行为后，runner要做的工作。官方文档有对其语法的描述。 环境基本上就搭建好了，下一篇我们就开始设计工作流了。","tags":[{"name":"docker","slug":"docker","permalink":"https://blog.kazaff.me/tags/docker/"},{"name":"持续集成","slug":"持续集成","permalink":"https://blog.kazaff.me/tags/持续集成/"},{"name":"gitlab","slug":"gitlab","permalink":"https://blog.kazaff.me/tags/gitlab/"},{"name":"gitlab-ci","slug":"gitlab-ci","permalink":"https://blog.kazaff.me/tags/gitlab-ci/"},{"name":"gitlab-runner","slug":"gitlab-runner","permalink":"https://blog.kazaff.me/tags/gitlab-runner/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://blog.kazaff.me/tags/ubuntu/"}]},{"title":"【转】调整虚拟机中Ubuntu Server屏幕分辨率","date":"2016-06-13T09:37:12.000Z","path":"2016/06/13/转-调整虚拟机中ubuntu server屏幕分辨率/","text":"转自：http://blog.csdn.net/weilanxing/article/details/7664324。 VMware中的Ubuntu Server的控制台窗口有点儿小，使用起来不太方便，要调整控制台的窗口大小，需要修改屏幕的分辨率，修改方法如下： 打开grub文件($vim /etc/default/grub), 修改参数GRUB_CMDLINE_LINUX的值,GRUB_CMDLINE_LINUX=”vga=0x317”, 参数值参考下图： 123456| 640x480 800x600 1024x768 1280x1024----|--------------------------------------256 | 0x301 0x303 0x305 0x30732k | 0x310 0x313 0x316 0x31964k | 0x311 0x314 0x317 0x31A16M | 0x312 0x315 0x318 0x31B $sudo update-grub $sudo reboot","tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://blog.kazaff.me/tags/ubuntu/"},{"name":"分辨率","slug":"分辨率","permalink":"https://blog.kazaff.me/tags/分辨率/"}]},{"title":"Docker for Win","date":"2016-06-13T09:00:00.000Z","path":"2016/06/13/docker for win/","text":"我所在的项目，小伙伴们都在fire in coding，我也不能闲着，我开始为项目的持续交付进行思考和设计。 第一个要解决的问题，就是开发环境，测试环境和线上环境的一致，那不用说，今天最流行的做这个事儿的应该就是docker了吧？！我的小mac还在“医院”躺着，而且考虑到同事都是win用户，所以我从docker for win下手，毕竟不希望一下子挑战大家的所有习惯。 作为docker新手的我，花了2天的时间过了一遍手册，然后就开始了实测！下面记录一下我在实际测试的时候碰到的一些小坑。 如何使用dockerfile这在linux环境下可能不是啥问题，直接根据手册提供的命令即可： 1$ sudo docker build -t myrepo/myapp /tmp/test1/ 可在win环境下，你首先会碰到无法定位win系统下dockerfile文件的问题，不过docker box已经帮你做了大量的工作（和一些历史教程相比容易好多），它将我们的c:盘直接映射到/c位置，也就是说在Docker Quickstart Terminal提供的shell环境下，直接就可以访问c:盘下的文件结构。 接下来我们就可以执行上面的build命令了，不过你会失望的看到下面的这个error: 1Error: $ docker build -t kz/test . unable to prepare context: unable to evaluate symlinks in Dockerfile path: GetFileAttributesEx C:\\Users\\Kazaff\\Test\\testdocker\\Dockerfile: The system cannot find the file specified. 唉，就知道不会那么顺利，gg了一下，这个问题已经有了不错的答案，我们只需要加一个-f即可： 1docker build -t kz/test -f ./myDockerFile . 注意，最后还有一个”.”，而且我们执行命令时，终端上下文已经是在目标dockerfile所谓的文件夹中了！ 这样你就可以在win下正常build dockerfile了。 还有，记得给你要RUN的命令加上-y参数，避免运行命令时的人机交互动作。 docker compose虽然很多教程上提到win下的docker需要手动安装docker compose，但目前我安装的最新版DockerToolbox-1.11.2已经帮我们预装好了！So，你不需要折腾了！ 端口映射和文件挂载这两件事儿一般都是在你成功安装docker，并成功拥有了一个镜像后会碰到，一般你会运行这样的命令来启动一个容器： 1docker run --name webserver -v /myweb:/etc/nginx/nginx.conf:ro -d -p 80:80 -d nginx 然后你打开DockerToolbox提供的Kitematic，选择virtualbox，你会看到确实已经运行了一个叫”webserver”的容器，切换到”Settings”界面的Port选项卡，你会看到并没有正确按照命令配置端口（或者一切正常~），不过没关系，可以直接在这个面板里进行配置。 这里需要注意，你只是将容器内部的指定端口映射到了virtualbox中docker虚拟机的指定端口，而非win下的指定端口，所以你直接在win下浏览器上使用”localhost”是无法访问的！这里有一篇文章教你如何配置virtualbox的网络端口映射来允许我们使用”localhost”。 接下来我们切换到”Settings”界面的Volumes选项卡，同样会发现并没有按照我们的命令成功映射本地文件夹到容器内！在这个界面上直接点击”change”按钮会给我们提示一些线索，原来是因为我们只能使用 本地用户文件夹的内容 作为映射地址，所以上面的命令应该改成： 1docker run --name webserver -v /c/Users/kazaff/myweb:/etc/nginx/nginx.conf:ro -d -p 80:80 -d nginx 注意，这里我使用的”/c/Users/kazaff”前缀是根据我本地真实路径写的，你可能需要查看一下自己的对应路径，在终端中执行: 1pwd ~ 即可查看你应该使用的路径前缀。","tags":[{"name":"docker","slug":"docker","permalink":"https://blog.kazaff.me/tags/docker/"}]},{"title":"Travis CI自动部署Hexo","date":"2016-06-12T09:37:12.000Z","path":"2016/06/12/Travis CI自动部署Hexo/","text":"最近在研究Docker，想搭建开发团队的持续集成（CI）和持续交付（CD）的工作流。之前一直偷懒的安排同事手工部署，后来发现这不是真的“懒”，而是“笨”！！不过非运维出身的我，确实要学的东西有太多太多，好在先辈们乐于分享，很容易就可以搜集到大量的相关文献来参考，这真是互联网时代程序员的黄金时代啊！ 在开始自动化整个项目之前，先把自己的博客自动化了试试手！但这一篇文章并不是简单重复前辈们说过的话，而是主要记录一下在实测过程中的小坑。 首先，先来看两篇不错的教程贴： 用 Travis CI 自動部署網站到 GitHub 利用travis自动部署hexo搭建在github的博客 这两篇文章可以说是相互补充吧，推荐仔细阅读！ 根据前面两位前辈的指导，如果你人品好，应该就已经不需要往下阅读了！不过，如果还是一头雾水，那可以看看我是怎么消化上面的内容的。 首先，按照步骤，我们需要在本地环境下安装Travis命令，执行：1sudo gem install travis 你以为你会一切顺利的安装好，那真是太天真了！你可能会碰到机器没有ruby环境，所以没有gem命令，那你就需要根据自己的操作系统来安装，或者你也可能和我一样碰到这个问题，那你就需要执行：1gem source -a http://rubygems.org/ 我并不是ruby用户，所以我不太清楚为啥，总之就这么弄！ 你还要记得在生成密匙，在将公有密匙添加到github对应repo中时，记得勾选“Allow write access”选项！ 然后，根据指导你需要使用travis命令来制作私有密匙的加密版本，但之前你最好先创建.travis.yml文件，并且让你的终端先进入到你repo的本地路径下，这样该步骤才能找到正确的.travis.yml文件（可能不需要手动新建也行，命令会自动创建，不过推荐手动创建，并用博客中提供的模板作为内容）。 这一步搞定以后，会生成加密文件，此时你本地ssh-keygen生成的文件已经不需要了，删掉即可。 特别注意的是，上一步自动向.travis.yml加的内容会有换行符，需要手动清除掉（这在上面第二篇文章中也有提醒！）。这里我直接给出我使用的.travis.yml。 由于我是自定义的hexo主题，所以我并没有按照前辈的做法，而是直接将theme也上传到repo中了！ 这里有个小插曲，由于我的theme皮肤文件夹是直接从之前的项目中拷贝过来的，其中携带了它自己的.git文件夹，导致后面我发现发布后博客都是白页~~呵呵，逗逼！ 还要记得，将自己博客的_config.yml文件中的 deploy 设置填写正确的repo地址，否则travis在build时会提示失败的，不过这都很好发觉，只需要观察travis提示的build信息即可！ 最后，祝你成功！ 后记前段时间我的macbook pro被我玩坏了，充不进去电了。难道是装逼遭到报应了吗？！ 自己找了家维修店（小城市没有苹果专卖，且笔记本早已过保），检修以后说要替换掉整个主板~~玩儿蛋去吧！ 找了个熟人，简直是吊炸天，直接给我把板子上的芯片给更新了，由于我不是搞硬件的，所以我可能无法描述清楚（尽管人家已经给我解释的很仔细了~），感激不尽！！~省去了我将近1000块！真是恩人啊！ 这篇文章是笔记本原地复活后的的第一篇，值得留念！ 如果有小伙伴遇到了和我一样的问题，可以给我要大牛的联系方式！","tags":[{"name":"持续集成","slug":"持续集成","permalink":"https://blog.kazaff.me/tags/持续集成/"},{"name":"hexo","slug":"hexo","permalink":"https://blog.kazaff.me/tags/hexo/"},{"name":"travis","slug":"travis","permalink":"https://blog.kazaff.me/tags/travis/"}]},{"title":"刚知道的Wordpress设计精妙","date":"2016-06-01T09:37:12.000Z","path":"2016/06/01/刚知道的wordpress设计精妙/","text":"越是老的经久不衰的系统，越是会存在非常多的惊喜。 这话放在wordpress（以下简称wp）上更是贴切！三五年前在我告别了百度空间后，其实就已经开始使用wp当博客系统了。只可惜那时候的我能力还有限（其实就是懒），从没有好好研读过wp的架构和代码。这段时间需要将公司的形象网站重构，经领导的推荐，最终决定要基于wp来做！那么总算有了解她的理由了！ 简单的gg了一下，海量的教程，皮肤，插件。可见其生态环境还是依然不错，尽管最近几年不少轻量级博客系统的诞生，但似乎并没有对wp造成太大的影响，尤其是在针对一些比较复杂需求的企业站开发时，wp的强大就体现出来了。 作为新手，这篇文章会记录下我的学习过程，作为我个人的记忆存根，如果对你有帮助，那真是让我非常荣幸~ wordpress的要点随着我对wp的了解，目前感觉到它的核心要点有以下几点： hook 模版层级 自定义查询 Widgets wordpress的小惊喜 shortcode 子主题 子主题下覆盖原主题的shortcode这个必须在子主题的functions.php中如下定义： 12345678910111213add_action( 'after_setup_theme', 'my_ag_child_theme_setup' );function my_ag_child_theme_setup() &#123; remove_shortcode( 'divider' ); add_shortcode( 'divider', 'my_ag_divider' );&#125;function my_ag_divider( $atts, $content = null ) &#123; extract(shortcode_atts(array( ), $atts)); $out = '&lt;div class=\"divider\"&gt;&lt;h2&gt;&lt;span&gt;'.do_shortcode($content).'&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt;'; return $out;&#125; 上述方案参考这里。 子主题下覆盖wordpress默认widget看这里。 彻底关闭post下面的comment不管网上你搜到什么样的教程教你实现这个需求，最简单的还是安装“disable comments”插件。 创建page template只需要在你的主题文件夹下建立模版文件，然后在模版文件顶部添加下面的注释即可： 123/*Template Name: My Custom Template*/ 设置自定义url后导致403首先你要保证你开启了apache的rewrite模块，然后在你配置的虚拟主机环境下设置下面两项： 12Options Indexes FollowSymLinksAllowOverride All nginx下就很简单了，按照这里教的做就可以了。 Contact Form 7 下的邮件发送问题目前我用的这套theme，需要激活Contact Form 7 插件，这个插件就是用来生成像“联系我们”的表单的，不同于wp的comments在于，可以使用自定义的语法来自定义你需要的表单项，并通过发送mail到指定邮箱来通知管理员的。 自定义表单项我没咋搞，这里主要说一下邮件发送的问题，花了我一天的时间，痛苦死了。默认该插件会使用wp自带的wp_mail()来完成邮件发送，但奇怪的时，wp后台并没有给出smtp配置的地方。从一些文档上看貌似是需要你的主机支持邮件功能的（具体我也没深究，总之就是我本地环境下不行）。按照网上的推荐，我安装了另一个插件：Postman SMTP。这样意味着可以将CF7的邮件发送托管给postman来做，不过奇怪的是我本地环境下cf7就是无法使用postman，看了不少文档也没有头绪。奇迹发生在我将项目部署在阿里云服务器以后，一切都正常了！阿西吧~ 正在执行例行维护 请一分钟后回来遇过你碰见这个问题，请参考这里。 多国语言由于我司直接购买的商用theme，其自带了ACF5PRO，那我们要如何做多语言功能呢，现成的插件组合： qTranslate-X 免费 ACF qTranslate 免费 这样你的网站就可以摇身一变成为多语言站点了。其中需要注意的是，custom filed的多语言，acf qTranslate插件只提供了下面几种类型： text textarea WYSIWYG 这就需要你取舍了，像我原本还使用了ACF的true/false类型，也只能想办法用text表示了。 手把手不是我夸，老外就是屌，早有牛人出了系列教程，教你如何不写一行代码完成wp建站：教程 最后推荐的神器朋友强烈推荐给我的一个基于wp的牛逼框架：themosis，据说用上它以后，你将改变宇宙的格局。","tags":[{"name":"hook","slug":"hook","permalink":"https://blog.kazaff.me/tags/hook/"},{"name":"wordpress","slug":"wordpress","permalink":"https://blog.kazaff.me/tags/wordpress/"},{"name":"模板","slug":"模板","permalink":"https://blog.kazaff.me/tags/模板/"},{"name":"主题","slug":"主题","permalink":"https://blog.kazaff.me/tags/主题/"},{"name":"插件","slug":"插件","permalink":"https://blog.kazaff.me/tags/插件/"}]},{"title":"了解协程（Coroutine）","date":"2016-05-29T09:37:12.000Z","path":"2016/05/29/了解协程(coroutine)/","text":"这个技术名词并不是第一次听到，但如果你和我一样并未深究过其细节，那可以先来看看大家是怎么评价协程的：openstack中的协程 知乎贴1 知乎贴2 协程与事件循环 Java里的协程 除此之外，还有不少文章在对比线程和协程的区别： 线程使用了隐式切换+抢占机制，而协程则是显式+协作式； （隐式指的是靠OS来中断并切换任务，显式则表明交给程序员来做，这就意味着协程需要开发人员更多的精力来设计和开发。而且，资源非抢占会更容易导致 饥饿任务。） 线程可以实现多核并行，而协程则并非真正的并行。 其实仔细想想，前面提到的大牛们的解释，协程不过也就是提供给编程人员来自己调度执行流程的一种接口，在我看来，这与之前的goto语句比起来并没有太多差别！（其实不仅要解决执行流跳转问题，还需要保持跳转时的数据栈） Nodejs中大量的异步函数都集中在IO上，而在IO阻塞时，将计算机资源释放，将控制权转让给其它可执行语句非常好理解。而对于非io密集型呢？是否也需要进行调度？我们举一个非IO的阻塞例子：一个线程由于锁，或信号量而阻塞。其实，只要是存在协作的任务，就需要调度，不管是何种类型的。以这种视角来理解协程，就容易接受很多！ 既然线程才是真正意义上可以利用多核来并行的，那么协程这样的东西的价值是什么？问题在于，如果我们的场景里存在大量的并发逻辑，且这些逻辑可能会经常的自阻塞，那么就会造成os级别的线程上下文切换，即便是在创建的线程总数没有造成资源瓶颈的情况下也会造成性能折损。在这种场景下想要获得性能优化，协程就会发挥作用。如果我们将阻塞任务调度交给开发人员来管理，由于他比OS更了解场景的特殊性，那理应做出更佳的调度结果，相对的，我们的os的调度规则是面向通用场景的，无法照顾所有特殊的情况。 我们来看下面两个协程库： node-fibers quasar node-fibers听名字就知道该库是nodejs系的，node一直是我比较关注的一个生态环境，所以我们就从它开始吧。 从官方例子来看，node-fibers很好的解决node的回调地狱问题，不过这在Promise和Generator已主流的今天，似乎不再有价值了（纯属我个人观点）。 quasar前面说过协程和线程的差别，那么我们想想，在java中常见的处理并发业务时的做法应该是多线程吧，为了避免频繁的创建线程，一般还会配合一种线程池使用。 但可以创建的线程数量是受操作系统和硬件限制的，而且超过了最佳值后性能还会受到影响。那如何可以打破这个瓶颈呢？ quasar带来了轻量级线程概念，其实也就是协程的意思。它提供了M:N(M&gt;N)模型来实现M个协程映射到N个系统级别线程，这样就可以让java拥有类似nodejs那样的性能。可以通过阅读这篇博文来了解一下quasar的实现细节。 其中，官方给出了一条法则： Code that runs in short bursts and blocks often (e.g. code that receives and responds to messages) is better suited to run in a fiber, while code performs a lot of computations and blocks infrequently better belongs in a plain thread. 意思是： fiber适合那些阻塞频繁而间歇性执行任务的场景； thread适合那些大量计算性任务而偶尔阻塞的场景。 这和上面的分析也是吻合的，只有阻塞频繁的业务才应该交给应用自己来处理。 quasar默认借助java提供的ForkJoinPool来做调度器的，使用java agent来对相关代码进行字节码增强，内部使用特定的异常机制来触发资源让步，并使用应用级别的栈容器来管理fiber的内部状态和代码指针的。 面对web领域，我们可以直接使用comsat提供的高级框架层来写我们的逻辑，貌似很容易上手。它提供了actor model的编程模型，就是针对高性能应用场景的，有兴趣的朋友可以耍耍。 fiber概念其实不难，用于解决异步执行中的回调模式带来的各种问题（不易理解，不易阅读等），各种语言天生或第三方扩展出来fiber，也叫纤程，其实就是协程的意思。 神作偶然间看到一篇译文，感觉简直屌爆了！感谢那么牛逼的原作者，也感激同样牛逼的译者！ 文章中有一段话很深刻： 实现了一个虚拟机，将所有的代码转成状态机，然后通过抛异常来保存所有函数调用栈的状态。这样做意味着每一个函数都被转为一个非常大的 switch 语句，每一个表达式（语句）都对应成一个单独的 case 语句，这样做了之后，我就能够在代码的任意处跳转。 这似乎就是协程的秘密吧（只能算是一种实现方式~）。 这是神作的姊妹篇，同样强烈推荐！！！！","tags":[{"name":"quasar","slug":"quasar","permalink":"https://blog.kazaff.me/tags/quasar/"},{"name":"fiber","slug":"fiber","permalink":"https://blog.kazaff.me/tags/fiber/"},{"name":"线程","slug":"线程","permalink":"https://blog.kazaff.me/tags/线程/"},{"name":"协程","slug":"协程","permalink":"https://blog.kazaff.me/tags/协程/"}]},{"title":"Centos安装nginx+php-Fpm","date":"2016-05-21T09:37:00.000Z","path":"2016/05/21/centos下安装nginx+php-fpm/","text":"好久没装过PHP环境了，好久没有手动配置LNMP环境了，今天就让我头疼了一把！ 不过随着时间的推移，yum的源里越来越多的库可以直接使用了，现在自己在配置nginx和php环境就不再需要源码编译，也不再需要往yum中添加啥源了，直接就可以通过下面的命令完成安装：1yum install -y nginx php php-fpm 若系统之前yum安装过php，可以先卸载了： 1yum remove httpd* php* 安装完毕后，需要稍微修改一下配置文件来完成最后的工作，php-fpm需要修改一下权限： 1vi /etc/php-fpm.d/www.conf 将Unix user/group of processes改成你os对应的设置，例如： 12user = wwwgroup = www 然后需要开启nginx对应的php配置项： 1vi /etc/nginx/conf.d/default.conf 开启下面这部分配置： 123456location ~ \\.php$ &#123; include /etc/nginx/fastcgi_params; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; &#125; 一切就绪，就可以分别开启对应服务了： 12/etc/init.d/php-fpm restart/etc/init.d/nginx restart nginx File not found 错误这个时候如果你访问本地的nginx服务，如果看到了”File not found”错误提醒，原因多半是：php-fpm进程找不到SCRIPT_FILENAME配置的要执行的.php文件。 由于默认nginx将root参数放在了location内部，所以你得注意一下对应设置的文件目录是否正确，或者推荐你将root参数从location中移到server中，这样所有的子location将使用统一的web根目录。 此外，为了避免一些php cms系统的默认行为，你还是最好将index参数里增加index.php，来适配系统的默认首页匹配规范，避免不必要的麻烦。","tags":[{"name":"php-fpm","slug":"php-fpm","permalink":"https://blog.kazaff.me/tags/php-fpm/"},{"name":"nginx","slug":"nginx","permalink":"https://blog.kazaff.me/tags/nginx/"},{"name":"centos","slug":"centos","permalink":"https://blog.kazaff.me/tags/centos/"}]},{"title":"Vuejs使用中的两个小坑","date":"2016-05-20T09:37:00.000Z","path":"2016/05/20/vuejs使用中的两个小坑/","text":"我承认，这篇文章题目起的很不将就，不过相信我，已经尽力了。 最近一直在为项目写demo，算是帮同事提前采坑吧，不过也告一段落了。最后来记录两个小细节~ VueStrap和bootstrap的冲突测试使用的版本是： vue-strap 1.0.7 bootstrap 3.3.6 按说vue-strap只是封装了一下bootstrap，可实际使用时，竟然发现VueStrap.radioGroup和bootstrap.[min].js文件有冲突，导致前者失效。 由于我的场景是想使用bootstrap的tooltip功能，所以我的解决方案是：不要加载bootstrap.[min].js，而是直接加载其独立的./node_modules/bootstrap/js/tooltip.js即可快速的越过该冲突。 组件的ready回调中$broadcast事件给子组件无效这个问题要比第一个更有代表意义。vue组件化后，我们就有了很多父子关系的组件，在此场景下，如果你在父组件的ready回调中试图向其子组件广播事件，你可能会发现一切什么都未发生，原因也很简单，因为此时可能子组件还未渲染，更谈不上绑定事件了！ 解决方案也很简单，就是使用vue提供的$nextTick方法，将广播事件的逻辑放在下一个时钟触发，即可以保证子组件响应指定事件。 预告好了，vuejs相关的主题就暂时告一段落，接下来我可能会写一些wordpress二次开发的内容，因为项目要用~~","tags":[{"name":"vue.js","slug":"vue-js","permalink":"https://blog.kazaff.me/tags/vue-js/"},{"name":"VueStrap","slug":"VueStrap","permalink":"https://blog.kazaff.me/tags/VueStrap/"},{"name":"bootstrap","slug":"bootstrap","permalink":"https://blog.kazaff.me/tags/bootstrap/"},{"name":"生命周期hook","slug":"生命周期hook","permalink":"https://blog.kazaff.me/tags/生命周期hook/"},{"name":"$nextTick","slug":"nextTick","permalink":"https://blog.kazaff.me/tags/nextTick/"}]},{"title":"Vue组件通信和一个灵异事件","date":"2016-05-10T09:37:00.000Z","path":"2016/05/10/vue组件通信和一个灵异事件/","text":"这次来细聊一下vue的组件间通信，这部分内容官方有花了不少的篇幅介绍，但却没有一个清晰的例子供参考，真是不知道这算不算偷懒。不过还好热心人不少，这里有一个配套的例子很屌。我个人推荐 vuex来管理通信，不过这也意味着你要引入更多的依赖。从资料中可以看到，目前只用vue来实现兄弟组件间的通信，还是很麻烦的。不过，一定要搞清楚的是，兄弟间通信的需求场景是否合理，确保你确实需要兄弟组件间通信的前提下，我们继续往下聊。 这里要提到一个理论概念，之前在翻译react相关的文章中看到过：smart component 和 dummy component。简单的解释就是说，你的组件到底是否需要 保存 外部的状态，注意这里是指的保存，而不是感知。如果一个组件只是需要感知一个外部状态的话，起始使用props来做是非常合适的。 不过，如果你的组件确实需要独自保存一份来自外部的数据状态，那它就不再是一个dummy component，至少它的可复用性要大打折扣。所以除非你有足够的理由来这么做，不然还是尽可能保持组件的“愚蠢”吧！ 关于组件间通信的内容，上面给的第二个链接已经讲的很透彻了，我不准备在重复了。本篇我们主要来讨论一下新手容易碰到的灵异事件（我就是新手）。 灵异事件之一：子组件无法捕获事件我在项目中并没有引入vuex，所以我使用的是原始的父组件代理事件的方式来实现兄弟组件间通信的功能。不过在测试时发现子组件死活无法接受到父组件转发的事件，真是日了狗了！ 通过debug，我发现父组件在$broadcast()内部，并没有检查到目标事件监听的子组件，我才恍然大悟，确实页面上连该子组件的dom都不曾出现。 经过排查，原来是因为：vue不能很好的识别单独html元素标签，如下： 1234&lt;div id=\"toolbar\" class=\"container\"&gt; &lt;a-bar @event-route=\"eventRoute\" /&gt; &lt;b-detail @event-route=\"eventRoute\" /&gt;&lt;/div&gt; 改成： 1234&lt;div id=\"toolbar\" class=\"container\"&gt; &lt;a-bar @event-route=\"eventRoute\"&gt;&lt;/a-bar&gt; &lt;b-detail @event-route=\"eventRoute\"&gt;&lt;/b-bar&gt;&lt;/div&gt; 一切就正常了，真是见鬼了，谁可以来解释一下为何如此奇葩？！ 灵异事件之二：vue-strap的按钮组控件其实严格意义上这个并非vue组件间通信的问题，但却让我足足摆弄了3个小时。直到现在，我也只能归结于是自己的乱用导致的这个问题。 问题描述： 当首次切换选中按钮时，会触发两次change或click事件，但再次切换就一切正常了！ 前提是，你像我一样任性的如下使用： 12345&lt;radio-group :value.sync=\"radioValue\" type=\"primary\" :change=\"log()\"&gt; &lt;radio value=\"left\"&gt;Left&lt;/radio&gt; &lt;radio value=\"middle\" checked&gt;Middle&lt;/radio&gt; &lt;radio value=\"right\"&gt;Right&lt;/radio&gt;&lt;/radio-group&gt; vue-strap对开发者来说可以理解为一个黑盒子，而官方例子又非常的不具体，导致我们很容易用错。这里我可能就犯了一个思维错误，至少vue-strap的哲学不推荐我这么干（我猜的）。 到底哪里有问题呢？我想当然的根据使用jquery的思维来使用radio-group组件，导致我使用onchange事件来处理变更事件。 但注意到:value.sync的绑定方式，我们这里应该使用双向绑定的思维来实现这个需求： 12345ready: function()&#123; this.$watch('radioValue', function(newVal, oldVal)&#123; this.$dispatch(\"event-route\", &#123;eventName:\"change\", data: newVal&#125;)&#125;); &#125;);&#125; 这里我们在对应组件的生命周期方法中绑定了$watch回调，这样每当radioValue变更时就会捕捉到，我们并没有直接在radio-group组件上来做这件事儿，一切灵异事件都不见了（包括初始化问题）。之所以会想到这么做，是因为在排查bug时，我们不应该对黑盒子做任何假设，一切逻辑都尽可能由我们自己来完成，以最大程度的排除干扰。 感悟：只用使用匹配的思维来使用一个组件或库，才能真正的提升效率，否则可能事倍功半。","tags":[{"name":"vue","slug":"vue","permalink":"https://blog.kazaff.me/tags/vue/"},{"name":"组件间通信","slug":"组件间通信","permalink":"https://blog.kazaff.me/tags/组件间通信/"},{"name":"vue-strap","slug":"vue-strap","permalink":"https://blog.kazaff.me/tags/vue-strap/"}]},{"title":"只谈服务切分问题","date":"2016-05-02T09:37:00.000Z","path":"2016/05/02/只谈服务切分问题/","text":"一直以来都对“架构”非常感兴趣，早在大学期间，逃课成瘾的我连css都不会但就已经对架构师产生了浓厚的兴趣！ 阴差阳错的成了程序员后，又稀里糊涂的做了这么久。工作重心一直围绕着web方向：网站，系统。随着2014年底开始，“微服务”这三个字就好像病毒广告一样烂大街了，丝毫不输于它的前辈“大数据”。 之所以要从“微服务”这个广告语开始谈起，除了因为我自身性格的缘故（总是不自觉的对主流事物感到莫名的反感）外，主要是因为这和我们今天谈的主题非常相关。从敲下第一行代码开始，我相信每个程序员都在不停的思考一个问题：如何写出更好的代码。这里的“好”，具体应该至少包含： 能解决一个具体的问题（本质） 代码排版格式规整（强迫症） 可阅读性强（尊重他人） 维护成本低（修改成本低，让自己活的更滋润） bug少（逻辑思维能力） 抽象程度尽可能高（适配更多的场景，对外提供的接口灵活度高） 文档齐全（大师级别的最低要求） 以上几点，从前到后是有优先级的，希望你可以用心体会。 上面提到的这几点常用于某个具体的业务代码，或者函数方法，或者某个能解决通用问题的类库。其实对于“架构”来说，也同样适用，不过可能需要考虑的因素更宽泛。 说这些和我们今天的主题有关系么？是的。真的？嗯！那到底有什么关系？说白了就是服务边界的问题，这和“职责单一”的概念有些相似，但又不完全相同。 相同之处在于让开发者时刻敏感于当前做的上下文规模问题，不同点在于服务切分时存在更多的 “灰色区域”。 正是这些灰色区域的存在，才让大师成为大师，才让新手如此纠结。不过，这正是架构设计的乐趣和挑战！ 我并没有许多成功的架构设计经验（是真的，如果你对我下面说的产生了质疑，那这个信息无疑是成为你反驳我的有理证据），但在实际工作中却真真切切的感过那种纠结和尴尬！曾经以为已经考虑的完美无缺，但文档才写了一半就傻眼了，现实业务总是像碰瓷大娘一样拦住去路~ 那到底哪些细节需要在设计服务时需要我们警惕呢？我尽自己最大努力总结了一下： 能解决一个具体且尽可能完整的业务问题 尽力避免分布式事务 从服务使用者的角度去规划服务的颗粒度 考虑服务监控、容错、负载均衡、服务发现和治理方案 尽量避免采用中心化的服务编排设计，交给服务调用方来做 最后一条应该最具争议性，毕竟这多少有些违背了 开放关闭 原则，至少我们很可能误将原本应该隐藏起来的细节不必要的暴露给了用户，而这个错误在某些时候是致命的（造成了数据的不一致）。 不过，如果拿捏的好呢？假设我们刚好抓住了蛇的七寸，那么我们即将设计出一套实现成本低且实用性强的服务方案。 来举一个实际的例子： 假设我们有一个获取学生列表的api，在返回的数据集中是否应该包含每个学生所属学校的内容呢？还是考虑设计一个专门用来获取指定学生学校的api？ 根据数据库设计范式，为了避免不必要的数据冗余，我们可能会有两张表：学生表和学校表。 而界面上为了用户的体验，学生列表页面当然应该显示出对应的所属学校信息。貌似矛盾就这样理所应当的产生了。 强大的sql给我们提供了第一个解决方案：多表join，甚至让你可以跨库做“join”。似乎让我们没有不用的理由？！ 不过不要被迷惑，要记住，不论何时，join都不应该是首选。理由是你 不应该将业务逻辑写在sql中（没有例外） ，尽管从我们这里的例子上来看join是完美的。但很多前辈都总结出了这种做法在web项目（互联网）的弊端，我们确实应该反思。 so，既然不推荐直接在sql层做数据拼接，那就上移到代码层，而在传统的分层架构中又面临一个问题：放在哪一层合适？service层还是control层？还是前端调用时（针对前后端分离项目）做数据关联？ 考虑到现在REST架构风格比较流行，让我们假设上述的例子是发生在rest api中，到底这个api应该帮用户处理完所有数据关系？还是交给用户自己来按需获取呢？ 在这个简单的例子中，我个人坚持后者，这种思想也和Facebook推的GraphQL在某种层面上不谋而合。这么做的好处是，既保证了最终用户的业务需求，又保证了api的可复用性，毕竟学生和学校是比较独立的两个概念域，到底如何来处理它们之间的关系，应该交给数据调用方来自行解决。 当然，这么做也并非毫无缺点的。暴露给调用方，意味着更多的风险： 调用方没有正确的使用（谁会喜欢去看枯燥的使用说明书？） 调用方恶意的使用（暴露出去的东西越多，被攻击的面就越大！） 当提供方api升级时调用方的成本无法估计（怪我喽~） 最后一点其实尤为重要，这也是rest官方资料中为何如此反复强调“服务版本号”概念的初衷。比较保险的方法是增加一层（计算机世界解决问题的银弹！！！）：gateway，不仅可以以较粗的颗粒度提供服务，而且可以做很多流控方面的事情。 由于我们的例子是如此的简单，以至于根本不会在数据库表设计上产生什么分歧，但实际项目中，往往也会让我们纠结如何设计数据库，如何将众多的表归类到各自合适的库中。若一开始对领域模型把握的不是很精准，后期可能会导致数据一致性的噩梦：分布式事务。 尽管很多业界巨头都开源出了解决方案或论文，社区也提供了很多可选的解决方案，但相信我，你依然不应该随意的引入这个问题。如何避免分布式事务呢？如果业务并非要求强一致性，那选择最终一致性的概念会让你的生活变得美好很多。否则，最好还是将事务性强表放在同一个物理位置。 当然，现实是残酷的，有时候你还是不得不直面困难，此时你才需要大胆的尝试市面上提供的分布式事务的解决方案，相信我，你是可以搞定的（不然就等着下岗吧！）。 对于业务相对比较简洁的项目，不会需要ESB这样的中心化服务编排的中间件。主动权交给调用方，毕竟鞋子合不合脚，只有自己知道。如果这种设计造成了性能瓶颈，那异步化将会成为你的主战场。 最后，还要提到的是，服务的切分，绝壁不是一锤子买卖。随着需求的变迁，服务会分分合合，所以这个时候，一个良好的服务分层会是一个开启美好明天的基础： 基础服务：不 依赖其它服务的服务 复合服务：依赖其它服务的服务 听起来很简单不是么？但相信我，这是最纠结的地方。不过值得分享的是：不要陷入选择恐惧症中。要知道，关键时刻迈出一步，比犹豫不决要更有意义。 感悟：软件架构师之所以称之为架构师而非高级工程师，本质上讲是因为其思考和需要解决的问题更加宽泛，横跨软件开发的整个生命周期，而且“架构演化论”也指明，考核架构师优劣的一个重要标准是：应对需求变化的能力。","tags":[{"name":"rest","slug":"rest","permalink":"https://blog.kazaff.me/tags/rest/"},{"name":"soa","slug":"soa","permalink":"https://blog.kazaff.me/tags/soa/"},{"name":"服务","slug":"服务","permalink":"https://blog.kazaff.me/tags/服务/"}]},{"title":"gmailWatcher之node","date":"2016-05-01T09:37:00.000Z","path":"2016/05/01/gmailWatcher之node/","text":"Email是一个可能比很多人岁数都大的“上古神器”，尽管和它同期出生的小伙伴很多都已经寿终正寝，但它却依然坚挺，可见其代表了多大的用户刚需！ 最近热映的《北京遇上西雅图2》讲的就是邮件的爱情故事，正巧又碰上项目新需求包含mail的收发要求，就借此良机好好学习一下这个陪伴我们很久的大伙伴。 理论知识如果你是在不知道什么是Email，那我只能说：看看这篇生动的微小说吧。 好，我们来先聊聊几个关键词： 邮件协议：LDAP、SMTP、POP、IMAP 邮件服务器：商用厂商（gmail.com、163.com）、自建邮件服务 邮件客户端：原生客户端（outlook）、网页邮箱系统、浏览器插件、自建邮件客户端 提到“自建邮件服务器”，勾起了我痛苦的回忆，之前在前任公司为了在centos上手动搭建邮件服务，让我反复安装操作系统N次（不要问我为什么）。所以这篇文章并不是讲如何搭建邮件服务器的，我们会使用gmail作为我们的邮件服务器。 由于我们的主题是邮件获取，所以POP和IMAP协议是我们主要需要了解的内容，这里我使用的是 IMAP，因为它新。 需求分析我们要做的是这么一件事儿： 监控我们指定的一个gmail帐号，一旦该邮箱收到邮件，就解析邮件内容（包括附件）,并将该邮件标识为已读（确保不会重复处理该邮件）。 目标明确，可以开始写代码了！等等，其实如此常见的需求一定已经有大量的实现方案，不管是php，java，还是python，或是今天我们要使用的nodejs，都已经有现成的库或框架来供我们使用了。 不过我们上面的理论知识可以辅助我们知道，要实现这么一个需求，我们的思路是： 使用对应协议和目标邮箱建立链接 获取满足条件的邮件数据 根据协议解析数据，获取我们需要的属性（发件人，标题，内容以及附件） 更新该邮件的状态为已读 持续监听“新邮件”事件，重复第2步 这里面我们忽略了一些细节复杂问题，如： 邮箱安全认证问题 邮箱链接中断处理 邮件解析失败 大邮件解析资源损耗（主要是大附件导致的内存损耗） 海量邮件的并发处理模型 其它 现有库相信现有的解决方案可以帮我们完成上述的各个方面，那么nodejs为我们提供了怎样的现成代码呢？经过一番搜索，锁定了这个库：mail-listener2(注意这里给的链接是我已经fork后的，并加了一些中文注释和小修改)。 我已经在其中的关键部分添加了近乎啰嗦的注释，所以这里就不重复制造垃圾了。这里需要特别说明一下的是，该库使用了aysnc并发库，依赖其提供的并发模型让实现代码简化了不少，推荐大家在自己的项目中也大胆使用。 还有就是，如果你针对附件解析开启了流处理，那你需要自己来处理附件流，该库的并没有在开启流处理后帮你把附件保存在指定位置，不过扩展起来也很简单，只需要在对应的位置添加： 12//替换该行fs.writeFile(self.attachmentOptions.directory + attachment.generatedFileName, attachment.content, function(err)attachment.stream.pipe(fs.createWriteStream(self.attachmentOptions.directory + attachment.generatedFileName+attachment.generatedFileName)); 除此之外，该库并没有保留”node-imap”中的关于超时的相关参数，这会导致网速不太好的环境无法使用，建议后期加上下面俩参数： connTimeout 创建连接的超时时间，node-imap默认是10000毫秒 authTimeout 连接创建完毕后认证的超时时间，node-imap默认是5000毫秒 这个库其实只是简单的封装了一下node-imap和mailparser。 这俩库才是真正完成我们前面说的步骤，其中”node-imap”学习起来成本较高，主要是你得先了解imap协议本身。在协议之上该作者增加了nodejs的一些语言特性，靠事件来组织业务，所以需要静下来好好看看。 异常处理方案目前为止，基本上除了异常处理，其他工作都被库做完了，娃哈哈，生活就是如此美好！那，我们应该如何处理异常才比较合适呢？ 这还是要看业务需求，如果你开发的是一个用户参与度很高的客户端，那你只需要在异常发生后上报给用户即可，例如弹窗。 但如果是一个后台常驻服务，就不仅要考虑上报异常的方法，还需要考虑如何保证碰到无法恢复的异常后应用线程会自动重启，听起来很高大上，但其实PM2已经帮你实现了。除此之外，国内BAT好像也都有自己的nodejs运维方案供我们选择。 结尾相信已经说的够详细了，剩下的就靠大家留言了！","tags":[{"name":"gmail","slug":"gmail","permalink":"https://blog.kazaff.me/tags/gmail/"},{"name":"imap","slug":"imap","permalink":"https://blog.kazaff.me/tags/imap/"},{"name":"async","slug":"async","permalink":"https://blog.kazaff.me/tags/async/"},{"name":"mailparser","slug":"mailparser","permalink":"https://blog.kazaff.me/tags/mailparser/"}]},{"title":"Ztree如何优雅自定义ajax的header参数","date":"2016-04-30T09:37:00.000Z","path":"2016/04/30/ztree如何优雅自定义ajax的header参数/","text":"很久很久以前，我就把ztree夸成一朵花了，现在依然对它点赞！ 不过之前在ng中使用它时就碰到了一个小麻烦，直至昨天同事问我，我再次翻阅ztree的文档依然没有提供自定义ajax的header的功能，这让基于REST风格的api在使用起来很不方便。 当然，直接修改ztree的源码也十分简单，但这并不是一个追求完美的coder可以接受的！那我们来看看到底该怎么做来最优雅的解决这个问题！其实，这就要从jquery上下点功夫（ztree依赖jquery1.4+），我们在api的手册上早已经看到这么一个方法，所以我们只需要简单的在你需要使用ztree的页面，增加下面这段代码即可： 123$(document).ajaxSend(function(event, jqxhr, settings) &#123; jqxhr.setRequestHeader(\"Auth\",\"kazaff\");&#125;); 需要提醒的是，这样会全局修改所有基于jq的ajax请求，如果你页面上有其它依赖它的代码或第三方组件，那么可能会相互影响，不过，你可以按照官方的一个例子来做处理： 12345$(document).ajaxSend(function(event, jqxhr, settings) &#123; if ( settings.url == \"ajax/test.html\" ) &#123; $( \".log\" ).text( \"Triggered ajaxSend handler.\" ); &#125;&#125;); 好了，希望对你有帮助！","tags":[{"name":"ajax","slug":"ajax","permalink":"https://blog.kazaff.me/tags/ajax/"},{"name":"ztree","slug":"ztree","permalink":"https://blog.kazaff.me/tags/ztree/"},{"name":"header","slug":"header","permalink":"https://blog.kazaff.me/tags/header/"},{"name":"ajaxSend","slug":"ajaxSend","permalink":"https://blog.kazaff.me/tags/ajaxSend/"}]},{"title":"Vue-Validator初体验","date":"2016-04-20T09:37:00.000Z","path":"2016/04/20/Vue-validator初体验/","text":"之前一直都是做SPA项目，不过最近由于种种原因，需要返璞归真来基于jquery做项目！但无奈之前一直使用像ng或react这样的大杀器在编程，思维上已经有了一定的模式，突然回到jquery时代，感觉不会写代码了！真是感叹前端发展的迅猛啊~ 其实react已算很轻了，但jsx语法我感觉确实是一个坎，虽然在我看来是精华也是趋势，但在业务量面前我实在不忍心再给团队成员加砝码了！ 之所以考虑让项目基于jquery，是因为目前的前端开发人员和我所在的城市的前端开发人员整体程度比较低（只熟悉jquery），所以这么抉择，虽然保守，但让项目可以更快的完成，也有利于扩员，算是一种权衡！ 但jquery确实太老了，只用jquery显然不能满足项目的需求，尤其是我这种生活在美好的数据绑定环境下的人，确实无法接受老旧的dom操作！ 我在github上搜索：”jquery data binding”，推荐给我的一款基于jquery的数据绑定库是真尼玛难用，所以我就不贴链接了，怕被打脸！总之，我们的目的是尽量找学习成本低，侵入性低的库，哪怕功能比较弱一些，哪怕性能比较差一些（我们的项目是内部系统，so~）！ 结果，偶然间哥注意到了Vue.js，完美！！！学习成本非常的低，而且十分的优雅，社区也很繁荣，简直了！那么接下来就是掌握一下它的使用方法，由于它出生在和react差不多一个时代，所以思路和用法都类似，看看官方文档就能上手做项目了！（希望没有大坑在等我~） 单用vue，肯定也还是不够的，既然我们引入了它，自然要发挥它最大功效，回到今天的主题：表单验证！（别嫌我啰嗦，我就是话多！） 之前用过不少表单验证插件，但实话说，看完vue-validator的官方文档，真的让我非常的震撼，基本上属于扩展性非常强的插件了，作者考虑到了绝大多数适配的场景，堪称完美！ 不过，在实际使用的时候，还是有一些文档上没有写太详细的小坑，我的任务就是把我使用的时候碰到的问题记录下来，我们开始吧！ 安装由于我没有使用任何加载框架，只是简单的在浏览器上使用vue和vue-validator，所以我们只需要直接加载它们即可，不需要做额外的工作，这里我使用的是bootcdn提供的cdn，代码如下： 12&lt;script src=\"//cdn.bootcss.com/vue/1.0.17/vue.js\"&gt;&lt;/script&gt;&lt;script src=\"//cdn.bootcss.com/vue-validator/2.0.0-alpha.22/vue-validator.js\"&gt;&lt;/script&gt; 需要注意的是加载顺序和版本，我测试加载vue-validator的1.X版本的话并不会自动装载到vue中，不清楚是版本的事儿还是其它问题，总之，使用最新的版本即可。 绑定model最新的vue-validator下，在使用时已经不支持下面的写法： 1234&lt;input type=\"text\" v-model=\"test\" v-validate=\"&#123;required:true&#125;\"/&gt; 会提示： Uncaught TypeError: Cannot read property ‘replace’ of undefined 所以你只能老老实实的写成： 123&lt;input type=\"text\" v-validate:test=\"&#123;required:true&#125;\"/&gt; 结构复杂的model如何绑定我的例子中model的结构比较恶心，是这样的： 12345678910111213141516var blockBox = new Vue(&#123; el: \"#blockBox\", data: &#123; table: &#123; rowA: &#123; columnA: \"a-a\", columnB: 'a-b', columnC: 'a-c' &#125;, rowB: &#123; columnA: 'b-a', columnB: 'b-b', columnC: 'b-c' &#125; &#125; &#125;); 那我总不能写成这样吧： 123&lt;input type=\"text\" v-validate:table.rowA.columnA=\"&#123;required:true&#125;\"/&gt; 丑不说，何况这么写也是错误的！ 怎么办？按照官方的说法应该这么写： 12345678&lt;input class=\"form-control\" type=\"text\" v-model=\"table.rowA.columnA\" v-validate:message-aa=\"&#123;required:true&#125;\"/&gt;&lt;div class=\"errors\"&gt; &lt;p v-if=\"$blockBoxValidator.messageAa.required\" class=\"lang\" data-key=\"blockBoxAA\" /&gt;&lt;/div&gt; 完美，注意我在v-validate:message-aa中的写法！千万不能写成驼峰式的，如v-validate:messageAA，这么做会报错哟~原因我才和vue解析有关，这应该牵扯到了html属性的格式规范问题！总之你避免这么写就ok了！ 和jquery配合我不想说我下面的这个场景是否合理，但为了快糙猛的把项目demo搞出来，我遇到了下面的问题。 场景是我们的项目需要多语言支持，界面上能看到的元素（不包括用户输入）都应该支持切换语言，包括表单错误提醒！目前我所使用的切换语言的方法很简单： 123456789&lt;p v-if=\"$blockBoxValidator.message_aa.required\" class=\"lang\" data-key=\"blockBoxAA\" /&gt;&lt;script type=\"text/javascript\"&gt;$(function()&#123; //多语言处理 $(\".lang\").each(function(node)&#123; $(this).html($langCFG[$(this).data(\"key\")]); &#125;);&#125;);&lt;/script&gt; 利用jquery的domready事件来第一时间替换需要显示的文字！这就有个问题出现了，若你在domready事件触发前就初始化了vue和vue-validator，根据表单验证逻辑很可能导致v-if指令直接干掉表示错误信息的那个p标签，所以自然也不会正确的执行多语言替换的逻辑。 在不考虑花时间研究vue-validator生命周期的前提下，我们如何更快的绕过这个问题呢？第一感觉是，将初始化vue和vue-validator的工作也放在domready中（但注意一定要放在多语言处理的代码之后）。不过你还会遇到另外一个问题，这个时候你在浏览器调试终端中按照vue官方的例子打印你创建的vm时，会报错提示变量定义不存在~具体原因不清楚，不过我们依然可以搞定这个问题，最终的代码如下： 1234567891011121314151617181920212223242526272829&lt;p v-if=\"$blockBoxValidator.message_aa.required\" class=\"lang\" data-key=\"blockBoxAA\" /&gt;&lt;script type=\"text/javascript\"&gt;var blockBox = null; //变量声明，这样能保证在调试终端中可以直接使用'blockBox.$log()'$(function()&#123; //多语言处理 $(\".lang\").each(function(node)&#123; $(this).html($langCFG[$(this).data(\"key\")]); &#125;); //放在多语言处理下面，且在domready事件内 blockBox = new Vue(&#123; el: \"#blockBox\", data: &#123; table: &#123; rowA: &#123; columnA: \"a-a\", columnB: 'a-b', columnC: 'a-c' &#125;, rowB: &#123; columnA: 'b-a', columnB: 'b-b', columnC: 'b-c' &#125; &#125; &#125; );&#125;);&lt;/script&gt; 理论上，你只要把你想用js做的业务逻辑代码都放在domready事件的回调方法中的vue初始化之后，就可以像和单独使用vue写代码一样的流程来进行开发了！ 页面初次加载显示问题这应该不是vue-validator的问题，这应该属于vue的问题域。之前angular推荐的解决方案是提供了一个指令来监听模版解析完毕的事件。我搜了一圈没找到官方或第三方提供的类似功能的实现，不过自己来做也不是什么难事儿： 1234567891011&lt;body id=\"body\" style=\"display:none\"&gt;&#123;&#123;test&#125;&#125;&lt;/body&gt;&lt;script type=\"text/javascript\"&gt;new Vue(&#123; el:\"#body\", ready: function()&#123; $(\"#body\").show(); &#125;&#125;);&lt;/script&gt; 其实思路就是借助vue组件的生命周期hook来及时的修改css样式，你也可以添加loading动画等高级特效，我懒我就不弄了~ $resetValidation()重置验证的场景是这样的：当你的表单被用户合法修改保存后，用户并没有跳转到其他页面，仍然在当前页面下，由于表单内容已经和最原始的版本不一样，你需要resetValidation一下从而使vue-validator的各个状态匹配业务场景（包括但不限于dirty,modified等状态）。不过这里有两个小插曲： 我直接使用的bootcdn提供的版本导致js报错说$resetValidation()未定义，切换成官方的最新版本即可解决； 应该是跟vue的vm更新机制有关: 1234var that = this;setTimeout(function()&#123; that.$resetValidation();&#125;,0); 否则呢，你无法看到期望的结果~ lazy的正确使用方法有些表单的初始化数据需要异步获取，太正常不过了，对吧？vue-validator提供了lazy模式，刚好匹配这种场景。所以，你只需要在validator标签上添加lazy属性即可，官方例子简单明了，似乎没有什么要解释的！不过，问题来了，官方的例子代码是基于vue的动态组件写的，动态组件才有activate事件，而如果你像我一样是用在普通组件下的，那么自然不会触发activate事件，那回调逻辑自然不会被执行，this.$activateValidator()不会执行的话，看看vue-validator会提示什么： 1[vue-validator] validator element directive need to specify &apos;name&apos; param attribute: (e.g. &lt;validator name=&quot;validator1&quot;&gt;...&lt;/validator&gt;) 看起来提示的非常不准确，让我排查了好久，甚至去联系作者来查看这个问题！其实你只要确保在正确的时机调用$activateValidator()方法就可以了，那什么叫正确的时机： 普通组件的ready事件回调中 动态组件的activate事件回调中 否则你一定会看到上面的那个提醒，特别提醒，下面的写法依然会看到提醒： 12345ready: function()&#123; setTimeout(function()&#123; this.$activateValidator(); &#125;.bind(this), 3000);&#125;, 这应该是作者好心办的”错事儿”吧，他要求你只能在组件渲染前把该做的都做了（数据获取），不然你就是在自讨苦吃~当然，这种设计哲学并非没有道理，全看权衡和喜好！ 好了，先总结到这里，如果以后有啥新的结论，再说~","tags":[{"name":"vue.js","slug":"vue-js","permalink":"https://blog.kazaff.me/tags/vue-js/"},{"name":"validator","slug":"validator","permalink":"https://blog.kazaff.me/tags/validator/"},{"name":"jquery","slug":"jquery","permalink":"https://blog.kazaff.me/tags/jquery/"}]},{"title":"如何禁止Chrome自动跳转https","date":"2016-04-02T09:37:00.000Z","path":"2016/04/02/如何禁止chrome自动跳转https/","text":"首先我声明，我非常支持https。 之所以要写这篇文章，主要是因为我碰到了一个很尴尬的事儿！待我慢慢道来~ 之前给网上看到一篇文章，是教你如何简单的借助一个服务（我忘记名字了）为你的github pages项目来开通https支持，我的博客是hexo的，所以当然也是放在github pages上的！ 那个服务确实很给力，在我根据指南，配置好后（域名解析）几乎是一键开启https！美翻了！但是我发现可能小众问题吧，我的博客没过多久访问时就提示域名解析异常，再次到那个服务后台去配置，就发现提示域名所属权校验失败！ 本以为重新配置一下就可以了，结果又碰到了二级域名导致它提供的配置无法通过校验（首次没问题我也很纳闷！）~好了，说道这里，基本上这件事儿的背景也就差不多解释清楚了！ 既然如此，我自然会放弃使用那个服务，所以我把域名解析重新直接解析到github pages上！但是，问题来了，之前所有使用过https访问我域名的chrome再也无法使用http来访问我的网站了！ 一直提示网站出现安全异常，阿西吧！ 后来网上搜了一下，找到了解决方案，其实很简单，请在chrome的地址栏输入： chrome://net-internals/#hsts 在打开的页面中，Delete domain栏的输入框中输入：blog.kazaff.me（注意这里是二级域名），然后点击“delete”按钮，即可完成配置。 然后你可以在Query domain栏中搜索刚才输入的域名，点击“query”按钮后如果提示“Not found”，那么你现在就可以使用http来访问我的网站了！ 虽然，你可能从来没有访问过我的网站~~哇哈哈哈哈 PS：我看有同学留言说这个特性是chrome自作聪明，老实讲一开始我也是如此认为，何必非要记录一下domain和ip的关系呢？仅仅是缓存提速么？最近和同事讨论一个问题，关于中间人网络代理攻击的。面对这点，chrome hsts可以说是非常给力的一道保障，绝对不算是自作聪明！！！","tags":[{"name":"https","slug":"https","permalink":"https://blog.kazaff.me/tags/https/"},{"name":"chrome","slug":"chrome","permalink":"https://blog.kazaff.me/tags/chrome/"},{"name":"自动跳转","slug":"自动跳转","permalink":"https://blog.kazaff.me/tags/自动跳转/"}]},{"title":"Groovy下的field和property","date":"2016-04-01T09:37:00.000Z","path":"2016/04/01/groovy下的field和property/","text":"今天要聊的这个话题，是基于一个很有意思的jvm语言：groovy！有好奇心的童靴不妨去了解一下它的风骚~ 在java bean的定义中，一般都会为这个bean设置一些属性，然后按照javabean规范会创建getter和setter方法~（你是否也早已经烦透了这种样板代码？）groovy的哲学，就是在表达清晰的前提下让你尽可能少和样板代码打交道！而我们要讨论的，就是与java bean看起来极其相似的groovy bean！ 本文一下内容，翻译自groovy bean groovy bean 就是 java bean，但是提供了大量的简化语法！ 123456789101112class Customer &#123; // properties Integer id String name Date dob // sample code static void main(args) &#123; def customer = new Customer(id:1, name:\"Gromit\", dob:new Date()) println(\"Hello $&#123;customer.name&#125;\") &#125;&#125; 执行上面的代码后会看到输出 Hello Gromit 可以看到property似乎就好像是公开的类field。你甚至可以在构造函数里为这些property初始化。在groovy里，field和property的概念被合并了，它们不仅看起来一样，使用起来也一样。因此，上面的groovy代码和下面的java代码是等价的： 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.Date;public class Customer &#123; // properties private Integer id; private String name; private Date dob; public Integer getId() &#123; return this.id; &#125; public String getName() &#123; return this.name; &#125; public Date getDob() &#123; return this.dob; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public void setName(String name) &#123; this.name = name; &#125; public void setDob(Date dob) &#123; this.dob = dob; &#125; // sample code public static void main(String[] args) &#123; Customer customer = new Customer(); customer.setId(1); customer.setName(\"Gromit\"); customer.setDob(new Date()); System.out.println(\"Hello \" + customer.getName()); &#125;&#125; 我们来看看property和field的一些规则~ 当groovy被编译成字节码文件后，将遵守下面的规则： 如果声明时为name明确指定了一个操作限定符（public，private或protected）则将会创建一个field 若没有指定任何操作限定符，则该name将被创建为private field并且自动创建getter和setter方法（就是一个property） 如果property被定义为final，则创建的private field同样是final，且不会自动创建setter方法 你可以声明一个property，并且自己实现getter和setter方法（译者注：多半是在学习这个知识点的时候才会这么做吧~） 你可以同时定义相同name的property和field，此时property会默认使用同名的field（译者注：这是要疯了啊） 如果你想要一个private或protected的property，你就必须自己提供满足你操作限定需求的getter和setter方法 如果你在class内部操作property（例如this.foo或foo），groovy会直接操作对应的field，而不会调用对应的getter或setter方法 如果你试图操作一个不存在property，groovy会通过meta class来操作对应的property，这可能会在运行时失败（译者注：动态语言特性） 现在来看一个例子，创建一个包含只读property的类： 123456789101112class Foo &#123; // read only property //译者注：使用final，groovy不会为其创建setter方法 final String name = \"John\" // read only property with public getter and protected setter Integer amount protected void setAmount(Integer amount) &#123; this.amount = amount &#125; // dynamically typed property def cheese&#125; 注意：property声明是需要一个标识符，或者是明确的类型（如String），或者是无类型声明（def）。 为什么groovy不会为一个被定义为public的field自动创建getter和setter方法呢？（译者注：极好的问题！）如果我们在任何条件下都自动创建getter和setter方法，意味着groovy下你无法摆脱getter和setter，这在某些你就是不想要getter和setter方法的场景下就傻眼了！（译者注：此段为“随译”，就是随便翻译的意思~哈哈） Closures and listeners本段和主题关系不大，就不翻译了~","tags":[{"name":"field","slug":"field","permalink":"https://blog.kazaff.me/tags/field/"},{"name":"property","slug":"property","permalink":"https://blog.kazaff.me/tags/property/"},{"name":"groovy bean","slug":"groovy-bean","permalink":"https://blog.kazaff.me/tags/groovy-bean/"},{"name":"getter/setter","slug":"getter-setter","permalink":"https://blog.kazaff.me/tags/getter-setter/"}]},{"title":"SDKMAN安装的库位置给哪呢","date":"2016-03-31T09:37:00.000Z","path":"2016/03/31/SDKMAN安装的库位置给哪呢/","text":"废话不多说，直接教你做人： Mac下终端输入： echo $PATH 会看到类下面的输出： /Users/kazaff/.sdkman/candidates/xxxxxxxxx 知道了给哪了吧？bye~","tags":[{"name":"SDKMAN","slug":"SDKMAN","permalink":"https://blog.kazaff.me/tags/SDKMAN/"},{"name":"path","slug":"path","permalink":"https://blog.kazaff.me/tags/path/"}]},{"title":"前端多入口项目如何实现根据用户权限配置显示菜单","date":"2016-03-30T09:37:00.000Z","path":"2016/03/30/前端多入口项目如何实现根据用户权限配置显示菜单/","text":"目标是要为一个ERP项目做前端实现，该项目前后端通信完全走REST，不过由于项目时间和人力有限，前端开发人员对目前主流的MVVM框架并不了解，所以之前的单入口思路就无法复用了。 这次前端实现采用的是多入口设计，且不使用html iframe方案。基于jquery的dom操作和ajax来完成大多数开发任务，引入lodash作为工具库来处理复杂数据结构。 再来说一下项目的概况，和大多数后台管理系统一样，这个ERP依然提供为用户配置操作权限的功能，这就要求前端界面需要根据用户的实际操作权限来响应哪些操作链接可以显示在界面上，哪些需要隐藏起来。当然，这只是为了界面显示做的配置，实际后端服务依然会判断用户权限的，不过界面上根据用户权限来控制菜单或按钮的显示情况，也是必不可少的。 除此之外，考虑到这个ERP功能是按照模块划分的，每个模块的功能相对独立，映射到项目结构上，我希望每个模块的相关代码（html，js，css等）都应该存放在各自的模块文件夹下，为将来维护提供良好的基础。 万幸的是，目前项目只针对chrome高版本，意味着我们可以不鸟兼容问题！！！ 具体实现细节，推荐看项目的boot.js代码。","tags":[{"name":"权限","slug":"权限","permalink":"https://blog.kazaff.me/tags/权限/"},{"name":"jquery","slug":"jquery","permalink":"https://blog.kazaff.me/tags/jquery/"},{"name":"多入口","slug":"多入口","permalink":"https://blog.kazaff.me/tags/多入口/"},{"name":"菜单","slug":"菜单","permalink":"https://blog.kazaff.me/tags/菜单/"}]},{"title":"监听滚动条根据元素是否在屏幕上来触发指定逻辑","date":"2016-03-27T09:37:00.000Z","path":"2016/03/27/监听滚动条根据元素是否在屏幕上来触发指定逻辑/","text":"好久没写网站类型的项目，所以一直好奇一些网站右下角的导航是如何实现的，可以根据用户浏览的内容来动态匹配导航条。 根据项目需要，写了一个监听滚动条的模块，具体用法和实现细节可以去github看源码：传送门。 小总结： 组件化很锻炼思维：职责，容错，友好 常用的集合操作，使用lodash很爽 如果可以不理会浏览器兼容性，前端真的好幸福","tags":[{"name":"滚动条","slug":"滚动条","permalink":"https://blog.kazaff.me/tags/滚动条/"},{"name":"组件化","slug":"组件化","permalink":"https://blog.kazaff.me/tags/组件化/"},{"name":"屏幕位置","slug":"屏幕位置","permalink":"https://blog.kazaff.me/tags/屏幕位置/"}]},{"title":"忙里偷闲说说Atom","date":"2016-03-24T09:37:00.000Z","path":"2016/03/24/忙里偷闲说说atom/","text":"好久没有摆弄开发工具了，若不是换了新的环境，可能也不会折腾开发环境啊~自从切换到facebook技术栈下后，一直就在使用Atom，把mac下所有其他的编辑器都卸载了！可见是真爱！可是一直都没有特意的去研究atom，前几天装了一个推荐的皮肤：Seti-UI，瞬间感觉眼前一亮，不仅美观，而且实用！强烈推荐~ 然后就是推荐一个插件： Atom-beautify 这样你就可以不再被“别人”写的垃圾代码干扰了，一键“ctrl+alt+b”格式化代码排版，真刺激！ 还有一个比较实用的操作，就是多光标编辑，不需要记热键，只需要按住“ctrl”，双击鼠标即可！ 最后一个热键组合：ctrl+d，快速选择距离最近的相同标签！ 我这人记性差，再多的热键我就记不住了~~ 有兴趣的童靴可以去看这里","tags":[{"name":"插件","slug":"插件","permalink":"https://blog.kazaff.me/tags/插件/"},{"name":"atom","slug":"atom","permalink":"https://blog.kazaff.me/tags/atom/"},{"name":"代码排版","slug":"代码排版","permalink":"https://blog.kazaff.me/tags/代码排版/"},{"name":"皮肤","slug":"皮肤","permalink":"https://blog.kazaff.me/tags/皮肤/"},{"name":"快捷键","slug":"快捷键","permalink":"https://blog.kazaff.me/tags/快捷键/"}]},{"title":"window下mysql5.7无法启动","date":"2016-03-20T09:37:00.000Z","path":"2016/03/20/windows下mysql5.7无法启动/","text":"放了个周末，第一天上班就发现mysql无法启动了，阿西吧~一大早就先整了一个小时，真是不划算啊~之前都是使用集成开发环境，很少自己手动安装mysql，不过由于这次必须使用mysql5.7，而集成环境自带的mysql版本不够（或者直接提供的是MariaDB）,所以只能自己安装了~ 直接下载mysql5.7的window安装器，很简单就装好了，所以也没有特别在意，简单修改了一下my.ini就用了一个多礼拜~上周六关机的时候电源断早了，所以今天开机发现window开机提示非法关机了，导致我的工作区都丢了，郁闷！（平时我都是休眠系统的！） 启动到桌面后，发现mysql无法启动了！好灵异，更奇怪的是，我看了一下my.ini都自动给我恢复成my.default.ini了！草！mysql安装的时候我也没有把它加到服务中，这下好了，我tm无法启动它了！ 网上查了一下，win下，可以执行下面的命令（后面所有命令都建议在管理员权限下做）： mysqld --initialize 前提是，你得先有一个my.ini，并配置好basedir，basedir指向的文件夹也要有对应文件夹啊~ 这一步做完，mysql会初始化创建它要使用的data数据！（这不是废话么！） 然后将mysql加入到win的系统服务中： mysqld --install MySQL 或者先卸载服务: mysqld --remove 完事儿了你基本上就已经可以启动你的mysql服务了！然后你还不能高兴过头！因为，密码也丢了！！！ 网上很多资料讲的找回密码方案在mysql5.7下似乎都不管用了！我只能，在我的my.ini文件中加入下面这个参数： skip-grant-tables 除此之外，难倒我只能选择重新安装mysql了么？要知道卸载mysql也不是一件容易事儿啊~难倒我只能重装操作系统了吗？ 谁有高招，求赐教！","tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.kazaff.me/tags/mysql/"},{"name":"window","slug":"window","permalink":"https://blog.kazaff.me/tags/window/"},{"name":"密码","slug":"密码","permalink":"https://blog.kazaff.me/tags/密码/"}]},{"title":"Google Dirve的权限设计","date":"2016-03-11T09:37:00.000Z","path":"2016/03/11/Google Drive的权限设计/","text":"关于google drive的权限设计，一开始看官方文档，搞的云里雾里的！ 可能是对google drive不了解的关系，再加上没有参与设计过类似的平台化系统，所以对于它的权限设计内容，总觉得很绕~停下来花了点时间仔细捋了一遍，发现这个设计还是非常优雅的，至于通用性，我就持保留态度了！ 首先，我们来列出整套权限系统设计到的元素有哪几个： account（帐号） user user group domain anyone resource（资源） file folder permission（权限设置） role type 咱们简单的就这么先罗列出来。 首先我们先看一下三个主要的元素的关系： resource ------&gt; permission ------&gt; account [ ------&gt; resource] 1 n 1 n n n 解释： 一个资源对应多个权限设置 一个资源的metadata中还会包含多个特定帐号（owners, sharingUser, lastModifyingUser） 一个权限设置绑定到多个帐号（根据type来确定和帐号的绑定类型，根据domain和emailAddress来确定具体哪个或哪些帐号） 一个权限设置会预先设定好几个拥有不同操作范围的角色（role），具体关系可以看官方文档 最终，帐号就会和资源关联上，就是这么绕！ 其实只需要注意到，角色和权限的对应关系是固定的，而帐号和角色的关系是靠人为设定的，这两个环节只要一旦确定以后，自然就完成了帐号到资源的操作权限绑定。 现在应该已经解释的很清楚了吧？上面的关系图中最后的account与resource的关系其实是根据前面的关系推出来的，而不是直接可以获取到的（除了在resource的metadata中的那几个特定帐号字段，个人猜测之所有会有这几个metadata，是为了性能）。 不能确定这么设计是好还是坏，但整体来看，其实并不算复杂。","tags":[{"name":"google","slug":"google","permalink":"https://blog.kazaff.me/tags/google/"},{"name":"权限","slug":"权限","permalink":"https://blog.kazaff.me/tags/权限/"},{"name":"api","slug":"api","permalink":"https://blog.kazaff.me/tags/api/"}]},{"title":"关于使用Api为google-Doc创建带anchor的comment的问题","date":"2016-03-08T09:37:00.000Z","path":"2016/03/08/关于为google-doc创建带anchor的comment的问题/","text":"这个问题，如果你跟我一样只从google提供的官方文档上理解的话，那么你肯定是无法明白这篇文章的问题指的是啥？使用官方提供的接口调用工具，你无论如何都无法给你google drive中指定的google document创建带有anchor的comment的！ 为嘛？ 官方不是说的很清楚么，是不是你拼接的json字符串有问题？或者，你的region定义的有问题？或者是不是你用错了region classifier？ 我只想说：呵呵~ 再查看了不少资料后，发现其实很多人都吐槽这个细节，官方对anchor的注意事项确实解释的很清楚，但却没有讲清楚一个问题： 到底google doc是不是plain-text类型的文件？？ 你如果像我一样把它当成是普通的plain-text，你就也会想我一样浪费太久的时间在这个问题上！ 目前能查到的资料上都解释说，到目前为止你还是无法通过api实现对google doc添加绑定anchor的comment，所以，死心吧！","tags":[{"name":"google","slug":"google","permalink":"https://blog.kazaff.me/tags/google/"},{"name":"api","slug":"api","permalink":"https://blog.kazaff.me/tags/api/"},{"name":"anchor","slug":"anchor","permalink":"https://blog.kazaff.me/tags/anchor/"}]},{"title":"关于MariaDB和mysql5.7的json类型特性","date":"2016-03-04T09:37:00.000Z","path":"2016/03/04/关于mariaDB和mysql5.7的json类型特性/","text":"mysql5.7之前有仔细的了解并使用过MongoDb，大概在一两年前吧~但无奈记忆早已模糊！最近可能有需要解决一个数据结构问题，刚好比较符合文档型数据库的领域范畴。就在我正翻看以前记录的文章时，突然想起来，似乎mysql5.7开始支持json类型，心里琢磨，如果可以避免项目中引入过多的依赖，这无疑是最明智的选择。 GG一下，刚好找到了一个入门的文章，基本上把常用操作介绍的非常清楚了。 如果你想知道mysql5.7对json特性的实现细节，不妨看看这里，这样我们就可以开始尝试在业务中使用json类型啦！ 虽然看文档中也提到了，目前可以针对json内部数据进行索引以及检索，但似乎没有mongodb提供的查询强大，但优势是沿用了SQL的知识，可以很快上手！ 关于mysql5.7，先告一段落。 mariadb10.1.10我们再来看看社区版的mariadb，它从5.3版本开始就已经支持json了，不过和mysql的方法不太一样，它基于“Dynamic Columns”思路来实现的，底层和mysql方法一样都是blob类型存储。 目前来看，mariadb支持的json特性并没有mysql的多，或者说稍微有点复杂。官方资料：Dynamic Columns。 尤其是在处理json的嵌套时，使用的方法比较烧脑。 todo虽然目前不管是你选择mysql还是mariadb，都可以使用json类型来处理非结构化数据模型，但你的开发语言提供的db库是否跟得上节奏，这就是个疑问了？ 目前项目主要想在数据结构模型上能获得更大的灵活性，但针对非结构数据类型的检索性能并不是非常敏感，更多的是想持久化“文档概念”的类型！所以不出意外的话，将会暂时不考虑mongodb啦~ 听起来，数据库领域的革命还在激烈的进行着啊！","tags":[{"name":"json","slug":"json","permalink":"https://blog.kazaff.me/tags/json/"},{"name":"mysql","slug":"mysql","permalink":"https://blog.kazaff.me/tags/mysql/"},{"name":"Mariadb","slug":"Mariadb","permalink":"https://blog.kazaff.me/tags/Mariadb/"}]},{"title":"老生常谈Mac下的ntfs写问题","date":"2016-02-26T09:37:12.000Z","path":"2016/02/26/老生常谈Mac下的ntfs写问题/","text":"已经有很久没有更新自己的blog了，以至于域名过期了都不知道，昨天偶然发现后赶紧去续费，哇哈哈～ 假期就要结束了，马上就要开始人生新的旅程，抛弃那些不好的，没有意义的旧事和旧人，迎接美好未来！走起～回到主题，其实我用Mac系统也有年头了，但一直没有怎么折腾过，之前装过一个盗版的ntfs工具，名字不记得了，只记得导致我的移动硬盘差点报废！具体原因我也不清楚，用着用着突然就报错了，再插一遍设备竟然不识别了，换到window下提示硬盘已损坏！ 我攒了二十几年的片儿要是真丢了，我感觉人生都没有意义了！不过还好一番折腾后在win下是可以识别了！经历这次后我乖乖的删除了mac下的各种关于ntfs读写的工具！！ 那为啥今天又要提这事儿呢？当然是因为我的mac硬盘满了～～哇哈哈哈～抱着试一试的态度，我再次GG了一下这方面的技术贴，这次换了一种很简单的方法，这并不是一个新方案，只是之前的我并没有找到。 简单的说，步骤有3： 确保你的移动硬盘名称不包含空格（其实我觉得空格也没问题，转义一下应该就可以） 把硬盘插到mac上，打开终端，输入“sudo nano /etc/fstab” 在打开的编辑器中输入“LABEL=你硬盘的名字 none ntfs rw,auto,nobrowse”，保存退出编辑器 拔掉硬盘，再插上，终端中输入“open /Volumes”，你就会看到你的硬盘已经识别了，并且已经可以读写内容了 将文件夹中的硬盘图标拖拽到Finder的侧边栏，以后你就可以直接访问了。 就这么简单，其它帖子说的什么获取UUID啥的，都不需要！一切就是这么顺利！ 然后做啥？当然是拷片儿了啊！ 好不容易拷了50G的片儿，突然我发现一个问题，放到硬盘里的片儿都是灰色的，并且双击会提示： 项目“XXX”已被 OS X 使用，不能打开。 这可吓死宝宝了，本尊可是已经删除了Mac中的片儿了啊！赶紧GG，找到了解决方案，同样不知道原理是啥，同样很简单就搞定，只需要： 在终端中进入到硬盘的对应位置，执行“xattr -c 有问题的文件名称”即可搞定！ 哇哈哈，你可以说这篇文章很水，不过确实很实用，就当是顿快餐吧～～88","tags":[{"name":"mac","slug":"mac","permalink":"https://blog.kazaff.me/tags/mac/"},{"name":"ntfs","slug":"ntfs","permalink":"https://blog.kazaff.me/tags/ntfs/"}]},{"title":"［译］Relay101制作HackerNew客户端","date":"2016-01-02T09:37:00.000Z","path":"2016/01/02/译-Relay101制作HackerNew客户端/","text":"原文地址：https://medium.com/@clayallsopp/relay-101-building-a-hacker-news-client-bb8b2bdc76e6 React让我们可以使用javascrip创建用户界面组件；Relay则可以让我们很容易的打通react组件和远程服务器的数据通信。为了实现这个目标Relay需要一个条件–它假设客户端和服务器端必须满足某种要求，这可能增加了使用它的门槛，但对于一些项目来说这是非常值得的！没有Relay的日子，你需要自己实现数据的下载，传输，缓存。像Flux和Redux这类工具帮你避免了在这过程中的一些bug，但当大量数据来往于应用与服务器之间时还是有非常多的人为性错误存在的可能。Relay会减少非常多的通用样板代码，让客户端开发人员可以更简洁安全的获取他们想要的数据。 按照之前Rails的十五分钟搭建blog的传统，我们这里将快速使用Relay搭建一个HAcker News客户端。教程假设你了解Node，NPM和react，没有其他要求了～ ###Getting GraphQL 目前Relay需要你的服务器提供GraphQL接口。GraphQL非常优雅，但除非你是facebook的工程师，否则你可能并没有这么一个服务接口。 为了避免我们自己搭建GRaphQL服务接口，我们这里将会使用GraphQLHub。GraphQLHub是一个发展迅速的GraphQL现有API翻译的仓库，和HackerNews和Reddit很像，我会一直维护它:) 本篇指南会带领你快速了解GraphQL基础语法，所以你并不需要提前学习它的内容。如果你对GraphQL很感兴趣，你可以阅读Your First GraphQL Server（译：我已经翻译过了，可以在本博客查找）。 ###Setting Up The Project 在2015年，有大量的工具帮助你创建浏览器端javascript应用。我们选择使用Webpack来打包我们的代码供浏览器解析，并使用Babel来编译我们的React和Relay代码。这些工具都是Relay小组推荐使用的，但你也可以选择使用其它的。 我们不会关注太多Webpack和Babel的内容，毕竟我们的关注点在Relay。 让我们创建一个新的项目： $ mkdir relay-101 &amp;&amp; cd ./relay-101 $ npm init # you can hit Enter a bunch of times 这将会在你的文件夹中创建package.json文件，然后我们安装一些包： $ npm install webpack@1.12.2 webpack-dev-server@1.11.0 babel-core@5.8.25 babel-loader@5.3.2 --save Webpack会查找”webpack.config.js”文件，所以我们需要在当前目录下创建它： $ touch webpack.config.js 并把下面的内容复制粘贴进去： var path = require(&apos;path&apos;); module.exports = { entry: path.resolve(__dirname, &apos;index.js&apos;), module: { loaders: [ { test: /\\.js$/, loader: &apos;babel&apos;, query: {stage: 0} } ] }, output: {filename: &apos;index.bundle.js&apos;, path: &apos;./&apos;} }; 你可能注意到了我们这里提到了一个index.js文件。目前我们创建一个简单的实现： $ echo &apos;alert(&quot;hello relay!&quot;);&apos; &gt; index.js 所有客户端的代码都会放在这个文件里，所以建议你在你的编辑器中保持打开它。 我们还需要在package.json文件中加入“start”入口设定： { // ... &quot;scripts&quot;: { &quot;start&quot;: &quot;./node_modules/.bin/webpack-dev-server&quot;, &quot;test&quot;: &quot;echo \\&quot;Error: no test specified\\&quot; &amp;&amp; exit 1&quot; }, } 这么做是为了允许我们直接使用“npm start”来运行我们之前安装的webpack development server的。我们可以在我们的终端中瞄一眼： $ npm start &gt; relay-101-test@1.0.0 start ~/relay-101 &gt; webpack-dev-server http://localhost:8080/webpack-dev-server/ webpack result is served from / 浏览器中打开http://localhost:8080/webpack-dev-server，你会看到文件列表！这很好，但我们需要的是我们客户端的html页面。回到我们的项目文件夹，创建一个index.html并输入下面的内容： $ touch index.html # paste this inside of index.html &lt;html&gt; &lt;head&gt;&lt;/head&gt; &lt;body&gt; &lt;div id=&apos;container&apos;&gt; &lt;/div&gt; &lt;script src=&quot;/index.bundle.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; 刷新我们的开发服务器，将会看到一个期望的弹窗： 现在我们就可以开始做一些好玩的事儿了。 ###Building A Static Component 我们的小app将会模仿Hacker News界面。开始之前，我们需要安装React和React-DOM包： $ npm install react@0.14.0-rc1 react-dom@0.14.0-rc1 --save 注意我们这里指定了明确的版本号（如果将来你发现接口变更，请通知我）。回到index.js，删除我们原先的alert代码，开始实现我们的Item组件： // inside index.js let React = require(&apos;react&apos;); let ReactDOM = require(&apos;react-dom&apos;); class Item extends React.Component { render() { let item = this.props.store.item; return ( &lt;div&gt; &lt;h1&gt;&lt;a href={item.url}&gt;{item.title}&lt;/a&gt;&lt;/h1&gt; &lt;h2&gt;{item.score} - {item.by.id}&lt;/h2&gt; &lt;hr /&gt; &lt;/div&gt; ); } }; 注意，我们所有的数据都来自于“store”属性–稍后我们会解释～ 让我们先在屏幕上伪造一些item： // at the bottom of index.js let mountNode = document.getElementById(&apos;container&apos;); let item = { id : &apos;1337&apos;, url : &apos;http://google.com&apos;, title : &apos;Google&apos;, score : 100, by : { id : &apos;clay &apos;} }; let store = { item }; let rootComponent = &lt;Item store={store} /&gt;; ReactDOM.render(rootComponent, mountNode); 刷新我们的开发服务器，你将会看到： ###Data From The Server 是时候来加点Relay了。我们将会从GraphQLHub上根据ID获取一些itme数据来代替我们的静态数据。我们先来安装一些Relay包： $ npm install react-relay@0.3.2 babel-relay-plugin@0.2.5 sync-request@2.0.1 graphql@0.4.4 --save 为什么我们安装了这么多东西而不仅仅是react-relay？好吧，目前Relay要求我们做的确实有点多–特别是，我们需要“babel-relay-plugin”来结合Babel。这个plugin会去GraphQLHub获取更多的Relay的配置。 为了连接plugin，我们需要修改一下webpack.config.js中“query”选项： module.exports = { // ... module: { loaders: [ { // ..., // note that this is different! query: {stage: 0, plugins: [&apos;./babelRelayPlugin&apos;]} } ] } // ... }; 这么做将会告诉Babel去加载一个叫babelRelayPlugin.js的文件。创建这个文件并放入下面的内容： $ touch babelRelayPlugin.js // inside that file var babelRelayPlugin = require(&apos;babel-relay-plugin&apos;); var introspectionQuery = require(&apos;graphql/utilities&apos;).introspectionQuery; var request = require(&apos;sync-request&apos;); var graphqlHubUrl = &apos;http://www.GraphQLHub.com/graphql&apos;; var response = request(&apos;GET&apos;, graphqlHubUrl, { qs: { query: introspectionQuery } }); var schema = JSON.parse(response.body.toString(&apos;utf-8&apos;)); module.exports = babelRelayPlugin(schema.data, { abortOnError: true, }); Cool–现在杀掉你的“npm start”进程并重启它。现在每次你重新打包你的app，它都会去GraphQLHub服务器（使用GraphQL优雅的自解释API）询问和预处理我们的Relay代码。 回到index.js，是时候导入Relay啦： let React = require(&apos;react&apos;); let ReactDOM = require(&apos;react-dom&apos;); let Relay = require(&apos;react-relay&apos;); 接下来该做甚？我们将把我们的Item组件用higher-order包装一下。这个新的组件将被Relay管理，这就是黑科技的地方： class Item extends React.Component { ... } Item = Relay.createContainer(Item, { fragments: { store: () =&gt; Relay.QL` fragment on HackerNewsAPI { item(id: 8863) { title, score, url by { id } } } `, }, }); 哒啦，搞定。翻译成大白话，上面的写法解释为： 嘿 Relay，我将把我的item组件作为原件放到新的组件容器中。 对于组件的“store”属性，我需要数据已经用GraphQL片段描述了。 我知道这些数据在http://graphqlhub.com/playground/hn的“HackerNewsAPI”对象中。 注意这里我们只是描述了一个GraphQL片段（片段的概念很类似于查询中的别名），并不是最终获取所有数据的完整query。这是Relay的一个优点–组件只需要声明它需要的数据，而不用管如何获取数据。 有些时候我们确实需要一个完整的GraphQL query，这就是Relay Routes的主场了。Relay.Route跟浏览器里的history或URLs半毛钱关系都木有–它是用来创建引导我们数据获取请求的“root query”的。 嗖，让我们来搞一个Relay Route。在我们的Itme定义下面加入： Item = // ...; class HackerNewsRoute extends Relay.Route { static routeName = &apos;HackerNewsRoute&apos;; static queries = { store: ((Component) =&gt; { // 这里Component就是我们的Item组件 return Relay.QL` query root { hn { ${Component.getFragment(&apos;store&apos;)} }, } `}), }; } 现在我们的GraphQL补全了root query。Relay允许我们使用ES6的字符串解析特性注入我们的片段，这就完成了组件分享（不同于复制）它的数据需求给父组件的过程。 是时候展示点数据到屏幕上了！修改我们之前的代码如下： class HackerNewsRoute { // ... } Relay.injectNetworkLayer( new Relay.DefaultNetworkLayer(&apos;http://www.GraphQLHub.com/graphql&apos;) ); let mountNode = document.getElementById(&apos;container&apos;); let rootComponent = &lt;Relay.RootContainer Component={Item} route={new HackerNewsRoute()} /&gt;; ReactDOM.render(rootComponent, mountNode); 这里Relay.RootContainer是顶级组件，它构建了一个组件层级的query。我们还做了一些网络配置，最终渲染新的组件到DOM。你会在浏览器中看到下面的景象： ###A List Of Components 我们下面开始做一个类似Hacker News首页的页面。相比硬编码一个特定的item，我们需要展示一个item列表。用Relay的话说，这需要我们创建一个新的list组件，其中嵌套多个独立的item组件（每个都请求自己的数据）。 回到代码，我们开始创建我们的TopItems组件： class TopItems extends React.Component { render() { let items = this.props.store.topStories.map( (store, idx) =&gt; &lt;Item store={store} key={idx} /&gt; ); return &lt;div&gt; { items } &lt;/div&gt;; } } 我们就不再想刚才那样先“创建伪造数据”了，而是直接动真格的，用Relay封装TopItems： TopItems = Relay.createContainer(TopItems, { fragments: { store: () =&gt; Relay.QL` fragment on HackerNewsAPI { topStories { ${Item.getFragment(&apos;store&apos;)} }, } `, }, }); 相比之前的单独item，现在我们需要请求“topStories”。针对每个item，GraphQL会根据声明的片段请求对应的数据，所以我们将只请求我们需要的数据。 但是稍等–目前我们的item片段定义了一个特定的id（#8863）。我们需要修改我们的query： Item = Relay.createContainer(Item, { fragments: { store: () =&gt; Relay.QL` fragment on HackerNewsItem { id title, score, url by { id } } `, }, }); 由于我们不再请求一个特定的item片段，所以我们需要修改在render函数中存取prop的方式： class Item extends React.Component { render() { let item = this.props.store; // ... } } 最后，我们需要更新Relay RootContainer来使用我们的TopItems组件： let rootComponent = &lt;Relay.RootContainer Component={TopItems} route={new HackerNewsRoute()} /&gt;; 瞧： ###Variables in Queries 现在我们对创建一个Relay app有了最基础的了解，但我希望展现Relay另一个特性：variables。 在多数挨派派里，查询并不是一成不变的，我们经常需要请求不同的数据。Relay允许我们在GraphQL query中注入变量来达到这个目的。在我们这个小挨派派里，我们将添加一个开关来切换我们需要的数据类型（按照热度或时间等排序）。 开始之前，我们需要修改我们的TopItems的query： TopItems = Relay.createContainer(TopItems, { initialVariables: { storyType: &quot;top&quot; }, fragments: { store: () =&gt; Relay.QL` fragment on HackerNewsAPI { stories(storyType: $storyType) { ${Item.getFragment(&apos;store&apos;)} }, } `, }, }); “$storyType”代表一个GraphQL变量（这并不是ES6的字符串解析语法）。我们通过initialVariables配置给它设置了一个初始默认值“top”。 这只是Relay层面我们需要做的简单修改。我们并不需要对具体的组件做任何渲染或数据获取的修改–完全解耦了。 现在我们需要编辑我们的TopItems组件来使用开关类型。更新render函数如下： class TopItems extends React.Component { render() { let items = this.props.store.stories.map( (store, idx) =&gt; &lt;Item store={store} key={idx} /&gt; ); let variables = this.props.relay.variables; // To reduce the perceived lag // There are less crude ways of doing this, but this works for now let currentStoryType = (this.state &amp;&amp; this.state.storyType) || variables.storyType; return &lt;div&gt; &lt;select onChange={this._onChange.bind(this)} value={currentStoryType}&gt; &lt;option value=&quot;top&quot;&gt;Top&lt;/option&gt; &lt;option value=&quot;new&quot;&gt;New&lt;/option&gt; &lt;option value=&quot;ask&quot;&gt;Ask HN&lt;/option&gt; &lt;option value=&quot;show&quot;&gt;Show HN&lt;/option&gt; &lt;/select&gt; { items } &lt;/div&gt;; } // to be continued 这里有些新的知识点！我们使用了“Relay”prop，它包含一些特定的属性。任何使用Relay封装的组件都会被注入这个prop–如果我们想对我们的TopItems组件进行单元测试，我们需要自己注入一个伪造的对象。 除此Relay变量之外，其它都是普通的React–我们创建一个下拉菜单，并给它一个默认值，并监听它的修改。当改变下拉菜单的选项，我们会告诉Relay去使用新的变量值，如下： class TopItems extends React.Component { render() { // ... } _onChange(ev) { let storyType = ev.target.value; this.setState({ storyType }); this.props.relay.setVariables({ storyType }); } } 一切都很简答–Relay会察觉query的一部分发生了变化，并且根据需要重新去获取数据。为了简单，我们设置了组件的内部状态， 刷新你的浏览器并且切换不同的下拉选项。你会发现，Relay并不会重复获取已经加载了的数据类型。 ###Relay 102 SO，以上就是关于Relay的简单介绍。我们还没有涉及到mutations（用来完成更新服务端数据）和在获取数据时显示加载提醒。Relay非常的灵活，但代价是需要比我们这里提到的更多的配置。 Relay可能并不是适合所有的挨派派和团队，但它在解决某些常见问题上确实令人非常激动。 这篇文章中的源码放在Github，关注我@clayallsopp和@GraphQLHub来获取更多的信息。 ###译者注 看了不少相关资料，个人感觉，目前Relay+GraphQL相比react确实还有待社区的考量，并且它们确实还太新，官方也强调可能会出现比较大的变动。所以，目前用在实际项目里的风险确实较大，还是继续观望吧～纯属个人看法！","tags":[{"name":"react","slug":"react","permalink":"https://blog.kazaff.me/tags/react/"},{"name":"GraphQL","slug":"GraphQL","permalink":"https://blog.kazaff.me/tags/GraphQL/"},{"name":"Relay","slug":"Relay","permalink":"https://blog.kazaff.me/tags/Relay/"}]},{"title":"GraphQL什么鬼","date":"2016-01-01T09:37:12.000Z","path":"2016/01/01/GraphQL什么鬼/","text":"昨天看了一个技术分享，讲react生态圈的，很不错，尽管纯技术干货没太多，但贵在拓宽知识面，讲师也很有激情，推荐看之～在react生态圈里，越来越多的富有创意和激情的东西呈现在我们眼前，对于前端工程师来说真的是求之不得的时代啊～ react之前也玩过了，和它相关的一些类库（例如router，redux）我们也都陆续介绍过，甚至包含react-natvie也有所涉及，那么今天要解惑的，就是GraphQL。 官方文档冗长且晦涩，我们不妨延续以往的学习经验，先gg几篇不错的博文来共大家品尝，那么和GraphQL相关的姿势哪里找呢？请看这里，这里包含了各种语言的实现版本，当然也包含了入门文章的推荐，有兴趣的童鞋不妨长期关注该项目～ 由于我也是第一天开始了解GraphQL，并不比大家知道的多多少，所以打算翻译两篇感觉很适合入门的实战文章来让大家过过瘾，这两篇文章虽然不是出自于同一人所写，但内容上关联性却很大，个人感觉合并在一起刚刚好，所以在此斗胆合并在一起翻译，原文地址： Your First GraphQL Server Writing a Basic API with GraphQL 如果你按照我提供的轨迹，看过前面提到的那个关于“react生态圈”的技术分享，那你应该就已经知道GraphQL是为了解决什么问题而产生的，如果你没看，也没事儿，再开始翻译之前我还是会简单的阐述一下GraphQL的背景。 ###GraphQL对比SQL 首先，官方说了，GraphQL是一个查询语言，而且目前还未完成，意味着未来可能会有更多更大的变动。 单这并不妨碍世界上那么多前端工程师的折腾之路！查询语言？那就是说和sql类似喽？你看名字里都有“QL”啊～ 确实如此，确实是用来声明要查询的数据的，但要解决的问题却完全不一样，伴随我们多年的sql主要解决的是如何在数据库基础上提供简单易用且功能强大的沟通语言，使得我们人类可以轻而易举的从海量数据中获取到我们想要知道的数据片段。 GraphQL产生的背景却完全不同，在facebook内部，大量不同的app和系统共同使用着许许多多的服务api，随着业务的变化和发展，不同app对相同资源的不同使用方法最终导致需要维护的服务api数量爆炸式的增长，非常不利于维护（我们主要在restful场景上思考）。而另一方面，创建一个大而全的通用性接口又非常不利于移动端使用（流量损耗），而且后端数据的无意义聚合也对整个系统带来了很大的资源浪费。 在这样的背景下，fb工程师坐不住了，于是乎GraphQL的概念就诞生了～最了解客户端需要什么数据的只有客户端自己，所以如果给客户端提供一种机制，让其表述自己所需的数据结构，这岂不是最合理的么？ ###GraphQL对比Rest 目前，最热的前后端通信方案应该是Restful，基于http的轻量级api，前端通过ajax请求服务端提供的rest api完成数据获取。 我们再往前一步，假设你的项目前端已经组件化了，一个业务肯定是需要多个组件结合来完成的，每个组件都各自管理自己的内部状态和所需数据，那么，目前的做法是，一旦前端路由匹配了对应的业务页面，那么自然会加载相关的组件实现，同时，你还需要调用rest api来获取组件所需数据。 不同的页面，组件的组合肯定也略有不同，不同的组件组合后，所需的数据自然也不会完全一致。这里你可能会说，既然以组件为单位复用，那rest api针对组件颗粒度提供一对一的服务即可，话是没错，但实际操作起来就不work了，试想前端狗被产品狗每天要求加这加那，前端狗就会不得不去求后端狗协同开发，这样最后就剩下死逼了！而且前面也说了，这样做创造出来的大量的api会变得无法维护～ 那么，是时候考虑采用Rest以外的解决方案了！（尽管我认为，GraphQL并非是来取代Rest的，但为了我们的描述简单，这里就直接这么写了！）后端根据GraphQL机制提供一个具有强大功能的接口，用以满足前端数据的个性化需求，既保证了多样性，又控制了接口数量，完美～ ###GraphQL到底是什么 A GraphQL query is a string interpreted by a server that returns data in a specified format. 这端描述如果不了解它的背景，确实很容易和sql混淆～但，现在，你应该知道GraphQL到底是个什么鬼了吧～ 好的，下面我们就来开始GraphQL in Action! Your First GraphQL Server今天我们将要实现一个GraphQL小服务，我不没有打算让你放弃一切转而拥抱GraphQL，但如果你对这玩意儿很好奇，并想知道它是如何实现的，那么就往下读～ ###Setup an HTTP Server 我们需要一个服务来接受GraphQL查询，GraphQL文档虽然并没有规定其一定要基于HTTP协议，但既然目前GraphQL参考实现是基于JavaScript的，那么我们就基于Express来快速打造一个http服务器完成需求吧～ $ mkdir graphql-intro &amp;&amp; cd ./graphql-intro $ npm install express --save $ npm install babel --save $ touch ./server.js $ touch ./index.js 我们创建了项目文件夹（graphql-intro），并且安装了Express和Babel作为依赖。Babel并不是GraphQL所必须的，但它可以让我们使用ES2015特性，从而是我们可以书写更精彩的js。（译：我瞎掰的～） 最后，让我们写点代码： // index.js // by requiring `babel/register`, all of our successive `require`s will be Babel&apos;d require(&apos;babel/register&apos;); require(&apos;./server.js&apos;); // server.js import express from &apos;express&apos;; let app = express(); let PORT = 3000; app.post(&apos;/graphql&apos;, (req, res) =&gt; { res.send(&apos;Hello!&apos;); }); let server = app.listen(PORT, function () { let host = server.address().address; let port = server.address().port; console.log(&apos;GraphQL listening at http://%s:%s&apos;, host, port); }); 现在运行我们的服务： $ node index.js GraphQL listening at http://0.0.0.0:3000 来测试一下我们的代码： $ curl -XPOST http://localhost:3000/graphql Hello! 我们选择使用/graphql作为入口，并使用HTTP POST方法请求，这些都不是影硬性要求–GraphQL并没有限制你如何与GraphQL服务端通信。 ###Create a GraphQL Schema 现在咱们有了一个服务，是时候来加点GraphQL啦。具体改怎么做呢？ 让我们回想一下GraphQL请求的模样（译：我们目前不太关心GraphQL文档的细节，只需要跟着作者的步骤即可）： query getHighScore { score } 目前，我们的GraphQL客户端需要请求getHighScore字段中的score字段，字段是用来告诉GraphQL服务返回哪些数据的，字段也可以拥有参数，如下： query getHighScores(limit: 10) { score } 它还能做更多的事儿，但让我们先往下看。 我们的GraphQL服务需要进行配置才能响应上面那样的请求–这种配置被成为schema。 构建一个schema其实和构建restful路由规则是很相似的。我们的schema需要描述哪些字段是服务器需要响应的，同时也需要包含这些响应对象的类型。类型信息对GraphQL来说是非常重要的，客户端可以放心的假设服务端会返回所指定的字段类型（或者一个error）。 如你所想，schema声明可以非常的复杂。但对于我们这个简单的GraphQL服务来讲，我们需要一个简单的字段：Count。 回到我们的终端： $ npm install graphql --save $ npm install body-parser --save $ touch ./schema.js 就是这么靠谱，对么？graphql模块包含了GraphQL的技术实现，可以允许我们来组合我们的schema和处理graphql请求。而body-parser则是一个简单的Express中间件，用来让我们获取GraphQL请求体哒～ 是时候来声明我们的schema： //schema.js import { GraphQLObjectType, GraphQLSchema, GraphQLInt } from &apos;graphql/lib/type&apos;; let count = 0; let schema = new GraphQLSchema({ query: new GraphQLObjectType({ name: &apos;RootQueryType&apos;, fields: { count: { type: GraphQLInt, resolve: function() { return count; } } } }) }); export default schema; 这里我们所做的就是创建了一个GraphQLSchema实例，它提供了一个配置。后面GRaphQL的其他部件会使用我们这个schema实例。通常情况下我们喜欢将schema的创建放在单独的文件中。 简单的解释，我们创建的schema的含义是： 我们的顶级查询对象需要返回一个RootQueryType对象，它包含一个类型为整型的count字段。 你可以猜到还有很多类似的内置基础类型（strings，lists等），当然你也可以创建自定义的特殊类型。 ###Connect the Schema 目前我们意淫的schema并没有毛用，除非我们针对它进行查询。让我们把这个schema挂载到我们的http服务上： import express from &apos;express&apos;; import schema from &apos;./schema&apos;; // new dependencies import { graphql } from &apos;graphql&apos;; import bodyParser from &apos;body-parser&apos;; let app = express(); let PORT = 3000; // parse POST body as text app.use(bodyParser.text({ type: &apos;application/graphql&apos; })); app.post(&apos;/graphql&apos;, (req, res) =&gt; { // execute GraphQL! graphql(schema, req.body) .then((result) =&gt; { res.send(JSON.stringify(result, null, 2)); }); }); let server = app.listen(PORT, function () { var host = server.address().address; var port = server.address().port; console.log(&apos;GraphQL listening at http://%s:%s&apos;, host, port); }); 现在任何POST请求/graphql都将会执行我们的GRaphQL schema。我们为每个请求强制设了一个“content type”（译：’application/graphql’），这并不是GraphQL规定的，但这么做是一个好的选择，特别是当我们在现有代码中加入GraphQL功能时。 执行下面的命令： $ node ./index.js // restart your server // in another shell $ curl -XPOST -H &quot;Content-Type:application/graphql&quot; -d &apos;query RootQueryType { count }&apos; http://localhost:3000/graphql { &quot;data&quot;: { &quot;count&quot;: 0 } } 完美！GraphQL允许我们省略掉query RootQueryType前缀，如下： $ curl -XPOST -H &apos;Content-Type:application/graphql&apos; -d &apos;{ count }&apos; http://localhost:3000/graphql { &quot;data&quot;: { &quot;count&quot;: 0 } } 现在我们已经完成了一个GraphQL例子，我们来花点时间讨论一下introspection。（译：应该翻译成“自解释”吧？） ###Introspect the server 有趣的是：你可以写一个GraphQL查询来请求GraphQL服务获取它的fields。 听起来很疯狂？看一下这个： $ curl -XPOST -H &apos;Content-Type:application/graphql&apos; -d &apos;{__schema { queryType { name, fields { name, description} }}}&apos; http://localhost:3000/graphql { &quot;data&quot;: { &quot;__schema&quot;: { &quot;queryType&quot;: { &quot;name&quot;: &quot;RootQueryType&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;count&quot;, &quot;description&quot;: null } ] } } } } 格式化一下我们上面发送的查询语句： { __schema { queryType { name, fields { name, description } } } } 通常，每个GraphQL根字段自动包含一个__Schema字段，其包含用来查询的描述自身meta信息的字段–queryType。 更可爱的是，你可以定义一些很有意义的元信息，例如description，isDeprecated，和deprecationReason。facebook宣成他们的工具可以很好的利用这些元信息来提升开发者的体验～ 为了让我们的服务更容易使用，我们这里添加了description字段： let schema = new GraphQLSchema({ query: new GraphQLObjectType({ name: &apos;RootQueryType&apos;, fields: { count: { type: GraphQLInt, // add the description description: &apos;The count!&apos;, resolve: function() { return count; } } } }) }); 重启服务后看一下新的元数据展示： $ curl -XPOST -H &apos;Content-Type:application/graphql&apos; -d &apos;{__schema { queryType { name, fields { name, description} }}}&apos; http://localhost:3000/graphql { &quot;data&quot;: { &quot;__schema&quot;: { &quot;queryType&quot;: { &quot;name&quot;: &quot;RootQueryType&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;count&quot;, &quot;description&quot;: &quot;The count!&quot; } ] } } } } 我们几乎已经完成了我们的GraphQL旅程，接下来我将展示mutations。 ###Add a Mutation 如果你只对只读数据接口感兴趣，你就不用读下去了。但大多数应用，我们都需要去更改我们的数据。GraphQL把这种操作称为mutations。 Mutations仅仅是一个字段，所以语法和query字段都差不多，Mutations字段必须返回一个类型值–目的是如果你更改了数据，你必须提供更改后的值。 我们该如何为我们的schema增加mutations？和query非常相似，我们定义一个顶级键mutation： let schema = new GraphQLSchema({ query: ... mutation: // todo )} 除此之外，还有啥不特别的？我们需要在count字段函数中更新我们的计数器或者做一些GraphQL根本不需要感知的其他变更操作。 mutation和query有一个非常重要的不同点，mutation是有执行顺序的，但是query没有这方面的保证（事实上，GraphQL推荐并行处理那些不存在依赖的查询）。GraphQL说明书给了下面的mutation例子来描述执行顺序： { first: changeTheNumber(newNumber: 1) { theNumber }, second: changeTheNumber(newNumber: 3) { theNumber }, third: changeTheNumber(newNumber: 2) { theNumber } } 最终，请求处理完毕后，theNumber字段的值应该是2。 让我们新增一个简单的mutation来更新我们的计数器并返回新值： let schema = new GraphQLSchema({ query: ... mutation: new GraphQLObjectType({ name: &apos;RootMutationType&apos;, fields: { updateCount: { type: GraphQLInt, description: &apos;Updates the count&apos;, resolve: function() { count += 1; return count; } } } }) }); 重启我们的服务，试一下： $ curl -XPOST -H &apos;Content-Type:application/graphql&apos; -d &apos;mutation RootMutationType { updateCount }&apos; http://localhost:3000/graphql { &quot;data&quot;: { &quot;updateCount&quot;: 1 } } 看–数据已经更改了。你可以重新查询一下： $ curl -XPOST -H &apos;Content-Type:application/graphql&apos; -d &apos;{ count }&apos; http://localhost:3000/graphql { &quot;data&quot;: { &quot;count&quot;: 1 } } 你可以多执行几次。 为了严格遵守GraphQ实现，我们应该提供一个更语意化的值名（例如CountValue），这样会让mutation和query返回值都更加有意义。 ###Wrapping Up 本文章教你如何使用facebook提供的GraphQL javascript版实现来完成服务的。但我并没有涉及过多的强大话题–fields with arguments, resolving promises, fragments, directives等等。GraphQL说明书中介绍了很多特别屌的特性。其实还有很多不同的服务端GraphQL实现和schema API。你可以使用像java这样的强类型语言来实现GraphQL服务。 本篇是基于我的GraphQL的48小时体验而来–如果有什么遗漏或错误，别忘了让我知道。你可以在这里看到源码（每次提交都对应流程中的每一步）： https://github.com/clayallsopp/graphql-intro 十分感谢RisingStack的关于GraphQL的文章和例子。 Writing a Basic API with GraphQL这篇文章假设你了解GraphQL，并试图将你的后端实现转换成GraphQL，内容覆盖了同步/异步，query/mutation。 源码：https://github.com/davidchang/graphql-pokedex-api 基本的Pokedex客户端实现来自于我上周的Redux Post，让我们使用GraphQL来实现一个Pokedex后端API。我们需要两个query方法（查询口袋妖怪列表和指定用户所拥有的口袋妖怪）和两个mutation方法（创建用户和用户捕获口袋妖怪）。 口袋妖怪列表数据存储在内存中，而用户和其拥有的口袋妖怪数据则将存储再MongoDB。 ###Starting 继续前面提到的文章（其实就是上一篇），我使用babel在node上创建了一个Express服务端，它包含了一个/graphql路由。我的server.js入口文件如下： import express from &apos;express&apos;; import schema from &apos;./schema&apos;; import { graphql } from &apos;graphql&apos;; import bodyParser from &apos;body-parser&apos;; let app = express(); // parse POST body as text app.use(bodyParser.text({ type: &apos;application/graphql&apos; })); app.post(&apos;/graphql&apos;, (req, res) =&gt; { // execute GraphQL! graphql(schema, req.body) .then(result =&gt; res.send(result)); }); let server = app.listen( 3000, () =&gt; console.log(`GraphQL running on port ${server.address().port}`) ); ###Synchronous Query ####{ pokemon { name } } 让我们从Pokemon列表开始。我们的列表数据来自这里，每个Pokemon对象都包含name，type，stage和species属性。我们需要定义一个新的GraphQLObjectType来描述这种对象。定义如下： import { GraphQLObjectType, GraphQLInt, GraphQLString, } from &apos;graphql&apos;; let PokemonType = new GraphQLObjectType({ name: &apos;Pokemon&apos;, description: &apos;A Pokemon&apos;, fields: () =&gt; ({ name: { type: GraphQLString, description: &apos;The name of the Pokemon.&apos;, }, type: { type: GraphQLString, description: &apos;The type of the Pokemon.&apos;, }, stage: { type: GraphQLInt, description: &apos;The level of the Pokemon.&apos;, }, species: { type: GraphQLString, description: &apos;The species of the Pokemon.&apos;, } }) }); 其中name，type和species都是字符串类型（type和species也可能是枚举类型），stage是整型。 为了使用GraphQL来请求Pokemon，我们需要在GraphQLSchema中定义一个根query。一个标准的空的schema看起来是这样的： import { GraphQLSchema } from &apos;graphql&apos;; let schema = new GraphQLSchema({ query: new GraphQLObjectType({ name: &apos;RootQueryType&apos;, fields: { // root queries go here! } }) }); export default schema; 为了定义我们的新的根查询，我们需要在fields中添加一个键值对对象，key为查询的名字，值为定义查询如何工作的一个对象。如下所示： pokemon: { type: new GraphQLList(PokemonType), resolve: () =&gt; Pokemon // here, Pokemon is an in-memory array } type指定了GraphQL的返回类型 - 一个PokemonType对象类型的列表。resolve告诉GraphQL如何获取所需的数据。这里的数据仅仅是前面提到的pokemontype类型的js内存数组。 鼓掌，我们的GraphQL API现在已经有了一个root query。我们可以发送一个携带查询内容的post请求来验证这部分代码（这么做可避免因使用GET而带来的编码问题）。（GraphQL并不关心如何获取这个查询–这完全有实现者来解决。） curl -XPOST -H &apos;Content-Type:application/graphql&apos; -d &apos;{ pokemon { name } }&apos; http://localhost:3000/graphql 如果我们只想获取type和species，而不要name，我们可以这么做： curl -XPOST -H &apos;Content-Type:application/graphql&apos; -d &apos;{ pokemon { type, species } }&apos; http://localhost:3000/graphql ###Asynchronous Query with an Argument ####{ user(name: “david”) { name, caught } } 接下来，我们需要提供一种查询，用来根据用户名来获取对应的pokemon。让我们再次从定义user类型开始： let UserType = new GraphQLObjectType({ name: &apos;User&apos;, description: &apos;A User&apos;, fields: () =&gt; ({ name: { type: GraphQLString, description: &apos;The name of the User.&apos;, }, caught: { type: new GraphQLList(GraphQLString), description: &apos;The Pokemon that have been caught by the User.&apos;, }, created: { type: GraphQLInt, description: &apos;The creation timestamp of the User.&apos; } }) }); user包含一个字符串类型的name，字符串数组类型的表示所拥有pokemon的caught和一个整型的时间戳。 回到我们的GraphQLSchema，我们将添加一个user根查询，并期望得到一个UserType类型的返回。我们需要可以指定用户名的参数-我们利用args来实现： user: { type: UserType, args: { name: { description: &apos;The name of the user&apos;, type: new GraphQLNonNull(GraphQLString) } }, resolve: (root, {name}) =&gt; { } } 我们在resolve函数中接受name参数。在这个例子中，我想使用MongoDB，所以我们的resolve函数将需要去查询MongoDB数据库并获取对应的用户对象，不过GraphQL（包括读者你）都不需要关心这个实现细节-唯一需要关心的是resolve函数将返回一个promise。（promise是一个包含then函数的对象。）所以，resolve函数可以如下定义： resolve: (root, {name}) =&gt; { return someLogicReturningAPromise(); } [更屌的是如果你使用Babel，你就可以使用async/await特性（但我并没有使用，事实上，我使用的是q，而不是原生的Promises）]。 为了验证这个新增的root query，我们可以执行： curl -XPOST -H &apos;Content-Type:application/graphql&apos; -d &apos;{ user(name: “david”) { name, caught, created } }&apos; http://localhost:3000/graphql ###Mutations mutations通常会返回修改后的新的modified数据对象（不同于只读的query）。对于我们的Pokedex API，我们将实现一个创建user和为指定user添加pokemon到服务。为了实现这个目的，我们需要在GraphQLSchema中添加mutation定义，和我们添加query类似： let schema = new GraphQLSchema({ query: new GraphQLObjectType({ name: &apos;RootQueryType&apos;, fields: { // root queries went here } }), mutation: new GraphQLObjectType({ name: &apos;Mutation&apos;, fields: { // mutations go here! } }) }); mutation配置和query很类似-它包含type，args和resolve。 我们第一个mutation是添加用户： upsertUser: { type: UserType, args: { name: { description: &apos;The name of the user&apos;, type: new GraphQLNonNull(GraphQLString) } }, resolve: (obj, {name}) =&gt; { return someLogicReturningAPromise(); }) } 内部的，resolve函数会去mongo数据库中查询给定的用户名，如果不存在就会新建这个用户。但这些细节并不需要你关心。 调用这个mutation如下： curl -XPOST -H &apos;Content-Type:application/graphql&apos; -d ‘mutation M { upsertUser(name: “newUser”) { name, caught, created } }&apos; http://localhost:3000/graphql 参数mutation很重要，后面要跟一个name–这里，name就是M，如果你删除了mutation，意味着你告诉GraphQL你想在root query中执行upsertUser（这是不存在的）。如果你省略了M,GraphQL会报语法错误告诉你需要给定一个name。 我们要实现的第二个mutation是获取pokemon–这里的参数是用户名和pokemon名。我们的mutation非常简单： caughtPokemon: { type: UserType, args: { name: { description: &apos;The name of the user&apos;, type: new GraphQLNonNull(GraphQLString) }, pokemon: { description: &apos;The name of the Pokemon that was caught&apos;, type: new GraphQLNonNull(GraphQLString) } }, resolve: (obj, {name, pokemon}) =&gt; { return someLogicReturningAPromise(); }) } 调用这个mutation如下： curl -XPOST -H &apos;Content-Type:application/graphql&apos; -d ‘mutation M { caughtPokemon(name: “newUser” pokemon: “Snorlax&quot;) { name, caught, created } }&apos; http://localhost:3000/graphql ###Closing GraphQL的文档和生态圈仍处于婴儿时期，但就我们目前在一些会议和其发布的文档来看，它是很简单灵活的。从我在这篇文章中所获得的经验中来看是非常值得令人激动的。但更令我激动的是与Relay的结合。我非常乐观的认为，这些工具将会加速开发和减少代码，让我从死板的后端和数据中解脱出来。 ###读后感 看完这两篇文章，我觉得大家都应该大致了解GraphQL的用法了吧，当然这里面肯定还有很多高级特性没有看到，不过GraphQL官方说明书可真是好长好长啊！！ 其实我觉得，可以使用GraphQL作为中间件，后端依然请求rest api，这可能是目前我觉得最稳妥的方案，毕竟关于GraphQL我们还有太多的未知，而且GraphQL还存在不少变数！","tags":[{"name":"react","slug":"react","permalink":"https://blog.kazaff.me/tags/react/"},{"name":"GraphQL","slug":"GraphQL","permalink":"https://blog.kazaff.me/tags/GraphQL/"},{"name":"rest","slug":"rest","permalink":"https://blog.kazaff.me/tags/rest/"}]},{"title":"React-Native-Android关于我是如何打包APK的","date":"2015-12-21T21:37:12.000Z","path":"2015/12/21/react-native-android关于我是如何打包APK的/","text":"app写好了，最后一步多半应该是拿出来装逼喽～哇哈哈哈哈 但你总不能指望每次都USB连上你的开发机，然后run-android一下吧，虽然我承认看着命令行中刷刷刷的执行命令有种黑客帝国的范儿，但让所有人都连接的开发机，感觉也是怪怪的，更别提你还指望所有设备都在一个局域网下才能更新js文件。。。。 好吧，你还是应该考虑把你的项目打包成apk，然后传给任何你想装逼给他看的人！哇哈哈哈哈～ 时至今日，2015-12-21 晚上九点四十分，官方版本应该是0.16.0，我按照官方教程进行打包，还是没能一次性就成功！但这并不能阻挡我试图装逼的心！ 好吧，解决问题之旅开始了！ 官方文档中，在打包之初是让你先生成了一个签名文件，原因是，如果你打包未签名的APK，在非root的设备上是不允许安装的，所以，你懂的！官方已经给出了非常具体的签名步骤，我这里就不重复了！ 好的，签名也弄好，执行gradlew assembleRelease命令坐等完成吧～ 但是，怎么可能让你如此轻易就达到目的？毫不意外的，我碰到了报错： 123* What went wrong:Execution failed for task &apos;:app:packageRelease&apos;.&gt; Unable to compute hash of /Users/kazaff/Documents/React-Native/ZhuiYuan/android/app/build/intermediates/classes-proguard/release/classes.jar 不过GG了一下，看来碰见这个错误的人不少，按照stackoverflow给出的终结方案：*在proguard-rules.pro文件末尾增加： 12345678-dontwarn java.nio.file.Files-dontwarn java.nio.file.Path-dontwarn java.nio.file.OpenOption-dontwarn org.codehaus.mojo.animal_sniffer.IgnoreJRERequirement-keep class com.google.android.gms.** &#123; *; &#125;-dontwarn com.google.android.gms.**-dontwarn butterknife.** 还是被我机智的搞定了！ 注意，再次执行gradlew assembleRelease之前，请先执行gradlew clean，清除之前打包的一些临时文件，不然你可能还是会悲剧～ 好了，其实说了这么多，我只是想贱贱的贴一个下载连接： http://pan.baidu.com/s/1pKhY6wj 上图是ZhuiYuanDemoAPK的文件二维码，供大家把玩～","tags":[{"name":"react-native","slug":"react-native","permalink":"https://blog.kazaff.me/tags/react-native/"},{"name":"android","slug":"android","permalink":"https://blog.kazaff.me/tags/android/"},{"name":"打包","slug":"打包","permalink":"https://blog.kazaff.me/tags/打包/"},{"name":"APK","slug":"APK","permalink":"https://blog.kazaff.me/tags/APK/"}]},{"title":"React-Native-Android小结","date":"2015-12-21T09:37:12.000Z","path":"2015/12/21/React-Native-Android小结/","text":"最近尝试使用react-native android为我们的一个创业项目写了一个demo，项目放在github了，有兴趣的朋友可以看看，下面给出一些效果： ###关于height 很多时候，这都是一个不折不扣的大坑，比方说当你Image加载一个网络图片时，如果你不给该Image设置width和height，那你将什么都看不到。这还不算，当你使用ListView时如果该组件同时存在一个兄弟元素，那么此时ListView必须设置height，否则你会发现它不再响应用户的滚动操作。。。 ###关于Fetch 当你试图提交附件表单数据的时候，请一定要使用FormData对象将数据包裹，这应该算不上什么太古怪的事儿～ 如果你只是普通的json提交，就按照官方例子来做： fetch(API_LOGIN_URL, { method: &apos;POST&apos;, headers: { &apos;Accept&apos;: &apos;application/json&apos;, &apos;Content-Type&apos;: &apos;application/json&apos;, }, body: JSON.stringify({ account: this.state.account, password: this.state.password, }) }) ..... 不然fetch会报错说你使用了不支持的body属性值类型～ 如果你和我一样需要实现类似用户头像修改的功能，你可能需要下面两个类库： react-native-image-picker：用于头像图片获取 react-native-fileupload：用于图片文件表单上传，这是由于你使用image-picker获取到的图片只是本地url，你需要这个uploader来组装成原始的file上传表单～～ 顺便说一下，第三方依赖更新较快，代码也可能有bug，各种bug，所以你最好主动的去github上和项目的维护人员沟通，一起想办法解决问题～～ ###关于async/await 至少在我目前的环境（默认安装react-native0.16.0自带的babel）是没有默认开启该特性的，不过貌似是可以手动开启的，有兴趣可以看这里。 ###关于第三方类库 个人感觉，目前很多关注度高的第三方库还是值得信赖的，不过面向android平台的库功能还不足够强大，这可能由于fb刚开源不久的缘故，但是关注度高的库的作者响应也一般都很快，这一点很值得赞许～ 但是，老实讲，如果你要开发的app包含大量特殊的业务，或者效果，这个时候你就只能自己去实现FB暂时没有提供的原生控件了，这就要求你同时要熟悉android原生开发，目前这个阶段，我觉得你是无法避免这种尴尬局面的。 ###响应屏幕尺寸的变化 FB提供了Dimensions组件可以用来获取屏幕的当前尺寸： var deviceHeight = Dimensions.get(&apos;window&apos;).height; var deviceWidth = Dimensions.get(&apos;window&apos;).width; 不过，由于设备可能旋转，所以如果你的app支持设备旋转的话，你就需要找到一种方法来实时获取当前设备尺寸，官方推荐的方法是每次render都要调用这个方法来刷新设备尺寸，而目前我还没有解决，主要是不知道应该如何获取屏幕旋转事件～～ ###style小知识 当你打算在某个控件上套用设置好的某个样式，但需要单独为其设置一个额外特殊值时，你可以这样： &lt;View style={[styles.demo, {backgroundColor: &quot;blue&quot;}]}&gt;&lt;/View&gt; 我也不懂，为何用数组类型的，但就是能这么写哟～ ###布局 布局和web比起来其实不算复杂，但你务必要掌握Flex布局，强烈推荐看这里。 ###TextInput 这里主要说的是键盘问题，当用户点击这个控件后，系统会弹出键盘，这个键盘会占据屏幕控件的，所以，你可以将整个表单View放在ScollView中！这样用户就可以在屏幕上进行滚动了，但注意，滚动的是整个表单View，而不是滚动的TextInput内容（当你设置为支持多行输入时），最后还有就是别忘了给ScollView添加keyboardDismissMode属性，如下： 该属性会在滚动触发后自动关闭键盘～～ 至于怎么滚动TextInput内部数据，目前没找到方法。 默认android系统的TextInput会有一个丑陋的下边框，style是无法去掉它的，不过你可以使用underlineColorAndroid属性将其设置为与背景色一样，来达到隐藏效果，这个时候你在外部包一个View来定义样式即可～～","tags":[{"name":"react-native","slug":"react-native","permalink":"https://blog.kazaff.me/tags/react-native/"},{"name":"android","slug":"android","permalink":"https://blog.kazaff.me/tags/android/"}]},{"title":"关于React-Native的预备知识","date":"2015-12-05T09:37:12.000Z","path":"2015/12/05/关于React-Native的预备知识/","text":"最近在修React-Native（简称RN），发现官方文档很多都看不太透彻，仔细想来，应该是基础知识不扎实导致的吧～所以花了一些时间来学习一些相关的内容，之前其实多少也有为RN做过准备，不过都是在JS方向的。不过，想使用RN开发完整的app还需要开发者了解原生开发的相关知识，但到底需要掌握到什么程度呢？这就要跟项目而定了！作为初学者的我，深刻的体会到，一些android基本概念是必不可少的，像Activity，UI布局，android事件机制等，虽说不需要你多熟练，但也绝对不能眼生啊，下面推荐一个系列教程，可以帮你无痛升级： http://www.imooc.com/learn/96 http://www.imooc.com/learn/107 http://www.imooc.com/learn/142 http://www.imooc.com/learn/179 基本上应该看完就能入门了吧，其实android开发感觉确实很容易入门的，只要别怕SDK量大，基本上都能比较顺利的升级为android开发小能手！ 当然，IOS也要如此，不过这条路线我暂时没有投入时间啦～远离是相通的～ 感觉RN的难点，多在于如何和native app融合和通信上，单纯的使用RN体验和React几乎一样，所以并不会成为你走向人生巅峰路上的绊脚石。而目前来说，不仅仅是android版，即便是早就开放的IOS版的RN，也并没有支持太多的原生模块和服务，所以想要使用RN搞定一个拿得出手的app，确实需要多修几条技能线，这也是很多人目前不推荐投入RN开发的一个主要观点。 不过，RN的开发模式是主流的，尤其是在试图解决移动端跨平台开发的工程领域，而且就我个人的观察，RN社区的热度在全球范围内都一直是屈指可数，每天都会产生很多的RN控件，不久的将来，可能对于我们一般玩家来说，你基本上可以通过RN就完成一个完整的app了！ 希望那一天早日到来！","tags":[{"name":"react-native","slug":"react-native","permalink":"https://blog.kazaff.me/tags/react-native/"},{"name":"android","slug":"android","permalink":"https://blog.kazaff.me/tags/android/"}]},{"title":"Android的浏览器下无法表单提交附件","date":"2015-12-04T09:37:12.000Z","path":"2015/12/04/android的浏览器下无法表单提交附件/","text":"今天刚发现一个bug，很小，但是很恶心：用android的内置浏览器无法上传表单附件。 gg了一下，发现这里讲的方法貌似可行： &lt;input type=&quot;file&quot; name=&quot;photo&quot; accept=&quot;image/*&quot; capture=&quot;camera&quot;&gt; 注意添加的那个属性：capture=”camera” 本以为大功告成，谁知道微信内部浏览器还是不行，妈蛋，一搜才知道，好多人都反应这个问题，从13年就有相关提问，我也是日了狗了，怎么还是没修复？而且android下的微信内置浏览器连个刷新功能都没有，这尼玛是不是在耍贱呢？ 好，那大家都是怎么解决微信下的这个问题呢？看到几个方案貌似大家比较赞同： 方案一：大概意思就是一旦判断出用户所在的是微信环境，就引导用户切换到系统浏览器下，这算是成本最低方案了。 方案二：使用的是微信提供的js-sdk，相当于让用户把图片先提交给微信服务器，然后在让自己系统的后台去下载，听起来就麻烦，而且貌似这个上传接口是需要去认证公众号，还有调用频次限制！ 我觉得吧，这就是作！微信中明明看到了其使用的内置浏览器是基于qq浏览器的，单独使用qq浏览器就可以上传附件，怎么就在微信里就不行？！如果是基于安全考虑的，那为啥ios的微信就可以上传图片呢？！ 好了不说了，睡觉……","tags":[{"name":"android","slug":"android","permalink":"https://blog.kazaff.me/tags/android/"},{"name":"微信浏览器","slug":"微信浏览器","permalink":"https://blog.kazaff.me/tags/微信浏览器/"},{"name":"附件上传","slug":"附件上传","permalink":"https://blog.kazaff.me/tags/附件上传/"}]},{"title":"Mac下react-Native-Android环境搭建","date":"2015-12-02T09:37:12.000Z","path":"2015/12/02/mac下react-native-android环境搭建/","text":"最近闲来无事，就决定摆弄摆弄react-native，其实之前尝试过，不过那个时候react-native还没有出android版，而ios版本的环境搭建只需要给xcode装上，基本上就算搭建完成了。而这次大意了，原以为android环境会一样简单明了的，随便看了几篇环境搭建的帖子就以为能轻松完成，结果一搞就搞了一天半啊～ 首先吐槽的就是，很多博客里写的安装步骤其实都不是很具体，总是缺少一些细节，对我这种小白来说就会变成噩梦！虽然我这篇可能也无法保证事无巨细，但至少我碰见的坑我会叮嘱一下大家撒～ 其实说了那么多，都怪自己太懒，按照官方提供的搭建流程，基本上就会规避非常多的坑，强烈推荐看官方手册： 步骤一 步骤二 下面主要来说说我自己碰见的坑吧，希望能帮助到你～ 官方推荐（很多博客也推荐），使用brew来安装android-sdk，但我一开始是从另外一个博客上看到的，自己去找了一个mac版本的android sdk manger，按说应该和brew安装的一样，只不过位置不同而已～可是后面却碰到了死活无法找到android sdk的路径问题。单这一个问题就让我花了半天来排查，主要是无法搜索到有用的资料！ 删除掉自己下载的那个manger，使用brew安装即可： brew install android-sdk 不过一定要按照顺序来，第一步肯定是要保证你的mac下已经安装了jdk，直接gg可以搜索到mac版的jdk安装文件的！brew貌似无法安装jdk～～ 第二个坑呢，就是这个“ANDROID_HOME”环境变量的问题了，由于第一步我们是使用brew来安装android-sdk的，所以安装位置在“/usr/local/opt/android-sdk”，官方有叮嘱这一步，我的步骤是： cd ~ touch .bash_profile vi .bash_profile 然后在打开的输入界面贴进去： export ANDROID_HOME=/usr/local/opt/android-sdk 保存退出，并关闭终端！！！ 第三个坑，是一定要安装最新版本的watchman和flow： brew install watchman brew install flow 不然你执行“react-native run-android”后，开启的node server终端总是会报错，提示什么”root of null”！我由于之前就装过watchman，可能是版本问题，又让我花了tm的多半天！！ 第四个坑，是当执行“react-native run-android”后，会下载大量的jar包，如果中间出现卡住了，可以直接ctrl+C，然后重新运行命令，要说的不是这个问题，当你下载完所有的包后，会开始部署代码，这个时候可能会碰到一个类似“xxx parent directory xxxx”无法创建的问题，应该是gradle无权限操作目录的问题，我尝试使用sudo来切换成root账号，结果会碰到由于切换了用户，之前设置的“ANDROID_HOME”环境变量未定义的问题，所以对于和我一样的小白来说，最简单的就是在你的项目文件上右键“简介”，弹出的窗口最下面调整一下操作权限，并记得继承到所有下级目录上！！！ 第五个坑，还是绕回来说android-sdk，安装完后执行： android 该命令会打开一个界面，按照官方说的安装所有的推荐工具包，强烈推荐选择统一的版本，比方说23.0.1，不然会碰到对应版本找不到的报错！我贴一下我这边目前选择的包情况： 未必都有用，但是至少不保错了！ 最后，对于跟我一样使用老mac的玩家，强烈推荐不要使用android-sdk自建模拟器，因为那真的实在是太tm的卡了！我足足等了30分钟，开启后还进到android系统后还是很卡～可能是我配置的不好吧，但这种方式的体验真的好差！ 按照官方推荐的，我们乖乖的来使用Genymotion，选择个人开发者，注册账号即可免费下载，然后再根据要求下载个最新版本的VirtualBox即可！配置非常简单，启动很迅速，效果好极了！但很多功能都是收费版的，不过对于尝鲜的我来说，足够了！ 这一趟下来，感觉好麻烦！还是react-native-ios爽一些，不过android还是不能错过的！！ 一切就绪，只需要run-android了！ 真机调试一样的简单，只需要插上数据线即可，当然，要记得把手机开启usb调试模式，我使用的是锤子坚果手机，连接后会弹出授权窗口，不确认会提示无法连接设备的哟～ 最后你还可能碰见无法加载jsbundle的问题，只需要摇晃手机，在弹出菜单中选择 Dev Setting &gt; Debug Server host for device，然后填入 Mac 的 IP 地址（ifconfig 命令可查看本机 IP）即可！ 参考：http://www.liaohuqiu.net/cn/posts/react-native-1/","tags":[{"name":"mac","slug":"mac","permalink":"https://blog.kazaff.me/tags/mac/"},{"name":"react-native","slug":"react-native","permalink":"https://blog.kazaff.me/tags/react-native/"},{"name":"android","slug":"android","permalink":"https://blog.kazaff.me/tags/android/"}]},{"title":"QQ第三方登录","date":"2015-11-25T09:37:12.000Z","path":"2015/11/25/qq第三方登录/","text":"昨天搞的是微信登录，今天我们要来搞一下qq登录！但作为过来人，我想假装平静的说出心声：为啥同一个公司的两个产品系，文档质量就他奶奶个抓儿差那么多呢？ 如果说昨天搞微信很嗨皮的话，那么今天搞qq真的想在吃翔，我这暴脾气，唉～ 当然，这篇文章并非只是来吐槽的。言归正传，整体思路还是和微信第三方登录一致，我就不细说了，有兴趣的朋友可以看之前的那篇文章。 我们接下来还是主要来关注qq登录接口不同于微信的地方！（文档，文档，渣文档就是最大的不同！妈蛋～） 先来看一下官方文档，如果你打开显示的是404错误，请多刷新几次，具体原因不清楚哇～ 文档一开始也正儿八经的给我们解释了解释OAuth，我们可以直接跳过啦～关键要注意的是它通过第1步得到的code换access_token的接口，也就是 “https://graph.qq.com/oauth2.0/token&quot; 这个接口，它并不是像微信那样走的rest风格的服务，而是又借住了一次重定向来完成的！ 这尼玛怎么搞的？这多出的一跳不仅让你蛋疼，而且也丢失了state参数，这让我们这种不依赖cookie来存sessionid的人肿么办？ 如果你是phper，那么curl的CURLOPT_FOLLOWLOCATION可以让你的请求跟踪这次重定向，但我用nodejs的restify相关实现是无法正常的接受重定向响应的！ 所以我只能自己解析响应头，取出这个url，当然我们就不用傻傻的请求这个url了，因为该url里就已经携带了我们需要的access_token！这也使得我们在获取Code时前台传递的用户会话id不会丢失，因为我们并没有跳走嘛～～ 然后我们再来看看关于获取用户openid的操作（https://graph.qq.com/oauth2.0/me），该接口竟然是根据jsonp机制返回的，这尼玛是多久前的啊？ 所以我们需要自己去解析返回内容，这里我需要特别叮嘱的是，js平台（包括nodejs）下尽量避免使用eval函数，还是老老实实的正则匹配吧，不然，你懂的！ 代码我已发到gihub上，有兴趣的童鞋不妨去看看～","tags":[{"name":"QQ","slug":"QQ","permalink":"https://blog.kazaff.me/tags/QQ/"},{"name":"OAuth","slug":"OAuth","permalink":"https://blog.kazaff.me/tags/OAuth/"},{"name":"第三方登录","slug":"第三方登录","permalink":"https://blog.kazaff.me/tags/第三方登录/"}]},{"title":"微信第三方登录","date":"2015-11-24T09:37:12.000Z","path":"2015/11/24/微信第三方登录/","text":"最近工作需要，运营想实现一个功能，让网站用户可以通过微信进行登录。这在社交媒体如火如荼的今天，再常见不过了。 提到第三方登录的技术范畴，就不得不提OAuth，要说这个OAuth，我就不啰嗦了，还是推荐大家直接看阮老师的理解OAuth 2.0，没错，这个协议已经是2.0版本了！ 不是太复杂，对吧？了解了理论姿势，我们再来结合实际场景完成功能，首先要知道，我们的场景不是要自己实现这个OAuth协议，而是根据第三方平台（这里特指微信）来完成我们的需求。 一直没写网站类型的系统，所以一直也没怎么去和常用的第三方系统对接，懒嘛，时间多花在打电动上了……不过其实知道并不复杂，我们来先梳理一下要用到的技术： ajax（看具体场景了，我所处理的系统需要） json OAuth2.0 大概就这些吧，前两个基本上是web2.0以后的必备知识，而第三个我们刚才也介绍过了，那么就开始动手吧！ 哦～稍等一下，在写代码之前，我们还没看微信接口的规则呢，说到这个，我这个小白就得叮嘱一下大家了，微信平台分2个：公众号平台和开放平台。前者多用来结合公众号后台来完成更加复杂的业务逻辑的，当然也能解决我们今天讨论的这个登录问题，但后者更适合我们！ 顺便说一下，要使用微信的接口是要申请的，申请是要审核的，审核是要收费的！目前应该是申请一次300块，申请所需要的手续和申请支付宝差不多，就是一些备案号啊，公司执照啊等等吧，可以访问这里。 目前微信开放平台针对网页提供的就只有用于第三方登录的接口（其实也就是获取用户信息的接口），相关接口文档可以访问这里。 我大概描述一下我们的业务流程走向： 用户登录网站，并进行微信账号绑定； 点击绑定后浏览器跳转到微信鉴权页面，用户完成授权； 微信平台将用户浏览器重定向到我们指定的地址，该地址作用是将当前用户id和该用户微信的openid（或unionid）建立映射关系； 用户再次打算登录网站时，点击微信登录按钮； 浏览器跳转到微信二维码页面，用户扫描确认后，浏览器跳转到我们指定的页面； 该页面根据用户微信的openid查找到对应的网站用户id，并完成session的创建； 引导用户浏览器跳转到网站首页，此时用户应该处于登录状态。 该流程很简单，具体你的项目是使用什么技术栈，都可以根据以上流程完成微信同步登录。有童鞋可能注意到，我们并没有按照官方说的那样去持久化access_token，是的，因为目前我们并没有必要持久化它，更不需要为了避免超时而去续期，因为我们只是一次性获取用户的openid而已。 那什么时候我们需要保存access_token呢？当你的网站除了登录外，还想让更深度的绑定微信账号时才需要（例如支付，当然你可能需要开通微信公众号平台喽～）。 我的场景由于我的网站是完全基于ajax的，前后台仅仅靠api来通信，所以和微信官方的例子还不完全一样，下面我来说一下这种场景下的注意细节。 请允许我先介绍一下我的场景： 富客户端，前台完全负责页面的路由 所有前后端通信按照Restful标准 会话id使用自定义请求头（token）来进行传递 满足以上条件的系统，通常是使用了像angularjs，emberjs或reactjs等前端mv*框架技术的。 出于各种原因，我们上面描述的流程中，有一些是被影响的。我们来具体说说吧～ 首先看第2步（点击绑定后浏览器跳转到微信鉴权页面，用户完成授权），由于这次浏览器跳转会将控制权交给微信平台，我们的前台路由不能插手，所以呢，我们需要传递足够的参数过去。微信接口提供了一个参数state，刚好可以让我们来使用，因为该参数微信会原样返回给我们指定的回调页面，为什么这一点很重要呢？ 第3步（微信平台将用户浏览器重定向到我们指定的地址，该地址作用是将当前用户id和该用户微信的openid（或unionid）建立映射关系）中，其实我们省略了一个子流程，根据OAuth规范，微信回调到我们的指定地址后，只传递了Code，而这个Code并不能用来换取用户的openid，我们还需要在该地址的处理逻辑中再次请求微信平台换取access_token，这次请求要携带一个appsecret参数，该参数不应当泄漏给其他人，所以注定我们的这个地址是一个不在客户浏览器中执行的脚本，也就是通俗的后台代码（可以是php，是java，是nodejs）。 所以呢，当我们的这个后台脚本顺利拿到了用户的openid后呢，它又该怎么知道用户的系统id呢？有人说很简单啊，直接读取cookie即可，我前面说了，我们没有使用cookie来存sessionid，我们使用的是自定义请求头，而微信回跳时肯定是不会携带我们自定义的请求头的，所以前面说的那个state参数就很有意义哒。 同样，我们持久化用户的系统id和openid是需要使用数据库的，你当然不会希望将数据库密码放在前台代码里吧？哇哈哈～～别告诉我你是这么做的！因此，微信回调的地址一定是后台脚本～ 同理，第6步依然是一个后台脚本，它创建好会话id后，需要将用户的浏览器重定向到前台系统页面，相当于把控制权交还给了我们的前台系统。接下来就是在我们的富客户端路由控制下完成用户的页面路由了。 还有一个细节，用户绑定过微信账号后，你网站的某个页面肯定还要展示这种绑定关系，之所以提这点，是想让你知道，用户系统id和openid的关系映射方式还是需要斟酌一下的，因为我们需要两种方向的查找： uid -&gt; openid：用于绑定 openid -&gt; uid：用于登录 如果你使用的是nosql（redis），你可能需要两组kv才能存下这两组数据。同时，你可能还需要考虑其他平台的账号绑定～～所以，你还是先好好考虑考虑吧～ 实际开发时你还需要考虑到足够多的异常系处理，毕竟在整个流程中存在多次跳转，多次查询，要考虑每次的出错场景。不说了，说多了都是泪！ 代码我已发到gihub上，有兴趣的童鞋不妨去看看～","tags":[{"name":"OAuth","slug":"OAuth","permalink":"https://blog.kazaff.me/tags/OAuth/"},{"name":"第三方登录","slug":"第三方登录","permalink":"https://blog.kazaff.me/tags/第三方登录/"},{"name":"微信","slug":"微信","permalink":"https://blog.kazaff.me/tags/微信/"}]},{"title":"React写小项目后感","date":"2015-11-16T09:37:12.000Z","path":"2015/11/16/react写小项目后感/","text":"用react差不多三个月了吧，断断续续的写了几个小项目，感觉美美哒。作为自我总结，也为其他新人指南，特此发一篇。 ####首先 你得搞清楚，你处在的时代。别给我说你没听过JS，也别说jQuery不熟，对了，还有ajax。这些已经算是这个时代前端开发人员的基本功了。 当然，仅仅会了这些，还差得远。各种css库，JS库和框架，ajax库，你多少也得略知一二吧，搞清楚什么叫浏览器兼容性，什么叫事件，什么是回调，还有响应布局啊乱七八糟的。 哎哟不错，掌握了这些小九九，你体内的查克拉就差不多了。接下来要面对的，就是前端工程化问题。说白点，就是你可能不光要能做出来，还得做的高效。 ####新概念 前端技术这些年真心不让人省心，几乎每时每刻都可能会发生点改变世界的事儿。你可以看一下github上的js项目，当然还有npm里。说到npm，自然又会扯出nodejs。唉，心塞啊。好像自己这几年积累的开发经验，都是围绕着js打转转。 那这些就是新概念么？其实也不算，css3，html5，es6，es7，这些概念有的已经快好几年了，至今仍未彻底落地。但身为前端，你却不能不去掌握它们，因为你会从中感受到优雅，感受到强大。 ####react 各大技术社区，充斥着react的身影，这也让facebook开源套件获得了越来越高的曝光率，react生态系统以迅雷不及掩耳盗铃之势，超越了神圣的angularJS，这让我这个老ng粉儿感叹世事无常啊。 不过react却真如它所说，确实以新的理念为开发者带来了大道至简的哲学。至少对我来讲，曾经头疼于如何给团队讲解ioc，讲指令，现在这些复杂概念不需要了（对于新手来讲）。开发人员只需要对组件，模块有些许的认知，就可以动手干了！ react组件的生命周期，是每一个使用者无论如何都一定要花时间提前掌握的，不然别说你懂react，最好都别说你听说过它！ ####工程化 前端工程化的概念，随着一大堆构建工具，也慢慢的被大家所认可，BAE的前端研发团队各自有响应的一套解决方案。当然，开源界在这一方面一点也不含糊，前端构建工具多的你都来不及用，可能它就已经消失了。 react的世界里，有webpack，有babel，这些林林总总的强大猛兽，让开发者战无不胜。你还在等什么？ ####移动 我并不会开发移动端app，这一点也已经给我带来了感受得到的机会流逝，但我骨子里却无法磨灭一个想法，我一直非常的看好web，十几年前，软件都是安装在pc上的，就好像今天安装在手机中的app一样。但，这不是常态，web才是互联网的常态，我相信不出几年，app的概念也会不见，web大统！ 这也是像google这样的科技巨头一直在努力的方向。 ####落地 扯蛋时间结束了，说点干货。 react只是一个近乎完美的view，它自己也这么说。你肯定还需要点儿别的，我之前的文章也反复强调过这一点了。相信你很容易就能找到各种缺少的组件，当然你也可以尝试一下我找到的一个脚手架：react_scaffolding。 它已经把react，react-router，redux，immutable，superagent等组装好了，开箱即用。当然，并不能说这就是最佳组合了，我相信还有更好的。 支持react的ui也越来越多，像之前推荐的妹子ui，还有阿里蚂蚁开发的ant，都非常优秀。但这里需要叮嘱的是，随着react0.14+的发布，独立了dom相关的操作，这让部分ui库不再可用，选择的时候一定要注意啊。 另外，es6的class语法，并不支持react曾经推从的mixins哲学，当然，你依然可以用es5的语法来使用mixins，这一点也不难。 组件之前的通信方式也有一大堆，但有时候你还是不得不跳出react，不过没关系，CustomEvent应该可以帮到你。 我还想说，一直都挺希望路由配置可以去中心化，这一点react-router也可以很好的帮你搞定。 其实，上面推荐的那个脚手架，在实际项目开发时，还是无法避免大量的样板代码，比方说你写reducer和api的地方，如果能利用代理或工厂模式动态创建相关逻辑，那也是极好的。 最后，思考了十分钟，确实想不到还要说点啥了，那就到此为止吧。","tags":[{"name":"react","slug":"react","permalink":"https://blog.kazaff.me/tags/react/"}]},{"title":"React-Router的组件生命周期","date":"2015-10-24T09:37:12.000Z","path":"2015/10/24/react-router的组件生命周期/","text":"昨天碰到个问题，GG了半天也没有发现一篇对应主题的文章，最后还是在react-router的github官方求助，才被热心的大牛给上了一课，其实这也都怪自己没有耐心仔细阅读react-router的官方资料，下面就来简单汉化一下官方针对这个问题的相关解释。 以下内容翻译自官方文档，原文链接：https://github.com/rackt/react-router/blob/master/docs/guides/advanced/ComponentLifecycle.md ##Component Lifecycle 理解router在其生命周中会触发哪些hooks调用对于实现你的应用场景非常重要，常见的场景是何时抓取数据。 在router中组件的生命周期和react中定义的并没有什么不同，让我们来假设一个场景，其route配置如下： &lt;Route path=&quot;/&quot; component={App}&gt; &lt;IndexRoute component={Home}/&gt; &lt;Route path=&quot;invoices/:invoiceId&quot; component={Invoice}/&gt; &lt;Route path=&quot;accounts/:accountId&quot; component={Account}/&gt; &lt;/Route&gt; ###Lifecycle hooks when routing 我们假设用户访问应用的”/“页面。 | 组件 | 生命周期内的hooks调用 | |———–|————————| | App | componentDidMount | | Home | componentDidMount | | Invoice | N/A | | Account | N/A | 然后，用户从”/“页面跳转到”/invoice/123” | 组件 | 生命周期内的hooks调用 | |———–|————————| | App | componentWillReceiveProps, componentDidUpdate | | Home | componentWillUnmount | | Invoice | componentDidMount | | Account | N/A | App组件由于之前已经被渲染过，但是由于将会从router中接受到新的props（children，params，location等），所以会调用其componentWillReceiveProps和componentDidUpdate方法 Home组件此时已经不再需要了，所以它会被卸载 Invoice组件此时将会被首次渲染 再然后，用户从”/invoice/123”页面跳转到”/invoice/789” | 组件 | 生命周期内的hooks调用 | |———–|————————| | App | componentWillReceiveProps, componentDidUpdate | | Home | N/A | | Invoice | componentWillReceiveProps, componentDidUpdate | | Account | N/A | 这次所有需要的组件之前都已经被加载了，此时它们都会接收到来自router传递的新props。 最后，用户从”/invoice/789”页面跳转访问”/accounts/123” | 组件 | 生命周期内的hooks调用 | |———–|————————| | App | componentWillReceiveProps, componentDidUpdate | | Home | N/A | | Invoice | componentWillUnmount | | Account | componentDidMount | ###Fetching Data 在使用router时有很多种方法来获取数据，其中最简单的一种方式就是利用组件的生命周期hooks，并保存数据在组件的state中（译者：这一点在Redux设计理念中被强烈反对，事实证明即便是不依赖state，这种方式照样可行，我们后面会给出例子）。我们已经知道了当router切换时组件的生命周期hooks调用过程，我们来为之前的Invoice组件来实现一个简单的获取数据逻辑。 let Invoice = React.createClass({ getInitialState () { return { invoice: null } }, componentDidMount () { // fetch data initially in scenario 2 from above this.fetchInvoice() }, componentDidUpdate (prevProps) { // respond to parameter change in scenario 3 let oldId = prevProps.params.invoiceId let newId = this.props.params.invoiceId if (newId !== oldId) this.fetchInvoice() }, componentWillUnmount () { // allows us to ignore an inflight request in scenario 4 this.ignoreLastFetch = true }, fetchInvoice () { let url = `/api/invoices/${this.props.params.invoiceId}` this.request = fetch(url, (err, data) =&gt; { if (!this.ignoreLastFetch) this.setState({ invoice: data.invoice }) }) }, render () { return &lt;InvoiceView invoice={this.state.invoice}/&gt; } }) ##译者总结 在本篇开头我的那篇问题贴中已描述场景，由于我一直以来对react的生命周期hooks错误的理解，导致死活无法优雅的实现兼容浏览器前进后退操作的组件。经过上面内容的学习，一切都不在话下。 首先，我之前总是习惯使用componentWillMount方法来初始化组件的状态，事实证明这种写法并非适用于所有场景（尽管使用它也可以，但至少不是更多人推荐的）。其次，尽可能避免让组件包含太多会话状态，保证组件的纯净。 在我的代码中，更改后的完美版本如下： （算了，想了想这部分代码的附加逻辑太多，不适合用来阐述上述观点，还是不贴了～）","tags":[{"name":"react","slug":"react","permalink":"https://blog.kazaff.me/tags/react/"},{"name":"react-router","slug":"react-router","permalink":"https://blog.kazaff.me/tags/react-router/"},{"name":"lifecycle","slug":"lifecycle","permalink":"https://blog.kazaff.me/tags/lifecycle/"},{"name":"组件生命周期","slug":"组件生命周期","permalink":"https://blog.kazaff.me/tags/组件生命周期/"},{"name":"componentDidUpdate","slug":"componentDidUpdate","permalink":"https://blog.kazaff.me/tags/componentDidUpdate/"}]},{"title":"Webpack科普","date":"2015-10-14T09:37:12.000Z","path":"2015/10/14/Webpack科普/","text":"到底什么是webpack。 一图蔽之，这就是它。我贴一些找到的感觉还不错的文章来加速大家的了解： 深入浅出React（二）：React开发神器Webpack Webpack 怎么用 webpack和react小书 Webpack 性能优化 （一） webpack文档中文版","tags":[{"name":"webpack","slug":"webpack","permalink":"https://blog.kazaff.me/tags/webpack/"}]},{"title":"［译]深入浅出Redux中间件","date":"2015-10-09T09:37:12.000Z","path":"2015/10/09/译-Redux中间件深入浅出/","text":"原文地址：https://medium.com/@meagle/understanding-87566abcfb7a 从2014年二月开始我就在使用facebook的React了，那时候它的版本才v0.9.x。我是从Backbone.js转过来的，当哥了解到Flux的好处和它提倡的单数据流架构时，我就已经决定不在使用Backbone.js提供的models和集合。这里我假设你已经大概了解了Flux，Redux，React和函数式编程的基本知识，在这篇文章中我将尽可能的涵盖所有内容。Dan的Redux库真的非常简单和出彩儿，我相信每个人都可以借助它开发炫酷屌炸天的前端web应用。但我更想了解一下它的内部实现。我开始了Redux源码的探索，要想能更好的理解源码，你必须非常熟悉函数式编程理念。它的代码写的非常简练，没有扎实的基本功你会觉得非常难看懂。我将尝试带领你了解一些需要用到的函数式编程概念，包括：复合函数，柯里化，和高阶函数等。 我鼓励你去阅读源码来搞明白reducer函数是如何创建和组合的，action creator函数是如何作用于dispatch方法的，还有如何增加中间件来影响默认的dispatch方法的。这篇文章将会帮你理解一些Redux框架的源码，提升你的函数式编程能力，同时也让你尝试一下部分ES6语法。 ###中间件 Redux最有趣的一个概念是它允许你通过自定义的中间件来影响你store的dispatch逻辑。然而，当我第一次试图查看applyMiddleware.js源码时，可把本宝宝吓坏了。所以我只能尝试参照Redux中间件的文档从功能层面来分析代码。看了下面这段话，让我找回了面子： “中间件”这个词听起来很恐怖，但它实际一点都不难。想更好的了解中间件的方法就是看一下那些已经实现了的中间件是怎么工作的，然后尝试自己写一个。函数嵌套写法看起来很恐怖，但是大多数你能找到的中间件，代码都不超过十行，但是它们的强大来自于它们的可嵌套组合性。 区区10行的中间件很容易写，但是你要想明白它们是如何放入中间件调用链，又是如何影响store的diapatch方法的，还真需要一些经验。首先让我们来简单定义一下中间件到底是个啥，并且找一些简单的中间件看一下它们的具体实现方式。关于中间件的定义我能找到的最简单的描述是： 中间件主要被用于分离那些不属于你应用的核心业务逻辑的可被组合起来使用的代码。 听起来不算太复杂，对吧;) 如果你之前用过Koa.jsn，那你可能早就接触过中间件这个概念了。我最早使用中间件是在我用Java Servlet写Filters和用Ruby写类似认证授权，日志，性能监控或一些需要在业务逻辑执行前处理的模块。 Redux的中间件主要用在store的dispatch函数上。dispatch函数的作用是发送actions给一个或多个reducer来影响应用状态的。中间件可以增强默认的dispatch函数，我们来看一下Redux1.0.1版本的applyMiddleware源码： export default function applyMiddleware(...middlewares) { return (next) =&gt; (reducer, initialState) =&gt; { var store = next(reducer, initialState); var dispatch = store.dispatch; var chain = []; var middlewareAPI = { getState: store.getState, dispatch: (action) =&gt; dispatch(action) }; chain = middlewares.map(middleware =&gt; middleware(middlewareAPI)); dispatch = compose(...chain, store.dispatch); return { ...store, dispatch }; }; } 就这么点儿代码，就使用了非常多的函数式编程的思想，包括：高阶函数，复合函数，柯里化和ES6语法。一开始我反复读了十遍，然后我疯了:)。让我们先来分别看一些函数式编程的知识，回头再理会上面这段让人不爽的代码。 ###函数式编程概念 在开始阅读Redux中间件源码之前，你需要先掌握一些函数式编程知识，如果你已经自学成才了请跳过本节。 ####复合函数 函数式编程是非常理论和非常数学化的。用数学的视角来解释复合函数，如下： given: f(x) = x^2 + 3x + 1 g(x) = 2x then: (f ∘ g)(x) = f(g(x)) = f(2x) = 4x^2 + 6x + 1 你可以将上面的方式扩展到组合两个或更多个函数这都是可以的。我们再来看个例子，演示组合两个函数并返回一个新的函数： var greet = function(x) { return `Hello, ${ x }` }; var emote = function(x) { return `${x} :)` }; var compose = function(f, g) { return function(x) { return f(g(x)); } } var happyGreeting = compose(greet, emote); // happyGreeting(“Mark”) -&gt; Hello, Mark :) 我们当然可以组合更多的函数，这里只是给大家简单阐述一下基础概念。更多的常用技巧我们我们可以在Redux的源码中看到。 ####柯里化 另一个屌炸天的函数式编程概念被称之为：柯里化。（译者注：这段翻译起来太难了，我放弃了，推荐你看这里来了解这个概念） var curriedAdd = function(a) { return function(b) { return a + b; }; }; var addTen = curriedAdd(10); addTen(10); //20 通过柯里化来组合你的函数，你可以创建一个强大的数据处理管道。 ###Redux的Dispatch函数 Redux的Store有一个dispatch函数，它关注你的应用实现的业务逻辑。你可以用它指派actions到你定义的reducer函数，用以更新你的应用状态。Redux的reducer函数接受一个当前状态参数和一个action参数，并返回一个新的状态对象： reducer:: state -&gt; action -&gt; state 指派action很像发送消息，如果我们假设要从一个列表中删除某个元素，action结构一般如下： {type: types.DELETE_ITEM, id: 1} store会指派这个action对象到它所拥有的所有reducer函数来影响应用的状态，然而只有关注删除逻辑的reducer会真的修改状态。在此期间没人会关注到底是谁修改了状态，花了多长时间，或者记录一下变更前后的状态数据镜像。这些非核心关注点都可以交给中间件来完成。 ###Redux Middleware Redux中间件被设计成可组合的，会在dispatch方法之前调用的函数。让我们来创建一个简单的日志中间件，它会简单的输出dispatch前后的应用状态。Redux中间件的签名如下： middleware:: next -&gt; action -&gt; retVal 我们的logger中间件实现如下： export default function createLogger({ getState }) { return (next) =&gt; (action) =&gt; { const console = window.console; const prevState = getState(); const returnValue = next(action); const nextState = getState(); const actionType = String(action.type); const message = `action ${actionType}`; console.log(`%c prev state`, `color: #9E9E9E`, prevState); console.log(`%c action`, `color: #03A9F4`, action); console.log(`%c next state`, `color: #4CAF50`, nextState); return returnValue; }; } 注意，我们的createLogger接受的getState方法是由applyMiddleware.js注入进来的。使用它可以在内部的闭包中得到应用的当前状态。最后我们返回调用next创建的函数作为结果。next方法用于维护中间件调用链和dispatch，它返回一个接受action对象的柯里化函数，接受的action对象可以在中间件中被修改，再传递给下一个被调用的中间件，最终dispatch会使用中间件修改后的action来执行。 更健壮的logger中间件实现可以看这里，为了节省时间，我们的logger中间件实现非常的简陋。 我们来看看上面的logger中间件的业务流程： 得到当前的应用状态； 将action指派给下一个中间件； 调用链下游的中间件全部被执行； store中的匹配reducer被执行； 此时得到新的应用状态。 我们这里来看一个拥有2个中间件组件的例子： ###剖析applyMiddleware.js 现在咱们已经知道Redux中间件是个啥，并且也掌握了足够的函数式编程知识，那就让我们再次尝试阅读applyMiddleware.js的源码来探个究竟吧。这次感觉比较好理解了吧： export default function applyMiddleware(...middlewares) { return (next) =&gt; (reducer, initialState) =&gt; { var store = next(reducer, initialState); var dispatch = store.dispatch; var chain = []; var middlewareAPI = { getState: store.getState, dispatch: (action) =&gt; dispatch(action) }; chain = middlewares.map(middleware =&gt; middleware(middlewareAPI)); dispatch = compose(...chain, store.dispatch); return { ...store, dispatch }; }; } applyMiddleware可能应该起一个更好一点的名字，谁能告诉我这是为谁来“申请中间件”？我觉得可以这么叫：applyMiddlewareToStore，这样是不是更加明确一些？ 我们来一行一行分析代码，首先我们看方法签名： export default function applyMiddleware(...middlewares) 注意这里有个很有趣的写法，参数：...middlewares，这么定义允许我们调用时传入任意个数的中间件函数作为参数。接下来函数将返回一个接受next作为参数的函数： return (next) =&gt; (reducer, initialState) =&gt; {...} next参数是一个被用来创建store的函数，你可以看一下createStore.js源码的实现细节。最后这个函数返回一个类似createStore的函数，不同的是它包含一个由中间件加工过的dispatch实现。 接下来我们通过调用next拿到store对象（译者注：”Next we assign the store implementation to the function responsible for creating the new store (again see createStore.js). “这句实在翻译不来～）。我们用一个变量保存原始的dispatch函数，最后我们声明一个数组来存储我们创建的中间件链： var store = next(reducer, initialState); var dispatch = store.dispatch; var chain = []; 接下来的代码将getState和调用原始的dispatch函数注入给所有的中间件： var middlewareAPI = { getState: store.getState, dispatch: (action) =&gt; dispatch(action) }; chain = middlewares.map(middleware =&gt; middleware(middlewareAPI)); 然后我们根据中间件链创建一个加工过的dispatch实现： dispatch = compose(...chain, store.dispatch); 最tm精妙的地方就是上面这行，Redux提供的compose工具函数组合了我们的中间件链，compose实现如下： export default function compose(...funcs) { return funcs.reduceRight((composed, f) =&gt; f(composed)); } 碉堡了！上面的代码展示了中间件调用链是如何创建出来的。中间件调用链的顺序很重要，调用链类似下面这样： middlewareI(middlewareJ(middlewareK(store.dispatch)))(action) 现在知道为啥我们要掌握复合函数和柯里化概念了吧？最后我们只需要将新的store和调整过的dispatch函数返回即可： return { ...store, dispatch }; 上面这种写法意思是返回一个对象，该对象拥有store的所有属性，并增加一个dispatch函数属性，store里自带的那个原始dispatch函数会被覆盖。这种写法会被Babel转化成： return _extends({}, store, { dispatch: _dispatch }); 现在让我们将我们的logger中间件注入到dispatch中： import { createStore, applyMiddleware } from ‘redux’; import loggerMiddleware from ‘logger’; import rootReducer from ‘../reducers’; const createStoreWithMiddleware = applyMiddleware(loggerMiddleware)(createStore); export default function configureStore(initialState) { return createStoreWithMiddleware(rootReducer, initialState); } const store = configureStore(); ###异步中间件 我们已经会写基础的中间件了，我们就要玩儿点高深得了，整个能处理异步action的中间件咋样？让我们来看一下redux-thunk的更多细节。我们假设有一个包含异步请求的action，如下： function fetchQuote(symbol) { requestQuote(symbol); return fetch(`http://www.google.com/finance/info?q=${symbol}`) .then(req =&gt; req.json()) .then(json =&gt; showCurrentQuote(symbol, json)); } 上面代码并没有明显的调用dispatch来分派一个返回promise的action，我们需要使用redux-thunk中间件来延迟dipatch的执行： function fetchQuote(symbol) { return dispatch =&gt; { dispatch(requestQuote(symbol)); return fetch(`http://www.google.com/finance/info?q=${symbol}`) .then(req =&gt; req.json()) .then(json =&gt; dispatch(showCurrentQuote(symbol, json))); } } 注意这里的dipatch和getState是由applyMiddleware函数注入进来的。现在我们就可以分派最终得到的action对象到store的reducers了。下面是类似redux-thunk的实现： export default function thunkMiddleware({ dispatch, getState }) { return next =&gt; action =&gt; typeof action === ‘function’ ? action(dispatch, getState) : next(action); } 这个和你之前看到的中间件没什么太大不同。如果得到的action是个函数，就用dispatch和getState当作参数来调用它，否则就直接分派给store。你可以看一下Redux提供的更详细的异步示例。另外还有一个支持promises的中间件是redux-promise。我觉得选择使用哪个中间件可以根据性能来考量。 ###接下来干啥 你现在已经知道Redux的核心代码，你现在可以尝试使用react-redux来把Redux整合到你的react项目中了。 如果你还是对函数式编程不太习惯，我鼓励你看一下来自Brian Lonsdorf的优秀文献： Hey Underscore, You are Doing it Wrong! ：它介绍了很多函数式编程知识和少量的Underscore内容； Professor Frisby’s Mostly Adequate Guide to Functional Programming ###总结 希望你已经了解了关于Redux中间件的足够信息，我也希望你掌握了更多的关于函数式编程的知识。我不断的尝试更多更好的函数式编程方法，尽管一开始并不容易，你需要不断的学习和尝试来参悟它的精髓。如果你完全掌握了这篇文章交给你的，那么你已经拥有了足够的信心去投入更多的学习当中。 最后，千万别使用那些你还没有搞明白的第三方类库，你必须确定它会给你的项目带来好处。掌握它的一个好方法就是去阅读它的源码，你将会学到新的编程技术，淘汰那些老的解决方案。将一个工具引入你的项目前，你有责任搞清楚它的细节。","tags":[{"name":"react","slug":"react","permalink":"https://blog.kazaff.me/tags/react/"},{"name":"Redux middleware","slug":"Redux-middleware","permalink":"https://blog.kazaff.me/tags/Redux-middleware/"},{"name":"函数式编程","slug":"函数式编程","permalink":"https://blog.kazaff.me/tags/函数式编程/"},{"name":"柯里化","slug":"柯里化","permalink":"https://blog.kazaff.me/tags/柯里化/"},{"name":"复合函数","slug":"复合函数","permalink":"https://blog.kazaff.me/tags/复合函数/"},{"name":"currying","slug":"currying","permalink":"https://blog.kazaff.me/tags/currying/"},{"name":"redux-thunk","slug":"redux-thunk","permalink":"https://blog.kazaff.me/tags/redux-thunk/"}]},{"title":"［译]全栈Redux实战","date":"2015-10-08T09:37:12.000Z","path":"2015/10/08/译-全栈Redux实战/","text":"本文乱译自一篇英文博文（Full-Stack Redux Tutorial），本人英语能力不足，技术能力有限，如有错误，多多包涵。 #关于Redux+React+Immutable的测试先行开发综合指南 Redux是最近发生在js界令人兴奋的事儿。它把众多优秀的库和框架中非常正确的特性保留了下来：简单且可预测的模型，强调函数式编程和不可变数据，基于api的轻量级实现……你还有什么理由不喜欢呢？ Redux是一个非常小的代码库，掌握它所有的api并不困难，但对很多同学来讲，它要求的：创建组件（blocks），自满足的纯函数和不可变数据会带来不少别扭，那到底应该怎么办呢？ 这篇文章将会带你创建一个全栈的Redux和Immutable-js应用。我们将详细讲解创建该应用的Node+Redu后端和React+Redux前端的所有步骤。本指南将使用ES6,Babel,Socket.io,Webpack和Mocha。这是一个非常令人着迷的技术栈选型，你肯定不及待的想要开始了。 ##目录（不翻译） 你需要准备什么这篇文章需要读者具备开发js应用的能力，我们讲使用Node，ES6，React，Webpack，和Babel，所以你最好能了解这些工具，这样你才不会掉队。 在上面提到的工具集中，你需要安装Node和NPM，和一款你喜欢的编辑器。 ##应用 我们将要开发一款应用，它用来为聚会，会议，集会等用户群提供实时投票功能。 这个点子来自于现实中我们经常需要为电影，音乐，编程语言等进行投票。该应用将所有选项两两分组，这样用户可以根据喜好进行二选一，最终拿到最佳结果。 举个例子，这里拿Danny Boyle电影做例子来发起投票： 这个应用有两类独立的界面：用于投票的移动端界面，用于其它功能的浏览器界面。投票结果界面设计成有利于幻灯片或其它更大尺寸的屏幕显示，它用来展示投票的实时结果。 ##架构 该系统应该有2部分组成：浏览器端我们使用React来提供用户界面，服务端我们使用Node来处理投票逻辑。两端通信我们选择使用WebSockets。 我们将使用Redux来组织前后端的应用代码。我们将使用Immutable数据结构来处理应用的state。 虽然我们的前后端存在许多相似性，例如都使用Redux。但是它们之间并没有什么可复用代码。这更像一个分布式系统，靠传递消息进行通信。 ##服务端应用 我们先来实现Node应用，这有助于我们专注于核心业务逻辑，而不是过早的被界面干扰。 实现服务端应用，我们需要先了解Redux和Immutable，并且明白它们如何协作。Redux常常被用在React开发中，但它并不限制于此。我们这里就要学习让Redux如何在其它场景下使用。 我推荐大家跟着我们的指导一起写出一个应用，但你也可以直接从github上下载代码。 ###设计应用的状态树（State Tree） 设计一个Redux应用往往从思考应用的状态树数据结构开始，它是用来描述你的应用在任何时间点下状态的数据结构。 任何的框架和架构都包含状态。在Ember和Backbone框架里，状态就是模型（Models）。在Anglar中，状态常常用Factories和Services来管理。而在大多数Flux实现中，常常用Stores来负责状态。那Redux又和它们有哪些不同之处呢？ 最大的不同之处是，在Redux中，应用的状态是全部存在一个单一的树结构中的。换句话说，应用的所有状态信息都存储在这个包含map和array的数据结构中。 这么做很有意义，我们马上就会感受到。最重要的一点是，这么做迫使你将应用的行为和状态隔离开来。状态就是纯数据，它不包含任何方法或函数。 这么做听起来存在局限，特别是你刚刚从面向对象思想背景下转到Redux。但这确实是一种解放，因为这么做将使你专注于数据自身。如果你花一些时间来设计你的应用状态，其它环节将水到渠成。 这并不是说你总应该一上来就设计你的实体状态树然后再做其它部分。通常你最终会同时考虑应用的所有方面。然而，我发现当你想到一个点子时，在写代码前先思考在不同解决方案下状态树的结构会非常有帮助。 所以，让我们先看看我们的投票应用的状态树应该是什么样的。应用的目标是可以针对多个选项进行投票，那么符合直觉的一种初始化状态应该是包含要被投票的选项集合，我们称之为条目[entries]： 当投票开始，还必须定位哪些选项是当前项。所以我们可能还需要一个vote条目，它用来存储当前投票的数据对，投票项应该是来自entries中的： 除此之外，投票的计数也应该被保存起来： 每次用户进行二选一后，未被选择的那项直接丢弃，被选择的条目重新放回entries的末尾，然后从entries头部选择下一对投票项： 我们可以想象一下，这么周而复始的投票，最终将会得到一个结果，投票也就结束了： 如此设计看起来是合情合理的。针对上面的场景存在很多不同的设计，我们当前的做法也可能不是最佳的，但我们暂时就先这么定吧，足够我们进行下一步了。最重要的是我们在没有写任何代码的前提下已经从最初的点子过渡到确定了应用的具体功能。 ###项目安排 是时候开始脏活累活了。开始之前，我们先创建一个项目目录： mkdir voting-server cd voting-server npm init #所有提示问题直接敲回车即可 初始化完毕后，我们的项目目录下将会只存在一个package.json文件。 我们将采用ES6语法来写代码。Node是从4.0.0版本后开始支持大多数ES6语法的，并且目前并不支持modules，但我们需要用到。我们将加入Babel，这样我们就能将ES6直接转换成ES5了： npm install --save-dev babel 我们还需要些库来用于写单元测试： npm install --save-dev mocha chai Mocha是一个我们将要使用的测试框架，Chai是一个我们用来测试的断言库。 我们将使用下面的mocha命令来跑测试项： ./node_modules/mocha/bin/mocha --compilers js:babel/register --recursive 这条命令告诉Mocha递归的去项目中查找并执行所有测试项，但执行前先使用Babel进行语法转换。 为了使用方便，可以在我们的package.json中添加下面这段代码： &quot;scripts&quot;: { &quot;test&quot;: &quot;mocha --compilers js:babel/register --recursive&quot; }, 这样以后我们跑测试就只需要执行： npm run test 另外，我们还可以添加test:watch命令，它用来监控文件变化并自动跑测试项： &quot;scripts&quot;: { &quot;test&quot;: &quot;mocha --compilers js:babel/register --recursive&quot;, &quot;test:watch&quot;: &quot;npm run test -- --watch&quot; }, 我们还将用到一个库，来自于facebook：Immutable，它提供了许多数据结构供我们使用。下一小节我们再来讨论Immutable，但我们在这里先将它加入到我们的项目中，附带chai-immutable库，它用来向Chai库加入不可变数据结构比对功能： npm install --save immutable npm install --save-dev chai-immutable 我们需要在所有测试代码前先加入chai-immutable插件，所以我们来先创建一个测试辅助文件： //test/test_helper.js import chai from &apos;chai&apos;; import chaiImmutable from &apos;chai-immutable&apos;; chai.use(chaiImmutable); 然后我们需要让Mocha在开始跑测试之前先加载这个文件，修改package.json： &quot;scripts&quot;: { &quot;test&quot;: &quot;mocha --compilers js:babel/register -- require ./test/test_helper.js --recursive&quot;, &quot;test:watch&quot;: &quot;npm run test -- --watch&quot; }, 好了，准备的差不多了。 ###酸爽的Immutable 第二个值得重视的点是，Redux架构下状态并非只是一个普通的tree，而是一棵不可变的tree。 回想一下前面我们设计的状态tree，你可能会觉得可以直接在应用的代码里直接更新tree：修改映射的值，或删除数组元素等。然而，这并不是Redux允许的。 一个Redux应用的状态树是不可变的数据结构。这意味着，一旦你得到了一棵状态树，它就不会在改变了。任何用户行为改变应用状态，你都会获取一棵映射应用改变后新状态的完整状态树。 这说明任何连续的状态（改变前后）都被分别存储在独立的两棵树。你通过调用一个函数来从一种状态转入下一个状态。 这么做好在哪呢？第一，用户通常想一个undo功能，当你误操作导致破坏了应用状态后，你往往想退回到应用的历史状态，而单一的状态tree让该需求变得廉价，你只需要简单保存上一个状态tree的数据即可。你也可以序列化tree并存储起来以供将来重放，这对debug很有帮助的。 抛开其它的特性不谈，不可变数据至少会让你的代码变得简单，这非常重要。你可以用纯函数来进行编程：接受参数数据，返回数据，其它啥都不做。这种函数拥有可预见性，你可以多次调用它，只要参数一致，它总返回相同的结果（冪等性）。测试将变的容易，你不需要在测试前创建太多的准备，仅仅是传入参数和返回值。 不可变数据结构是我们创建应用状态的基础，让我们花点时间来写一些测试项来保证它的正常工作。 为了更了解不可变性，我们来看一个十分简单的数据结构：假设我们有一个计数应用，它只包含一个计数器变量，该变量会从0增加到1，增加到2，增加到3，以此类推。 如果用不可变数据来设计这个计数器变量，则每当计数器自增，我们不是去改变变量本身。你可以想象成该计数器变量没有“setters”方法，你不能执行42.setValue(43)。 每当变化发生，我们将获得一个新的变量，它的值是之前的那个变量的值加1等到的。我们可以为此写一个纯函数，它接受一个参数代表当前的状态，并返回一个值表示新的状态。记住，调用它并会修改传入参数的值。这里看一下函数实现和测试代码： //test/immutable_spec.js import {expect} from &apos;chai&apos;; describe(&apos;immutability&apos;, () =&gt; { describe(&apos;a number&apos;, () =&gt; { function increment(currentState) { return currentState + 1; } it(&apos;is immutable&apos;, () =&gt; { let state = 42; let nextState = increment(state); expect(nextState).to.equal(43); expect(state).to.equal(42); }); }); }); 可以看到当increment调用后state并没有被修改，这是因为Numbers是不可变的。 我们接下来要做的是让各种数据结构都不可变，而不仅仅是一个整数。 利用Immutable提供的Lists，我们可以假设我们的应用拥有一个电影列表的状态，并且有一个操作用来向当前列表中添加新电影，新列表数据是添加前的列表数据和新增的电影条目合并后的结果，注意，添加前的旧列表数据并没有被修改哦： //test/immutable_spec.json import {expect} from &apos;chai&apos;; import {List} from &apos;immutable&apos;; describe(&apos;immutability&apos;, () =&gt; { // ... describe(&apos;A List&apos;, () =&gt; { function addMovie(currentState, movie) { return currentState.push(movie); } it(&apos;is immutable&apos;, () =&gt; { let state = List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;); let nextState = addMovie(state, &apos;Sunshine&apos;); expect(nextState).to.equal(List.of( &apos;Trainspotting&apos;, &apos;28 Days Later&apos;, &apos;Sunshine&apos; )); expect(state).to.equal(List.of( &apos;Trainspotting&apos;, &apos;28 Days Later&apos; )); }); }); }); 如果我们使用的是原生态js数组，那么上面的addMovie函数并不会保证旧的状态不会被修改。这里我们使用的是Immutable List。 真实软件中，一个状态树通常是嵌套了多种数据结构的：list，map以及其它类型的集合。假设状态树是一个包含了movies列表的hash map，添加一个电影意味着我们需要创建一个新的map，并且在新的map的movies元素中添加该新增数据： //test/immutable_spec.json import {expect} from &apos;chai&apos;; import {List, Map} from &apos;immutable&apos;; describe(&apos;immutability&apos;, () =&gt; { // ... describe(&apos;a tree&apos;, () =&gt; { function addMovie(currentState, movie) { return currentState.set( &apos;movies&apos;, currentState.get(&apos;movies&apos;).push(movie) ); } it(&apos;is immutable&apos;, () =&gt; { let state = Map({ movies: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;) }); let nextState = addMovie(state, &apos;Sunshine&apos;); expect(nextState).to.equal(Map({ movies: List.of( &apos;Trainspotting&apos;, &apos;28 Days Later&apos;, &apos;Sunshine&apos; ) })); expect(state).to.equal(Map({ movies: List.of( &apos;Trainspotting&apos;, &apos;28 Days Later&apos; ) })); }); }); }); 该例子和前面的那个类似，主要用来展示在嵌套结构下Immutable的行为。 针对类似上面这个例子的嵌套数据结构，Immutable提供了很多辅助函数，可以帮助我们更容易的定位嵌套数据的内部属性，以达到更新对应值的目的。我们可以使用一个叫update的方法来修改上面的代码： //test/immutable_spec.json function addMovie(currentState, movie) { return currentState.update(&apos;movies&apos;, movies =&gt; movies.push(movie)); } 现在我们很好的了解了不可变数据，这将被用于我们的应用状态。Immutable API提供了非常多的辅助函数，我们目前只是学了点皮毛。 不可变数据是Redux的核心理念，但并不是必须使用Immutable库来实现这个特性。事实上，官方Redux文档使用的是原生js对象和数组，并通过简单的扩展它们来实现的。 这个教程中，我们将使用Immutable库，原因如下： 该库将使得实现不可变数据结构变得非常简单； 我个人偏爱于将尽可能的使用不可变数据，如果你的数据允许直接修改，迟早会有人踩坑； 不可变数据结构更新是持续的，意味着很容易产生性能平静，特别维护是非常庞大的状态树，使用原生js对象和数组意味着要频繁的进行拷贝，很容易导致性能问题。 ###基于纯函数实现应用逻辑 根据目前我们掌握的不可变状态树和相关操作，我们可以尝试实现投票应用的逻辑。应用的核心逻辑我们拆分成：状态树结构和生成新状态树的函数集合。 ####加载条目 首先，之前说到，应用允许“加载”一个用来投票的条目集。我们需要一个setEntries函数，它用来提供应用的初始化状态： //test/core_spec.js import {List, Map} from &apos;immutable&apos;; import {expect} from &apos;chai&apos;; import {setEntries} from &apos;../src/core&apos;; describe(&apos;application logic&apos;, () =&gt; { describe(&apos;setEntries&apos;, () =&gt; { it(&apos;adds the entries to the state&apos;, () =&gt; { const state = Map(); const entries = List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;); const nextState = setEntries(state, entries); expect(nextState).to.equal(Map({ entries: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;) })); }); }); }); 我们目前setEntries函数的第一版非常简单：在状态map中创建一个entries键，并设置给定的条目List。 //src/core.js export function setEntries(state, entries) { return state.set(&apos;entries&apos;, entries); } 为了方便起见，我们允许函数第二个参数接受一个原生js数组（或支持iterable的类型），但在状态树中它应该是一个Immutable List： //test/core_spec.js it(&apos;converts to immutable&apos;, () =&gt; { const state = Map(); const entries = [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;]; const nextState = setEntries(state, entries); expect(nextState).to.equal(Map({ entries: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;) })); }); 为了达到要求，我们需要修改一下代码： //src/core.js import {List} from &apos;immutable&apos;; export function setEntries(state, entries) { return state.set(&apos;entries&apos;, List(entries)); } ####开始投票 当state加载了条目集合后，我们可以调用一个next函数来开始投票。这表示，我们到了之前设计的状态树的第二阶段。 next函数需要在状态树创建中一个投票map，该map有拥有一个pair键，值为投票条目中的前两个元素。这两个元素一旦确定，就要从之前的条目列表中清除： //test/core_spec.js import {List, Map} from &apos;immutable&apos;; import {expect} from &apos;chai&apos;; import {setEntries, next} from &apos;../src/core&apos;; describe(&apos;application logic&apos;, () =&gt; { // .. describe(&apos;next&apos;, () =&gt; { it(&apos;takes the next two entries under vote&apos;, () =&gt; { const state = Map({ entries: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;, &apos;Sunshine&apos;) }); const nextState = next(state); expect(nextState).to.equal(Map({ vote: Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;) }), entries: List.of(&apos;Sunshine&apos;) })); }); }); }); next函数实现如下： //src/core.js import {List, Map} from &apos;immutable&apos;; // ... export function next(state) { const entries = state.get(&apos;entries&apos;); return state.merge({ vote: Map({pair: entries.take(2)}), entries: entries.skip(2) }); } ####投票 当用户产生投票行为后，每当用户给某个条目投了一票后，vote将会为这个条目添加tally信息，如果对应的条目信息已存在，则需要则增： //test/core_spec.js import {List, Map} from &apos;immutable&apos;; import {expect} from &apos;chai&apos;; import {setEntries, next, vote} from &apos;../src/core&apos;; describe(&apos;application logic&apos;, () =&gt; { // ... describe(&apos;vote&apos;, () =&gt; { it(&apos;creates a tally for the voted entry&apos;, () =&gt; { const state = Map({ vote: Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;) }), entries: List() }); const nextState = vote(state, &apos;Trainspotting&apos;); expect(nextState).to.equal(Map({ vote: Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;), tally: Map({ &apos;Trainspotting&apos;: 1 }) }), entries: List() })); }); it(&apos;adds to existing tally for the voted entry&apos;, () =&gt; { const state = Map({ vote: Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;), tally: Map({ &apos;Trainspotting&apos;: 3, &apos;28 Days Later&apos;: 2 }) }), entries: List() }); const nextState = vote(state, &apos;Trainspotting&apos;); expect(nextState).to.equal(Map({ vote: Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;), tally: Map({ &apos;Trainspotting&apos;: 4, &apos;28 Days Later&apos;: 2 }) }), entries: List() })); }); }); }); 为了让上面的测试项通过，我们可以如下实现vote函数： //src/core.js export function vote(state, entry) { return state.updateIn( [&apos;vote&apos;, &apos;tally&apos;, entry], 0, tally =&gt; tally + 1 ); } updateIn让我们更容易完成目标。它接受的第一个参数是个表达式，含义是“定位到嵌套数据结构的指定位置，路径为：[‘vote’, ‘tally’, ‘Trainspotting’]”，并且执行后面逻辑：如果路径指定的位置不存在，则创建新的映射对，并初始化为0，否则对应值加1。 可能对你来说上面的语法太过于晦涩，但一旦你掌握了它，你将会发现用起来非常的酸爽，所以花一些时间学习并适应它是非常值得的。 ####继续投票 每次完成一次二选一投票，用户将进入到第二轮投票，每次得票最高的选项将被保存并添加回条目集合。我们需要添加这个逻辑到next函数中： //test/core_spec.js describe(&apos;next&apos;, () =&gt; { // ... it(&apos;puts winner of current vote back to entries&apos;, () =&gt; { const state = Map({ vote: Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;), tally: Map({ &apos;Trainspotting&apos;: 4, &apos;28 Days Later&apos;: 2 }) }), entries: List.of(&apos;Sunshine&apos;, &apos;Millions&apos;, &apos;127 Hours&apos;) }); const nextState = next(state); expect(nextState).to.equal(Map({ vote: Map({ pair: List.of(&apos;Sunshine&apos;, &apos;Millions&apos;) }), entries: List.of(&apos;127 Hours&apos;, &apos;Trainspotting&apos;) })); }); it(&apos;puts both from tied vote back to entries&apos;, () =&gt; { const state = Map({ vote: Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;), tally: Map({ &apos;Trainspotting&apos;: 3, &apos;28 Days Later&apos;: 3 }) }), entries: List.of(&apos;Sunshine&apos;, &apos;Millions&apos;, &apos;127 Hours&apos;) }); const nextState = next(state); expect(nextState).to.equal(Map({ vote: Map({ pair: List.of(&apos;Sunshine&apos;, &apos;Millions&apos;) }), entries: List.of(&apos;127 Hours&apos;, &apos;Trainspotting&apos;, &apos;28 Days Later&apos;) })); }); }); 我们需要一个getWinners函数来帮我们选择谁是赢家： //src/core.js function getWinners(vote) { if (!vote) return []; const [a, b] = vote.get(&apos;pair&apos;); const aVotes = vote.getIn([&apos;tally&apos;, a], 0); const bVotes = vote.getIn([&apos;tally&apos;, b], 0); if (aVotes &gt; bVotes) return [a]; else if (aVotes &lt; bVotes) return [b]; else return [a, b]; } export function next(state) { const entries = state.get(&apos;entries&apos;) .concat(getWinners(state.get(&apos;vote&apos;))); return state.merge({ vote: Map({pair: entries.take(2)}), entries: entries.skip(2) }); } ####投票结束 当投票项只剩一个时，投票结束： //test/core_spec.js describe(&apos;next&apos;, () =&gt; { // ... it(&apos;marks winner when just one entry left&apos;, () =&gt; { const state = Map({ vote: Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;), tally: Map({ &apos;Trainspotting&apos;: 4, &apos;28 Days Later&apos;: 2 }) }), entries: List() }); const nextState = next(state); expect(nextState).to.equal(Map({ winner: &apos;Trainspotting&apos; })); }); }); 我们需要在next函数中增加一个条件分支，用来匹配上面的逻辑： //src/core.js export function next(state) { const entries = state.get(&apos;entries&apos;) .concat(getWinners(state.get(&apos;vote&apos;))); if (entries.size === 1) { return state.remove(&apos;vote&apos;) .remove(&apos;entries&apos;) .set(&apos;winner&apos;, entries.first()); } else { return state.merge({ vote: Map({pair: entries.take(2)}), entries: entries.skip(2) }); } } 我们可以直接返回Map({winner: entries.first()})，但我们还是基于旧的状态数据进行一步一步的操作最终得到结果，这么做是为将来做打算。因为应用将来可能还会有很多其它状态数据在Map中，这是一个写测试项的好习惯。所以我们以后要记住，不要重新创建一个状态数据，而是从旧的状态数据中生成新的状态实例。 到此为止我们已经有了一套可以接受的应用核心逻辑实现，表现形式为几个独立的函数。我们也有针对这些函数的测试代码，这些测试项很容易写：No setup, no mocks, no stubs。这就是纯函数的魅力，我们只需要调用它们，并检查返回值就行了。 提醒一下，我们目前还没有安装redux哦，我们就已经可以专注于应用自身的逻辑本身进行实现，而不被所谓的框架所干扰。这真的很不错，对吧？ ###初识Actions和Reducers 我们有了应用的核心函数，但在Redux中我们不应该直接调用函数。在这些函数和应用之间还存在这一个中间层：Actions。 Action是一个描述应用状态变化发生的简单数据结构。按照约定，每个action都包含一个type属性，该属性用于描述操作类型。action通常还包含其它属性，下面是一个简单的action例子，该action用来匹配前面我们写的业务操作： {type: &apos;SET_ENTRIES&apos;, entries: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;]} {type: &apos;NEXT&apos;} {type: &apos;VOTE&apos;, entry: &apos;Trainspotting&apos;} actions的描述就这些，但我们还需要一种方式用来把它绑定到我们实际的核心函数上。举个例子： // 定义一个action let voteAction = {type: &apos;VOTE&apos;, entry: &apos;Trainspotting&apos;} // 该action应该触发下面的逻辑 return vote(state, voteAction.entry); 我们接下来要用到的是一个普通函数，它用来根据action和当前state来调用指定的核心函数，我们称这种函数叫：reducer： //src/reducer.js export default function reducer(state, action) { // Figure out which function to call and call it } 我们应该测试这个reducer是否可以正确匹配我们之前的三个actions： //test/reducer_spec.js import {Map, fromJS} from &apos;immutable&apos;; import {expect} from &apos;chai&apos;; import reducer from &apos;../src/reducer&apos;; describe(&apos;reducer&apos;, () =&gt; { it(&apos;handles SET_ENTRIES&apos;, () =&gt; { const initialState = Map(); const action = {type: &apos;SET_ENTRIES&apos;, entries: [&apos;Trainspotting&apos;]}; const nextState = reducer(initialState, action); expect(nextState).to.equal(fromJS({ entries: [&apos;Trainspotting&apos;] })); }); it(&apos;handles NEXT&apos;, () =&gt; { const initialState = fromJS({ entries: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;] }); const action = {type: &apos;NEXT&apos;}; const nextState = reducer(initialState, action); expect(nextState).to.equal(fromJS({ vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;] }, entries: [] })); }); it(&apos;handles VOTE&apos;, () =&gt; { const initialState = fromJS({ vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;] }, entries: [] }); const action = {type: &apos;VOTE&apos;, entry: &apos;Trainspotting&apos;}; const nextState = reducer(initialState, action); expect(nextState).to.equal(fromJS({ vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;], tally: {Trainspotting: 1} }, entries: [] })); }); }); 我们的reducer将根据action的type来选择对应的核心函数，它同时也应该知道如何使用action的额外属性： //src/reducer.js import {setEntries, next, vote} from &apos;./core&apos;; export default function reducer(state, action) { switch (action.type) { case &apos;SET_ENTRIES&apos;: return setEntries(state, action.entries); case &apos;NEXT&apos;: return next(state); case &apos;VOTE&apos;: return vote(state, action.entry) } return state; } 注意，如果reducer没有匹配到action，则应该返回当前的state。 reducers还有一个需要特别注意的地方，那就是当传递一个未定义的state参数时，reducers应该知道如何初始化state为有意义的值。我们的场景中，初始值为Map，因此如果传给reducer一个undefinedstate的话，reducers将使用一个空的Map来代替： //test/reducer_spec.js describe(&apos;reducer&apos;, () =&gt; { // ... it(&apos;has an initial state&apos;, () =&gt; { const action = {type: &apos;SET_ENTRIES&apos;, entries: [&apos;Trainspotting&apos;]}; const nextState = reducer(undefined, action); expect(nextState).to.equal(fromJS({ entries: [&apos;Trainspotting&apos;] })); }); }); 之前在我们的cores.js文件中，我们定义了初始值： //src/core.js export const INITIAL_STATE = Map(); 所以在reducer中我们可以直接导入它： //src/reducer.js import {setEntries, next, vote, INITIAL_STATE} from &apos;./core&apos;; export default function reducer(state = INITIAL_STATE, action) { switch (action.type) { case &apos;SET_ENTRIES&apos;: return setEntries(state, action.entries); case &apos;NEXT&apos;: return next(state); case &apos;VOTE&apos;: return vote(state, action.entry) } return state; } 事实上，提供一个action集合，你可以将它们分解并作用在当前状态上，这也是为什么称它们为reducer的原因：它完全适配reduce方法： //test/reducer_spec.js it(&apos;can be used with reduce&apos;, () =&gt; { const actions = [ {type: &apos;SET_ENTRIES&apos;, entries: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;]}, {type: &apos;NEXT&apos;}, {type: &apos;VOTE&apos;, entry: &apos;Trainspotting&apos;}, {type: &apos;VOTE&apos;, entry: &apos;28 Days Later&apos;}, {type: &apos;VOTE&apos;, entry: &apos;Trainspotting&apos;}, {type: &apos;NEXT&apos;} ]; const finalState = actions.reduce(reducer, Map()); expect(finalState).to.equal(fromJS({ winner: &apos;Trainspotting&apos; })); }); 相比直接调用核心业务函数，这种批处理或称之为重放一个action集合的能力主要依赖于状态转换的action/reducer模型。举个例子，你可以把actions序列化成json，并轻松的将它发送给Web Worker去执行你的reducer逻辑。或者直接通过网络发送到其它地方供日后执行！ 注意我们这里使用的是普通js对象作为actions，而并非不可变数据类型。这是Redux提倡我们的做法。 ###尝试Reducer协作 目前我们的核心函数都是接受整个state并返回更新后的整个state。 这么做在大型应用中可能并不太明智。如果你的应用所有操作都要求必须接受完整的state，那么这个项目维护起来就是灾难。日后如果你想进行state结构的调整，你将会付出惨痛的代价。 其实有更好的做法，你只需要保证组件操作尽可能小的state片段即可。我们这里提到的就是模块化思想：提供给模块仅它需要的数据，不多不少。 我们的应用很小，所以这并不是太大的问题，但我们还是选择改善这一点：没有必要给vote函数传递整个state，它只需要vote部分。让我们修改一下对应的测试代码： //test/core_spec.js describe(&apos;vote&apos;, () =&gt; { it(&apos;creates a tally for the voted entry&apos;, () =&gt; { const state = Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;) }); const nextState = vote(state, &apos;Trainspotting&apos;) expect(nextState).to.equal(Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;), tally: Map({ &apos;Trainspotting&apos;: 1 }) })); }); it(&apos;adds to existing tally for the voted entry&apos;, () =&gt; { const state = Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;), tally: Map({ &apos;Trainspotting&apos;: 3, &apos;28 Days Later&apos;: 2 }) }); const nextState = vote(state, &apos;Trainspotting&apos;); expect(nextState).to.equal(Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;), tally: Map({ &apos;Trainspotting&apos;: 4, &apos;28 Days Later&apos;: 2 }) })); }); }); 看，测试代码更加简单了。 vote函数的实现也需要更新： //src/core.js export function vote(voteState, entry) { return voteState.updateIn( [&apos;tally&apos;, entry], 0, tally =&gt; tally + 1 ); } 最后我们还需要修改reducer，只传递需要的state给vote函数： //src/reducer.js export default function reducer(state = INITIAL_STATE, action) { switch (action.type) { case &apos;SET_ENTRIES&apos;: return setEntries(state, action.entries); case &apos;NEXT&apos;: return next(state); case &apos;VOTE&apos;: return state.update(&apos;vote&apos;, voteState =&gt; vote(voteState, action.entry)); } return state; } 这个做法在大型项目中非常重要：根reducer只传递部分state给下一级reducer。我们将定位合适的state片段的工作从对应的更新操作中分离出来。 Redux的reducers文档针对这一细节介绍了更多内容，并描述了一些辅助函数的用法，可以在更多长场景中有效的使用。 ###初识Redux Store 现在我们可以开始了解如何将上面介绍的内容使用在Redux中了。 如你所见，如果你有一个actions集合，你可以调用reduce，获得最终的应用状态。当然，通常情况下不会如此，actions将会在不同的时间发生：用户操作，远程调用，超时触发器等。 针对这些情况，我们可以使用Redux Store。从名字可以看出它用来存储应用的状态。 Redux Store通常会由一个reducer函数初始化，如我们之前实现的： import {createStore} from &apos;redux&apos;; const store = createStore(reducer); 接下来你就可以向这个Store指派actions了。Store内部将会使用你实现的reducer来处理action，并负责传递给reducer应用的state，最后负责存储reducer返回的新state： store.dispatch({type: &apos;NEXT&apos;}); 任何时刻你都可以通过下面的方法获取当前的state： store.getState(); 我们将会创建一个store.js用来初始化和导出一个Redux Store对象。让我们先写测试代码吧： //test/store_spec.js import {Map, fromJS} from &apos;immutable&apos;; import {expect} from &apos;chai&apos;; import makeStore from &apos;../src/store&apos;; describe(&apos;store&apos;, () =&gt; { it(&apos;is a Redux store configured with the correct reducer&apos;, () =&gt; { const store = makeStore(); expect(store.getState()).to.equal(Map()); store.dispatch({ type: &apos;SET_ENTRIES&apos;, entries: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;] }); expect(store.getState()).to.equal(fromJS({ entries: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;] })); }); }); 在创建Store之前，我们先在项目中加入Redux库： npm install --save redux 然后我们新建store.js文件，如下： //src/store.js import {createStore} from &apos;redux&apos;; import reducer from &apos;./reducer&apos;; export default function makeStore() { return createStore(reducer); } Redux Store负责将应用的所有组件关联起来：它持有应用的当前状态，并负责指派actions，且负责调用包含了业务逻辑的reducer。 应用的业务代码和Redux的整合方式非常引人注目，因为我们只有一个普通的reducer函数，这是唯一需要告诉Redux的事儿。其它部分全部都是我们自己的，没有框架入侵的，高便携的纯函数代码！ 现在我们创建一个应用的入口文件index.js： //index.js import makeStore from &apos;./src/store&apos;; export const store = makeStore(); 现在我们可以开启一个Node REPL（例如babel-node）,载入index.js文件来测试执行了。 ###配置Socket.io服务 我们的应用服务端用来为一个提供投票和显示结果浏览器端提供服务的，为了这个目的，我们需要考虑两端通信的方式。 这个应用需要实时通信，这确保我们的投票者可以实时查看到所有人的投票信息。为此，我们选择使用WebSockets作为通信方式。因此，我们选择Socket.io库作为跨终端的websocket抽象实现层，它在客户端不支持websocket的情况下提供了多种备选方案。 让我们在项目中加入Socket.io： npm install --save socket.io 现在，让我新建一个server.js文件： //src/server.js import Server from &apos;socket.io&apos;; export default function startServer() { const io = new Server().attach(8090); } 这里我们创建了一个Socket.io 服务，绑定8090端口。端口号是我随意选的，你可以更改，但后面客户端连接时要注意匹配。 现在我们可以在index.js中调用这个函数： //index.js import makeStore from &apos;./src/store&apos;; import startServer from &apos;./src/server&apos;; export const store = makeStore(); startServer(); 我们现在可以在package.json中添加start指令来方便启动应用： //package.json &quot;scripts&quot;: { &quot;start&quot;: &quot;babel-node index.js&quot;, &quot;test&quot;: &quot;mocha --compilers js:babel/register --require ./test/test_helper.js --recursive&quot;, &quot;test:watch&quot;: &quot;npm run test --watch&quot; }, 这样我们就可以直接执行下面命令来开启应用： npm run start ###用Redux监听器传播State 我们现在拥有了一个Socket.io服务，也建立了Redux状态容器，但它们并没有整合在一起，这就是我们接下来要做的事儿。 我们的服务端需要让客户端知道当前的应用状态（例如：“正在投票的项目是什么？”，“当前的票数是什么？”，“已经出来结果了吗？”）。这些都可以通过每当变化发生时触发Socket.io事件来实现。 我们如何得知什么时候发生变化？Redux对此提供了方案：你可以订阅Redux Store。这样每当store指派了action之后，在可能发生变化前会调用你提供的指定回调函数。 我们要修改一下startServer实现，我们先来调整一下index.js： //index.js import makeStore from &apos;./src/store&apos;; import {startServer} from &apos;./src/server&apos;; export const store = makeStore(); startServer(store); 接下来我们只需监听store的状态，并把它序列化后用socket.io事件传播给所有处于连接状态的客户端。 //src/server.js import Server from &apos;socket.io&apos;; export function startServer(store) { const io = new Server().attach(8090); store.subscribe( () =&gt; io.emit(&apos;state&apos;, store.getState().toJS()) ); } 目前我们的做法是一旦状态有改变，就发送整个state给所有客户端，很容易想到这非常不友好，产生大量流量损耗，更好的做法是只传递改变的state片段，但我们为了简单，在这个例子中就先这么实现吧。 除了状态发生变化时发送状态数据外，每当新客户端连接服务器端时也应该直接发送当前的状态给该客户端。 我们可以通过监听Socket.io的connection事件来实现上述需求： //src/server.js import Server from &apos;socket.io&apos;; export function startServer(store) { const io = new Server().attach(8090); store.subscribe( () =&gt; io.emit(&apos;state&apos;, store.getState().toJS()) ); io.on(&apos;connection&apos;, (socket) =&gt; { socket.emit(&apos;state&apos;, store.getState().toJS()); }); } ###接受远程调用Redux Actions 除了将应用状态同步给客户端外，我们还需要接受来自客户端的更新操作：投票者需要发起投票，投票组织者需要发起下一轮投票的请求。 我们的解决方案非常简单。我们只需要让客户端发布“action”事件即可，然后我们直接将事件发送给Redux Store： //src/server.js import Server from &apos;socket.io&apos;; export function startServer(store) { const io = new Server().attach(8090); store.subscribe( () =&gt; io.emit(&apos;state&apos;, store.getState().toJS()) ); io.on(&apos;connection&apos;, (socket) =&gt; { socket.emit(&apos;state&apos;, store.getState().toJS()); socket.on(&apos;action&apos;, store.dispatch.bind(store)); }); } 这样我们就完成了远程调用actions。Redux架构让我们的项目更加简单：actions仅仅是js对象，可以很容易用于网络传输，我们现在实现了一个支持多人投票的服务端系统，很有成就感吧。 现在我们的服务端操作流程如下： 客户端发送一个action给服务端； 服务端交给Redux Store处理action； Store调用reducer，reducer执行对应的应用逻辑； Store根据reducer的返回结果来更新状态； Store触发服务端监听的回调函数； 服务端触发“state”事件； 所有连接的客户端接受到新的状态。 在结束服务端开发之前，我们载入一些测试数据来感受一下。我们可以添加entries.json文件： //entries.json [ &quot;Shallow Grave&quot;, &quot;Trainspotting&quot;, &quot;A Life Less Ordinary&quot;, &quot;The Beach&quot;, &quot;28 Days Later&quot;, &quot;Millions&quot;, &quot;Sunshine&quot;, &quot;Slumdog Millionaire&quot;, &quot;127 Hours&quot;, &quot;Trance&quot;, &quot;Steve Jobs&quot; ] 我们在index.json中加载它然后发起nextaction来开启投票： //index.js import makeStore from &apos;./src/store&apos;; import {startServer} from &apos;./src/server&apos;; export const store = makeStore(); startServer(store); store.dispatch({ type: &apos;SET_ENTRIES&apos;, entries: require(&apos;./entries.json&apos;) }); store.dispatch({type: &apos;NEXT&apos;}); 那么接下来我们就来看看如何实现客户端。 ##客户端应用 本教程剩余的部分就是写一个React应用，用来连接服务端，并提供投票给使用者。 在客户端我们依然使用Redux。这是更常见的搭配：用于React应用的底层引擎。我们已经了解到Redux如何使用。现在我们将学习它是如何结合并影响React应用的。 我推荐大家跟随本教程的步骤完成应用，但你也可以从github上获取源码。 ###客户端项目创建 第一件事儿我们当然是创建一个新的NPM项目，如下： mkdir voting-client cd voting-client npm init # Just hit enter for each question 我们的应用需要一个html主页，我们放在dist/index.html： //dist/index.html &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;body&gt; &lt;div id=&quot;app&quot;&gt;&lt;/div&gt; &lt;script src=&quot;bundle.js&quot;&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; 这个页面包含一个id为app的&lt;div&gt;，我们将在其中插入我们的应用。在同级目录下还需要一个bundle.js文件。 我们为应用新建第一个js文件，它是系统的入口文件。目前我们先简单的添加一行日志代码： //src/index.js console.log(&apos;I am alive!&apos;); 为了给我们客户端开发减负，我们将使用Webpack，让我们加入到项目中： npm install --save-dev webpack webpack-dev-server 接下来，我们在项目根目录新建一个Webpack配置文件： //webpack.config.js module.exports = { entry: [ &apos;./src/index.js&apos; ], output: { path: __dirname + &apos;/dist&apos;, publicPath: &apos;/&apos;, filename: &apos;bundle.js&apos; }, devServer: { contentBase: &apos;./dist&apos; } }; 配置表明将找到我们的index.js入口，并编译到dist/bundle.js中。同时把dist目录当作开发服务器根目录。 你现在可以执行Webpack来生成bundle.js： webpack 你也可以开启一个开发服务器，访问localhost:8080来测试页面效果： webpack-dev-server 由于我们将使用ES6语法和React的JSX语法，我们需要一些工具。Babel是一个非常合适的选择，我们需要Babel库： npm install --save-dev babel-core babel-loader 我们可以在Webpack配置文件中添加一些配置，这样webpack将会对.jsx和.js文件使用Babel进行处理： //webpack.config.js module.exports = { entry: [ &apos;./src/index.js&apos; ], module: { loaders: [{ test: /\\.jsx?$/, exclude: /node_modules/, loader: &apos;babel&apos; }] }, resolve: { extensions: [&apos;&apos;, &apos;.js&apos;, &apos;.jsx&apos;] }, output: { path: __dirname + &apos;/dist&apos;, publicPath: &apos;/&apos;, filename: &apos;bundle.js&apos; }, devServer: { contentBase: &apos;./dist&apos; } }; ###单元测试支持 我们也将会为客户端代码编写一些单元测试。我们使用与服务端相同的测试套件： npm install --save-dev mocha chai 我们也将会测试我们的React组件，这就要求需要一个DOM库。我们可能需要像Karma库一样的功能来进行真实web浏览器测试。但我们这里准备使用一个node端纯js的dom库： npm install --save-dev jsdom@3 在用于react之前我们需要一些jsdom的预备代码。我们需要创建通常在浏览器端被提供的document和window对象。并且将它们声明为全局对象，这样才能被React使用。我们可以创建一个测试辅助文件做这些工作： //test/test_helper.js import jsdom from &apos;jsdom&apos;; const doc = jsdom.jsdom(&apos;&lt;!doctype html&gt;&lt;html&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;&apos;); const win = doc.defaultView; global.document = doc; global.window = win; 此外，我们还需要将jsdom提供的window对象的所有属性导入到Node.js的全局变量中，这样使用这些属性时就不需要window.前缀，这才满足在浏览器环境下的用法： //test/test_helper.js import jsdom from &apos;jsdom&apos;; const doc = jsdom.jsdom(&apos;&lt;!doctype html&gt;&lt;html&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;&apos;); const win = doc.defaultView; global.document = doc; global.window = win; Object.keys(window).forEach((key) =&gt; { if (!(key in global)) { global[key] = window[key]; } }); 我们还需要使用Immutable集合，所以我们也需要参照后段配置添加相应的库： npm install --save immutable npm install --save-dev chai-immutable 现在我们再次修改辅助文件： //test/test_helper.js import jsdom from &apos;jsdom&apos;; import chai from &apos;chai&apos;; import chaiImmutable from &apos;chai-immutable&apos;; const doc = jsdom.jsdom(&apos;&lt;!doctype html&gt;&lt;html&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;&apos;); const win = doc.defaultView; global.document = doc; global.window = win; Object.keys(window).forEach((key) =&gt; { if (!(key in global)) { global[key] = window[key]; } }); chai.use(chaiImmutable); 最后一步是在package.json中添加指令： //package.json &quot;scripts&quot;: { &quot;test&quot;: &quot;mocha --compilers js:babel-core/register --require ./test/test_helper.js &apos;test/**/*.@(js|jsx)&apos;&quot; }, 这几乎和我们在后端做的一样，只有两个地方不同： Babel的编译器名称：在该项目中我们使用babel-core代替babel 测试文件设置：服务端我们使用--recursive，但这么设置无法匹配.jsx文件，所以我们需要使用glob 为了实现当代码发生修改后自动进行测试，我们依然添加test:watch指令： //package.json &quot;scripts&quot;: { &quot;test&quot;: &quot;mocha --compilers js:babel-core/register --require ./test/test_helper.js &apos;test/**/*.@(js|jsx)&apos;&quot;, &quot;test:watch&quot;: &quot;npm run test -- --watch&quot; }, ###React和react-hot-loader 最后我们来聊聊React！ 使用React+Redux+Immutable来开发应用真正酷毙的地方在于：我们可以用纯组件（有时候也称为蠢组件）思想实现任何东西。这个概念与纯函数很类似，有如下一些规则： 一个纯组件利用props接受所有它需要的数据，类似一个函数的入参，除此之外它不会被任何其它因素影响； 一个纯组件通常没有内部状态。它用来渲染的数据完全来自于输入props，使用相同的props来渲染相同的纯组件多次，将得到相同的UI。不存在隐藏的内部状态导致渲染不同。 这就带来了一个和使用纯函数一样的效果：我们可以根据输入来预测一个组件的渲染，我们不需要知道组件的其它信息。这也使得我们的界面测试变得很简单，与我们测试纯应用逻辑一样简单。 如果组件不包含状态，那么状态放在哪？当然在不可变的Store中啊！我们已经见识过它是怎么运作的了，其最大的特点就是从界面代码中分离出状态。 在此之前，我们还是先给项目添加React： npm install --save react 我们同样需要react-hot-loader。它让我们的开发变得非常快，因为它提供了我们在不丢失当前状态的情况下重载代码的能力： npm install --save-dev react-hot-loader 我们需要更新一下webpack.config.js，使其能热加载： //webpack.config.js var webpack = require(&apos;webpack&apos;); module.exports = { entry: [ &apos;webpack-dev-server/client?http://localhost:8080&apos;, &apos;webpack/hot/only-dev-server&apos;, &apos;./src/index.js&apos; ], module: { loaders: [{ test: /\\.jsx?$/, exclude: /node_modules/, loader: &apos;react-hot!babel&apos; }], } resolve: { extensions: [&apos;&apos;, &apos;.js&apos;, &apos;.jsx&apos;] }, output: { path: __dirname + &apos;/dist&apos;, publicPath: &apos;/&apos;, filename: &apos;bundle.js&apos; }, devServer: { contentBase: &apos;./dist&apos;, hot: true }, plugins: [ new webpack.HotModuleReplacementPlugin() ] }; 在上述配置的entry里我们包含了2个新的应用入口点：webpack dev server和webpack hot module loader。它们提供了webpack模块热替换能力。该能力并不是默认加载的，所以上面我们才需要在plugins和devServer中手动加载。 配置的loaders部分我们在原先的Babel前配置了react-hot用于.js和.jsx文件。 如果你现在重启开发服务器，你将看到一个在终端看到Hot Module Replacement已开启的消息提醒。我们可以开始写我们的第一个组件了。 ###实现投票界面 应用的投票界面非常简单：一旦投票启动，它将现实2个按钮，分别用来表示2个可选项，当投票结束，它显示最终结果。 我们之前都是以测试先行的开发方式，但是在react组件开发中我们将先实现组件，再进行测试。这是因为webpack和react-hot-loader提供了更加优良的反馈机制。而且，也没有比直接看到界面更加好的测试UI手段了。 让我们假设有一个Voting组件，在之前的入口文件index.html的#appdiv中加载它。由于我们的代码中包含JSX语法，所以需要把index.js重命名为index.jsx： //src/index.jsx import React from &apos;react&apos;; import Voting from &apos;./components/Voting&apos;; const pair = [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;]; React.render( &lt;Voting pair={pair} /&gt;, document.getElementById(&apos;app&apos;) ); Voting组件将使用pair属性来加载数据。我们目前可以先硬编码数据，稍后我们将会用真实数据来代替。组件本身是纯粹的，并且对数据来源并不敏感。 注意，在webpack.config.js中的入口点文件名也要修改： //webpack.config.js entry: [ &apos;webpack-dev-server/client?http://localhost:8080&apos;, &apos;webpack/hot/only-dev-server&apos;, &apos;./src/index.jsx&apos; ], 如果你此时重启webpack-dev-server，你将看到缺失Voting组件的报错。让我们修复它： //src/components/Voting.jsx import React from &apos;react&apos;; export default React.createClass({ getPair: function() { return this.props.pair || []; }, render: function() { return &lt;div className=&quot;voting&quot;&gt; {this.getPair().map(entry =&gt; &lt;button key={entry}&gt; &lt;h1&gt;{entry}&lt;/h1&gt; &lt;/button&gt; )} &lt;/div&gt;; } }); 你将会在浏览器上看到组件创建的2个按钮。你可以试试修改代码感受一下浏览器自动更新的魅力，没有刷新，没有页面加载，一切都那么迅雷不及掩耳盗铃。 现在我们来添加第一个单元测试： //test/components/Voting_spec.jsx import Voting from &apos;../../src/components/Voting&apos;; describe(&apos;Voting&apos;, () =&gt; { }); 测试组件渲染的按钮，我们必须先看看它的输出是什么。要在单元测试中渲染一个组件，我们需要react/addons提供的辅助函数renderIntoDocument： //test/components/Voting_spec.jsx import React from &apos;react/addons&apos;; import Voting from &apos;../../src/components/Voting&apos;; const {renderIntoDocument} = React.addons.TestUtils; describe(&apos;Voting&apos;, () =&gt; { it(&apos;renders a pair of buttons&apos;, () =&gt; { const component = renderIntoDocument( &lt;Voting pair={[&quot;Trainspotting&quot;, &quot;28 Days Later&quot;]} /&gt; ); }); }); 一旦组件渲染完毕，我就可以通过react提供的另一个辅助函数scryRenderedDOMComponentsWithTag来拿到button元素。我们期望存在两个按钮，并且期望按钮的值是我们设置的： //test/components/Voting_spec.jsx import React from &apos;react/addons&apos;; import Voting from &apos;../../src/components/Voting&apos;; import {expect} from &apos;chai&apos;; const {renderIntoDocument, scryRenderedDOMComponentsWithTag} = React.addons.TestUtils; describe(&apos;Voting&apos;, () =&gt; { it(&apos;renders a pair of buttons&apos;, () =&gt; { const component = renderIntoDocument( &lt;Voting pair={[&quot;Trainspotting&quot;, &quot;28 Days Later&quot;]} /&gt; ); const buttons = scryRenderedDOMComponentsWithTag(component, &apos;button&apos;); expect(buttons.length).to.equal(2); expect(buttons[0].getDOMNode().textContent).to.equal(&apos;Trainspotting&apos;); expect(buttons[1].getDOMNode().textContent).to.equal(&apos;28 Days Later&apos;); }); }); 如果我们跑一下测试，将会看到测试通过的提示： npm run test 当用户点击某个按钮后，组件将会调用回调函数，该函数也由组件的prop传递给组件。 让我们完成这一步，我们可以通过使用React提供的测试工具Simulate来模拟点击操作： //test/components/Voting_spec.jsx import React from &apos;react/addons&apos;; import Voting from &apos;../../src/components/Voting&apos;; import {expect} from &apos;chai&apos;; const {renderIntoDocument, scryRenderedDOMComponentsWithTag, Simulate} = React.addons.TestUtils; describe(&apos;Voting&apos;, () =&gt; { // ... it(&apos;invokes callback when a button is clicked&apos;, () =&gt; { let votedWith; const vote = (entry) =&gt; votedWith = entry; const component = renderIntoDocument( &lt;Voting pair={[&quot;Trainspotting&quot;, &quot;28 Days Later&quot;]} vote={vote}/&gt; ); const buttons = scryRenderedDOMComponentsWithTag(component, &apos;button&apos;); Simulate.click(buttons[0].getDOMNode()); expect(votedWith).to.equal(&apos;Trainspotting&apos;); }); }); 要想使上面的测试通过很简单，我们只需要让按钮的onClick事件调用vote并传递选中条目即可： //src/components/Voting.jsx import React from &apos;react&apos;; export default React.createClass({ getPair: function() { return this.props.pair || []; }, render: function() { return &lt;div className=&quot;voting&quot;&gt; {this.getPair().map(entry =&gt; &lt;button key={entry} onClick={() =&gt; this.props.vote(entry)}&gt; &lt;h1&gt;{entry}&lt;/h1&gt; &lt;/button&gt; )} &lt;/div&gt;; } }); 这就是我们在纯组件中常用的方式：组件不需要做太多，只是回调传入的参数即可。 注意，这里我们又是先写的测试代码，我发现业务代码的测试要比测试UI更容易写，所以后面我们会保持这种方式：UI测试后行，业务代码测试先行。 一旦用户已经针对某对选项投过票了，我们就不应该允许他们再次投票，难道我们应该在组件内部维护某种状态么？不，我们需要保证我们的组件是纯粹的，所以我们需要分离这个逻辑，组件需要一个hasVoted属性，我们先硬编码传递给它： //src/index.jsx import React from &apos;react&apos;; import Voting from &apos;./components/Voting&apos;; const pair = [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;]; React.render( &lt;Voting pair={pair} hasVoted=&quot;Trainspotting&quot; /&gt;, document.getElementById(&apos;app&apos;) ); 我们可以简单的修改一下组件即可： //src/components/Voting.jsx import React from &apos;react&apos;; export default React.createClass({ getPair: function() { return this.props.pair || []; }, isDisabled: function() { return !!this.props.hasVoted; }, render: function() { return &lt;div className=&quot;voting&quot;&gt; {this.getPair().map(entry =&gt; &lt;button key={entry} disabled={this.isDisabled()} onClick={() =&gt; this.props.vote(entry)}&gt; &lt;h1&gt;{entry}&lt;/h1&gt; &lt;/button&gt; )} &lt;/div&gt;; } }); 让我们再为按钮添加一个提示，当用户投票完毕后，在选中的项目上添加标识，这样用户就更容易理解： //src/components/Voting.jsx import React from &apos;react&apos;; export default React.createClass({ getPair: function() { return this.props.pair || []; }, isDisabled: function() { return !!this.props.hasVoted; }, hasVotedFor: function(entry) { return this.props.hasVoted === entry; }, render: function() { return &lt;div className=&quot;voting&quot;&gt; {this.getPair().map(entry =&gt; &lt;button key={entry} disabled={this.isDisabled()} onClick={() =&gt; this.props.vote(entry)}&gt; &lt;h1&gt;{entry}&lt;/h1&gt; {this.hasVotedFor(entry) ? &lt;div className=&quot;label&quot;&gt;Voted&lt;/div&gt; : null} &lt;/button&gt; )} &lt;/div&gt;; } }); 投票界面最后要添加的，就是获胜者样式。我们可能需要添加新的props： //src/index.jsx import React from &apos;react&apos;; import Voting from &apos;./components/Voting&apos;; const pair = [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;]; React.render( &lt;Voting pair={pair} winner=&quot;Trainspotting&quot; /&gt;, document.getElementById(&apos;app&apos;) ); 我们再次修改一下组件： //src/components/Voting.jsx import React from &apos;react&apos;; export default React.createClass({ getPair: function() { return this.props.pair || []; }, isDisabled: function() { return !!this.props.hasVoted; }, hasVotedFor: function(entry) { return this.props.hasVoted === entry; }, render: function() { return &lt;div className=&quot;voting&quot;&gt; {this.props.winner ? &lt;div ref=&quot;winner&quot;&gt;Winner is {this.props.winner}!&lt;/div&gt; : this.getPair().map(entry =&gt; &lt;button key={entry} disabled={this.isDisabled()} onClick={() =&gt; this.props.vote(entry)}&gt; &lt;h1&gt;{entry}&lt;/h1&gt; {this.hasVotedFor(entry) ? &lt;div className=&quot;label&quot;&gt;Voted&lt;/div&gt; : null} &lt;/button&gt; )} &lt;/div&gt;; } }); 目前我们已经完成了所有要做的，但是render函数看着有点丑陋，如果我们可以把胜利界面独立成新的组件可能会好一些： //src/components/Winner.jsx import React from &apos;react&apos;; export default React.createClass({ render: function() { return &lt;div className=&quot;winner&quot;&gt; Winner is {this.props.winner}! &lt;/div&gt;; } }); 这样投票组件就会变得很简单，它只需关注投票按钮逻辑即可： //src/components/Vote.jsx import React from &apos;react&apos;; export default React.createClass({ getPair: function() { return this.props.pair || []; }, isDisabled: function() { return !!this.props.hasVoted; }, hasVotedFor: function(entry) { return this.props.hasVoted === entry; }, render: function() { return &lt;div className=&quot;voting&quot;&gt; {this.getPair().map(entry =&gt; &lt;button key={entry} disabled={this.isDisabled()} onClick={() =&gt; this.props.vote(entry)}&gt; &lt;h1&gt;{entry}&lt;/h1&gt; {this.hasVotedFor(entry) ? &lt;div className=&quot;label&quot;&gt;Voted&lt;/div&gt; : null} &lt;/button&gt; )} &lt;/div&gt;; } }); 最后我们只需要在Voting组件做一下判断即可： //src/components/Voting.jsx import React from &apos;react&apos;; import Winner from &apos;./Winner&apos;; import Vote from &apos;./Vote&apos;; export default React.createClass({ render: function() { return &lt;div&gt; {this.props.winner ? &lt;Winner ref=&quot;winner&quot; winner={this.props.winner} /&gt; : &lt;Vote {...this.props} /&gt;} &lt;/div&gt;; } }); 注意这里我们为胜利组件添加了ref，这是因为我们将在单元测试中利用它获取DOM节点。 这就是我们的纯组件！注意目前我们还没有实现任何逻辑：我们并没有定义按钮的点击操作。组件只是用来渲染UI，其它什么都不需要做。后面当我们将UI与Redux Store结合时才会涉及到应用逻辑。 继续下一步之前我们要为刚才新增的特性写更多的单元测试代码。首先，hasVoted属性将会使按钮改变状态： //test/components/Voting_spec.jsx it(&apos;disables buttons when user has voted&apos;, () =&gt; { const component = renderIntoDocument( &lt;Voting pair={[&quot;Trainspotting&quot;, &quot;28 Days Later&quot;]} hasVoted=&quot;Trainspotting&quot; /&gt; ); const buttons = scryRenderedDOMComponentsWithTag(component, &apos;button&apos;); expect(buttons.length).to.equal(2); expect(buttons[0].getDOMNode().hasAttribute(&apos;disabled&apos;)).to.equal(true); expect(buttons[1].getDOMNode().hasAttribute(&apos;disabled&apos;)).to.equal(true); }); 被hasVoted匹配的按钮将显示Voted标签： //test/components/Voting_spec.jsx it(&apos;adds label to the voted entry&apos;, () =&gt; { const component = renderIntoDocument( &lt;Voting pair={[&quot;Trainspotting&quot;, &quot;28 Days Later&quot;]} hasVoted=&quot;Trainspotting&quot; /&gt; ); const buttons = scryRenderedDOMComponentsWithTag(component, &apos;button&apos;); expect(buttons[0].getDOMNode().textContent).to.contain(&apos;Voted&apos;); }); 当获胜者产生，界面将不存在按钮，取而代替的是胜利者元素： //test/components/Voting_spec.jsx it(&apos;renders just the winner when there is one&apos;, () =&gt; { const component = renderIntoDocument( &lt;Voting winner=&quot;Trainspotting&quot; /&gt; ); const buttons = scryRenderedDOMComponentsWithTag(component, &apos;button&apos;); expect(buttons.length).to.equal(0); const winner = React.findDOMNode(component.refs.winner); expect(winner).to.be.ok; expect(winner.textContent).to.contain(&apos;Trainspotting&apos;); }); ###不可变数据和纯粹渲染 我们之前已经讨论了许多关于不可变数据的红利，但是，当它和react结合时还会有一个非常屌的好处：如果我们创建纯react组件并传递给它不可变数据作为属性参数，我们将会让react在组件渲染检测中得到最大性能。 这是靠react提供的PureRenderMixin实现的。当该mixin添加到组件中后，组件的更新检查逻辑将会被改变，由深比对改为高性能的浅比对。 我们之所以可以使用浅比对，就是因为我们使用的是不可变数据。如果一个组件的所有参数都是不可变数据，那么将大大提高应用性能。 我们可以在单元测试里更清楚的看见差别，如果我们向纯组件中传入可变数组，当数组内部元素产生改变后，组件并不会重新渲染： //test/components/Voting_spec.jsx it(&apos;renders as a pure component&apos;, () =&gt; { const pair = [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;]; const component = renderIntoDocument( &lt;Voting pair={pair} /&gt; ); let firstButton = scryRenderedDOMComponentsWithTag(component, &apos;button&apos;)[0]; expect(firstButton.getDOMNode().textContent).to.equal(&apos;Trainspotting&apos;); pair[0] = &apos;Sunshine&apos;; component.setProps({pair: pair}); firstButton = scryRenderedDOMComponentsWithTag(component, &apos;button&apos;)[0]; expect(firstButton.getDOMNode().textContent).to.equal(&apos;Trainspotting&apos;); }); 如果我们使用不可变数据，则完全没有问题： //test/components/Voting_spec.jsx import React from &apos;react/addons&apos;; import {List} from &apos;immutable&apos;; import Voting from &apos;../../src/components/Voting&apos;; import {expect} from &apos;chai&apos;; const {renderIntoDocument, scryRenderedDOMComponentsWithTag, Simulate} = React.addons.TestUtils; describe(&apos;Voting&apos;, () =&gt; { // ... it(&apos;does update DOM when prop changes&apos;, () =&gt; { const pair = List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;); const component = renderIntoDocument( &lt;Voting pair={pair} /&gt; ); let firstButton = scryRenderedDOMComponentsWithTag(component, &apos;button&apos;)[0]; expect(firstButton.getDOMNode().textContent).to.equal(&apos;Trainspotting&apos;); const newPair = pair.set(0, &apos;Sunshine&apos;); component.setProps({pair: newPair}); firstButton = scryRenderedDOMComponentsWithTag(component, &apos;button&apos;)[0]; expect(firstButton.getDOMNode().textContent).to.equal(&apos;Sunshine&apos;); }); }); 如果你跑上面的两个测试，你将会看到非预期的结果：因为实际上UI在两种场景下都更新了。那是因为现在组件依然使用的是深比对，这正是我们使用不可变数据想极力避免的。 下面我们在组件中引入mixin，你就会拿到期望的结果了： //src/components/Voting.jsx import React from &apos;react/addons&apos;; import Winner from &apos;./Winner&apos;; import Vote from &apos;./Vote&apos;; export default React.createClass({ mixins: [React.addons.PureRenderMixin], // ... }); //src/components/Vote.jsx import React from &apos;react/addons&apos;; export default React.createClass({ mixins: [React.addons.PureRenderMixin], // ... }); //src/components/Winner.jsx import React from &apos;react/addons&apos;; export default React.createClass({ mixins: [React.addons.PureRenderMixin], // ... }); ###投票结果页面和路由实现 投票页面已经搞定了，让我们开始实现投票结果页面吧。 投票结果页面依然会显示两个条目，并且显示它们各自的票数。此外屏幕下方还会有一个按钮，供用户切换到下一轮投票。 现在我们根据什么来确定显示哪个界面呢？使用URL是个不错的主意：我们可以设置根路径#/去显示投票页面，使用#/results来显示投票结果页面。 我们使用react-router可以很容易实现这个需求。让我们加入项目： npm install --save react-router 我们这里使用的react-router的0.13版本，它的1.0版本官方还没有发布，如果你打算使用其1.0RC版，那么下面的代码你可能需要做一些修改，可以看router文档。 我们现在可以来配置一下路由路径，Router提供了一个Route组件用来让我们定义路由信息，同时也提供了DefaultRoute组件来让我们定义默认路由： //src/index.jsx import React from &apos;react&apos;; import {Route, DefaultRoute} from &apos;react-router&apos;; import App from &apos;./components/App&apos;; import Voting from &apos;./components/Voting&apos;; const pair = [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;]; const routes = &lt;Route handler={App}&gt; &lt;DefaultRoute handler={Voting} /&gt; &lt;/Route&gt;; React.render( &lt;Voting pair={pair} /&gt;, document.getElementById(&apos;app&apos;) ); 我们定义了一个默认的路由指向我们的Voting组件。我们需要定义个App组件来用于Route使用。 根路由的作用就是为应用指定一个根组件：通常该组件充当所有子页面的模板。让我们来看看App的细节： //src/components/App.jsx import React from &apos;react&apos;; import {RouteHandler} from &apos;react-router&apos;; import {List} from &apos;immutable&apos;; const pair = List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;); export default React.createClass({ render: function() { return &lt;RouteHandler pair={pair} /&gt; } }); 这个组件除了渲染了一个RouteHandler组件并没有做别的，这个组件同样是react-router提供的，它的作用就是每当路由匹配了某个定义的页面后将对应的页面组件插入到这个位置。目前我们只定义了一个默认路由指向Voting，所以目前我们的组件总是会显示Voting界面。 注意，我们将我们硬编码的投票数据从index.jsx移到了App.jsx，当你给RouteHandler传递了属性值时，这些参数将会传给当前路由对应的组件。 现在我们可以更新index.jsx： //src/index.jsx import React from &apos;react&apos;; import Router, {Route, DefaultRoute} from &apos;react-router&apos;; import App from &apos;./components/App&apos;; import Voting from &apos;./components/Voting&apos;; const routes = &lt;Route handler={App}&gt; &lt;DefaultRoute handler={Voting} /&gt; &lt;/Route&gt;; Router.run(routes, (Root) =&gt; { React.render( &lt;Root /&gt;, document.getElementById(&apos;app&apos;) ); }); run方法会根据当前浏览器的路径去查找定义的router来决定渲染哪个组件。一旦确定了对应的组件，它将会被当作指定的Root传给run的回调函数，在回调中我们将使用React.render将其插入DOM中。 目前为止我们已经基于React router实现了之前的内容，我们现在可以很容易添加更多新的路由到应用。让我们把投票结果页面添加进去吧： //src/index.jsx import React from &apos;react&apos;; import Router, {Route, DefaultRoute} from &apos;react-router&apos;; import App from &apos;./components/App&apos;; import Voting from &apos;./components/Voting&apos;; import Results from &apos;./components/Results&apos;; const routes = &lt;Route handler={App}&gt; &lt;Route path=&quot;/results&quot; handler={Results} /&gt; &lt;DefaultRoute handler={Voting} /&gt; &lt;/Route&gt;; Router.run(routes, (Root) =&gt; { React.render( &lt;Root /&gt;, document.getElementById(&apos;app&apos;) ); }); 这里我们用使用&lt;Route&gt;组件定义了一个名为/results的路径，并绑定Results组件。 让我们简单的实现一下这个Results组件，这样我们就可以看一下路由是如何工作的了： //src/components/Results.jsx import React from &apos;react/addons&apos;; export default React.createClass({ mixins: [React.addons.PureRenderMixin], render: function() { return &lt;div&gt;Hello from results!&lt;/div&gt; } }); 如果你在浏览器中输入http://localhost:8080/#/results，你将会看到该结果组件。而其它路径都对应这投票页面，你也可以使用浏览器的前后按钮来切换这两个界面。 接下来我们来实际实现一下结果组件： //src/components/Results.jsx import React from &apos;react/addons&apos;; export default React.createClass({ mixins: [React.addons.PureRenderMixin], getPair: function() { return this.props.pair || []; }, render: function() { return &lt;div className=&quot;results&quot;&gt; {this.getPair().map(entry =&gt; &lt;div key={entry} className=&quot;entry&quot;&gt; &lt;h1&gt;{entry}&lt;/h1&gt; &lt;/div&gt; )} &lt;/div&gt;; } }); 结果界面除了显示投票项外，还应该显示它们对应的得票数，让我们先硬编码一下： //src/components/App.jsx import React from &apos;react/addons&apos;; import {RouteHandler} from &apos;react-router&apos;; import {List, Map} from &apos;immutable&apos;; const pair = List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;); const tally = Map({&apos;Trainspotting&apos;: 5, &apos;28 Days Later&apos;: 4}); export default React.createClass({ render: function() { return &lt;RouteHandler pair={pair} tally={tally} /&gt; } }); 现在，我们再来修改一下结果组件： //src/components/Results.jsx import React from &apos;react/addons&apos;; export default React.createClass({ mixins: [React.addons.PureRenderMixin], getPair: function() { return this.props.pair || []; }, getVotes: function(entry) { if (this.props.tally &amp;&amp; this.props.tally.has(entry)) { return this.props.tally.get(entry); } return 0; }, render: function() { return &lt;div className=&quot;results&quot;&gt; {this.getPair().map(entry =&gt; &lt;div key={entry} className=&quot;entry&quot;&gt; &lt;h1&gt;{entry}&lt;/h1&gt; &lt;div className=&quot;voteCount&quot;&gt; {this.getVotes(entry)} &lt;/div&gt; &lt;/div&gt; )} &lt;/div&gt;; } }); 现在我们来针对目前的界面功能编写测试代码，以防止未来我们破坏这些功能。 我们期望组件为每个选项都渲染一个div，并在其中显示选项的名称和票数。如果对应的选项没有票数，则默认显示0： //test/components/Results_spec.jsx import React from &apos;react/addons&apos;; import {List, Map} from &apos;immutable&apos;; import Results from &apos;../../src/components/Results&apos;; import {expect} from &apos;chai&apos;; const {renderIntoDocument, scryRenderedDOMComponentsWithClass} = React.addons.TestUtils; describe(&apos;Results&apos;, () =&gt; { it(&apos;renders entries with vote counts or zero&apos;, () =&gt; { const pair = List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;); const tally = Map({&apos;Trainspotting&apos;: 5}); const component = renderIntoDocument( &lt;Results pair={pair} tally={tally} /&gt; ); const entries = scryRenderedDOMComponentsWithClass(component, &apos;entry&apos;); const [train, days] = entries.map(e =&gt; e.getDOMNode().textContent); expect(entries.length).to.equal(2); expect(train).to.contain(&apos;Trainspotting&apos;); expect(train).to.contain(&apos;5&apos;); expect(days).to.contain(&apos;28 Days Later&apos;); expect(days).to.contain(&apos;0&apos;); }); }); 接下来，我们看一下”Next”按钮，它允许用户切换到下一轮投票。 我们的组件应该包含一个回调函数属性参数，当组件中的”Next”按钮被点击后，该回调函数将会被调用。我们来写一下这个操作的测试代码： //test/components/Results_spec.jsx import React from &apos;react/addons&apos;; import {List, Map} from &apos;immutable&apos;; import Results from &apos;../../src/components/Results&apos;; import {expect} from &apos;chai&apos;; const {renderIntoDocument, scryRenderedDOMComponentsWithClass, Simulate} = React.addons.TestUtils; describe(&apos;Results&apos;, () =&gt; { // ... it(&apos;invokes the next callback when next button is clicked&apos;, () =&gt; { let nextInvoked = false; const next = () =&gt; nextInvoked = true; const pair = List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;); const component = renderIntoDocument( &lt;Results pair={pair} tally={Map()} next={next}/&gt; ); Simulate.click(React.findDOMNode(component.refs.next)); expect(nextInvoked).to.equal(true); }); }); 写法和之前的投票按钮很类似吧。接下来让我们更新一下结果组件： //src/components/Results.jsx import React from &apos;react/addons&apos;; export default React.createClass({ mixins: [React.addons.PureRenderMixin], getPair: function() { return this.props.pair || []; }, getVotes: function(entry) { if (this.props.tally &amp;&amp; this.props.tally.has(entry)) { return this.props.tally.get(entry); } return 0; }, render: function() { return &lt;div className=&quot;results&quot;&gt; &lt;div className=&quot;tally&quot;&gt; {this.getPair().map(entry =&gt; &lt;div key={entry} className=&quot;entry&quot;&gt; &lt;h1&gt;{entry}&lt;/h1&gt; &lt;div class=&quot;voteCount&quot;&gt; {this.getVotes(entry)} &lt;/div&gt; &lt;/div&gt; )} &lt;/div&gt; &lt;div className=&quot;management&quot;&gt; &lt;button ref=&quot;next&quot; className=&quot;next&quot; onClick={this.props.next}&gt; Next &lt;/button&gt; &lt;/div&gt; &lt;/div&gt;; } }); 最终投票结束，结果页面和投票页面一样，都要显示胜利者： //test/components/Results_spec.jsx it(&apos;renders the winner when there is one&apos;, () =&gt; { const component = renderIntoDocument( &lt;Results winner=&quot;Trainspotting&quot; pair={[&quot;Trainspotting&quot;, &quot;28 Days Later&quot;]} tally={Map()} /&gt; ); const winner = React.findDOMNode(component.refs.winner); expect(winner).to.be.ok; expect(winner.textContent).to.contain(&apos;Trainspotting&apos;); }); 我们可以想在投票界面中那样简单的实现一下上面的逻辑： //src/components/Results.jsx import React from &apos;react/addons&apos;; import Winner from &apos;./Winner&apos;; export default React.createClass({ mixins: [React.addons.PureRenderMixin], getPair: function() { return this.props.pair || []; }, getVotes: function(entry) { if (this.props.tally &amp;&amp; this.props.tally.has(entry)) { return this.props.tally.get(entry); } return 0; }, render: function() { return this.props.winner ? &lt;Winner ref=&quot;winner&quot; winner={this.props.winner} /&gt; : &lt;div className=&quot;results&quot;&gt; &lt;div className=&quot;tally&quot;&gt; {this.getPair().map(entry =&gt; &lt;div key={entry} className=&quot;entry&quot;&gt; &lt;h1&gt;{entry}&lt;/h1&gt; &lt;div className=&quot;voteCount&quot;&gt; {this.getVotes(entry)} &lt;/div&gt; &lt;/div&gt; )} &lt;/div&gt; &lt;div className=&quot;management&quot;&gt; &lt;button ref=&quot;next&quot; className=&quot;next&quot; onClick={this.props.next}&gt; Next &lt;/button&gt; &lt;/div&gt; &lt;/div&gt;; } }); 到目前为止，我们已经实现了应用的UI，虽然现在它们并没有和真实数据和操作整合起来。这很不错不是么？我们只需要一些占位符数据就可以完成界面的开发，这让我们在这个阶段更专注于UI。 接下来我们将会使用Redux Store来将真实数据整合到我们的界面中。 ###初识客户端的Redux Store Redux将会充当我们UI界面的状态容器，我们已经在服务端用过Redux，之前说的很多内容在这里也受用。现在我们已经准备好要在React应用中使用Redux了，这也是Redux更常见的使用场景。 和在服务端一样，我们先来思考一下应用的状态。客户端的状态和服务端会非常的类似。 我们有两个界面，并在其中需要显示成对的用于投票的条目： 此外，结果页面需要显示票数： 投票组件还需要记录当前用户已经投票过的选项： 结果组件还需要记录胜利者： 注意这里除了hasVoted外，其它都映射着服务端状态的子集。 接下来我们来思考一下应用的核心逻辑，actions和reducers应该是什么样的。 我们先来想想能够导致应用状态改变的操作都有那些？状态改变的来源之一是用户行为。我们的UI中存在两种可能的用户操作行为： 用户在投票页面点击某个投票按钮； 用户点击下一步按钮。 另外，我们知道我们的服务端会将应用当前状态发送给客户端，我们将编写代码来接受状态数据，这也是导致状态改变的来源之一。 我们可以从服务端状态更新开始，之前我们在服务端设置发送了一个state事件。该事件将携带我们之前设计的客户端状态树的状态数据。我们的客户端reducer将通过一个action来将服务器端的状态数据合并到客户端状态树中，这个action如下： { type: &apos;SET_STATE&apos;, state: { vote: {...} } } 让我们先写一下reducer测试代码，它应该接受上面定义的那种action，并合并数据到客户端的当前状态中： //test/reducer_spec.js import {List, Map, fromJS} from &apos;immutable&apos;; import {expect} from &apos;chai&apos;; import reducer from &apos;../src/reducer&apos;; describe(&apos;reducer&apos;, () =&gt; { it(&apos;handles SET_STATE&apos;, () =&gt; { const initialState = Map(); const action = { type: &apos;SET_STATE&apos;, state: Map({ vote: Map({ pair: List.of(&apos;Trainspotting&apos;, &apos;28 Days Later&apos;), tally: Map({Trainspotting: 1}) }) }) }; const nextState = reducer(initialState, action); expect(nextState).to.equal(fromJS({ vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;], tally: {Trainspotting: 1} } })); }); }); 这个renducers接受一个来自socket发送的原始的js数据结构，这里注意不是不可变数据类型哦。我们需要在返回前将其转换成不可变数据类型： //test/reducer_spec.js it(&apos;handles SET_STATE with plain JS payload&apos;, () =&gt; { const initialState = Map(); const action = { type: &apos;SET_STATE&apos;, state: { vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;], tally: {Trainspotting: 1} } } }; const nextState = reducer(initialState, action); expect(nextState).to.equal(fromJS({ vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;], tally: {Trainspotting: 1} } })); }); reducer同样应该可以正确的处理undefined初始化状态： //test/reducer_spec.js it(&apos;handles SET_STATE without initial state&apos;, () =&gt; { const action = { type: &apos;SET_STATE&apos;, state: { vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;], tally: {Trainspotting: 1} } } }; const nextState = reducer(undefined, action); expect(nextState).to.equal(fromJS({ vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;], tally: {Trainspotting: 1} } })); }); 现在我们来看一下如何实现满足上面测试条件的reducer： //src/reducer.js import {Map} from &apos;immutable&apos;; export default function(state = Map(), action) { return state; } reducer需要处理SET_STATE动作。在这个动作的处理中，我们应该将传入的状态数据和现有的进行合并，使用Map提供的merge将很容易来实现这个操作： //src/reducer.js import {Map} from &apos;immutable&apos;; function setState(state, newState) { return state.merge(newState); } export default function(state = Map(), action) { switch (action.type) { case &apos;SET_STATE&apos;: return setState(state, action.state); } return state; } 注意这里我们并没有单独写一个核心模块，而是直接在reducer中添加了个简单的setState函数来做业务逻辑。这是因为现在这个逻辑还很简单～ 关于改变用户状态的那两个用户交互：投票和下一步，它们都需要和服务端进行通信，我们一会再说。我们现在先把redux添加到项目中： npm install --save redux index.jsx入口文件是一个初始化Store的好地方，让我们暂时先使用硬编码的数据来做： //src/index.jsx import React from &apos;react&apos;; import Router, {Route, DefaultRoute} from &apos;react-router&apos;; import {createStore} from &apos;redux&apos;; import reducer from &apos;./reducer&apos;; import App from &apos;./components/App&apos;; import Voting from &apos;./components/Voting&apos;; import Results from &apos;./components/Results&apos;; const store = createStore(reducer); store.dispatch({ type: &apos;SET_STATE&apos;, state: { vote: { pair: [&apos;Sunshine&apos;, &apos;28 Days Later&apos;], tally: {Sunshine: 2} } } }); const routes = &lt;Route handler={App}&gt; &lt;Route path=&quot;/results&quot; handler={Results} /&gt; &lt;DefaultRoute handler={Voting} /&gt; &lt;/Route&gt;; Router.run(routes, (Root) =&gt; { React.render( &lt;Root /&gt;, document.getElementById(&apos;app&apos;) ); }); 那么，我们如何在react组件中从Store中获取数据呢？ ###让React从Redux中获取数据 我们已经创建了一个使用不可变数据类型保存应用状态的Redux Store。我们还拥有接受不可变数据为参数的无状态的纯React组件。如果我们能使这些组件从Store中获取最新的状态数据，那真是极好的。当状态变化时，React会重新渲染组件，pure render mixin可以使得我们的UI避免不必要的重复渲染。 相比我们自己手动实现同步代码，我们更推荐使用[react-redux][https://github.com/rackt/react-redux]包来做： npm install --save react-redux 这个库主要做的是： 映射Store的状态到组件的输入props中； 映射actions到组件的回调props中。 为了让它可以正常工作，我们需要将顶层的应用组件嵌套在react-redux的Provider组件中。这将把Redux Store和我们的状态树连接起来。 我们将让Provider包含路由的根组件，这样会使得Provider成为整个应用组件的根节点： //src/index.jsx import React from &apos;react&apos;; import Router, {Route, DefaultRoute} from &apos;react-router&apos;; import {createStore} from &apos;redux&apos;; import {Provider} from &apos;react-redux&apos;; import reducer from &apos;./reducer&apos;; import App from &apos;./components/App&apos;; import {VotingContainer} from &apos;./components/Voting&apos;; import Results from &apos;./components/Results&apos;; const store = createStore(reducer); store.dispatch({ type: &apos;SET_STATE&apos;, state: { vote: { pair: [&apos;Sunshine&apos;, &apos;28 Days Later&apos;], tally: {Sunshine: 2} } } }); const routes = &lt;Route handler={App}&gt; &lt;Route path=&quot;/results&quot; handler={Results} /&gt; &lt;DefaultRoute handler={VotingContainer} /&gt; &lt;/Route&gt;; Router.run(routes, (Root) =&gt; { React.render( &lt;Provider store={store}&gt; {() =&gt; &lt;Root /&gt;} &lt;/Provider&gt;, document.getElementById(&apos;app&apos;) ); }); 接下来我们要考虑一下，我们的那些组件需要绑定到Store上。我们一共有5个组件，可以分成三类： 根组件App不需要绑定任何数据； Vote和Winner组件只使用父组件传递来的数据，所以它们也不需要绑定； 剩下的组件（Voting和Results）目前都是使用的硬编码数据，我们现在需要将其绑定到Store上。 让我们从Voting组件开始。使用react-redux我们得到一个叫connect的函数： connect(mapStateToProps)(SomeComponent); 该函数的作用就是将Redux Store中的状态数据映射到props对象中。这个props对象将会用于连接到的组件中。在我们的Voting场景中，我们需要从状态中拿到pair和winner值： //src/components/Voting.jsx import React from &apos;react/addons&apos;; import {connect} from &apos;react-redux&apos;; import Winner from &apos;./Winner&apos;; import Vote from &apos;./Vote&apos;; const Voting = React.createClass({ mixins: [React.addons.PureRenderMixin], render: function() { return &lt;div&gt; {this.props.winner ? &lt;Winner ref=&quot;winner&quot; winner={this.props.winner} /&gt; : &lt;Vote {...this.props} /&gt;} &lt;/div&gt;; } }); function mapStateToProps(state) { return { pair: state.getIn([&apos;vote&apos;, &apos;pair&apos;]), winner: state.get(&apos;winner&apos;) }; } connect(mapStateToProps)(Voting); export default Voting; 在上面的代码中，connect函数并没有修改Voting组件本身，Voting组件依然保持这纯粹性。而connect返回的是一个Voting组件的连接版，我们称之为VotingContainer： //src/components/Voting.jsx import React from &apos;react/addons&apos;; import {connect} from &apos;react-redux&apos;; import Winner from &apos;./Winner&apos;; import Vote from &apos;./Vote&apos;; export const Voting = React.createClass({ mixins: [React.addons.PureRenderMixin], render: function() { return &lt;div&gt; {this.props.winner ? &lt;Winner ref=&quot;winner&quot; winner={this.props.winner} /&gt; : &lt;Vote {...this.props} /&gt;} &lt;/div&gt;; } }); function mapStateToProps(state) { return { pair: state.getIn([&apos;vote&apos;, &apos;pair&apos;]), winner: state.get(&apos;winner&apos;) }; } export const VotingContainer = connect(mapStateToProps)(Voting); 这样，这个模块现在导出两个组件：一个纯Voting组件，一个连接后的VotingContainer版本。react-redux官方称前者为“蠢”组件，后者则称为”智能”组件。我更倾向于用“pure”和“connected”来描述它们。怎么称呼随你便，主要是明白它们之间的差别： 纯组件完全靠给它传入的props来工作，这非常类似一个纯函数； 连接组件则封装了纯组件和一些逻辑用来与Redux Store协同工作，这些特性是redux-react提供的。 我们得更新一下路由表，改用VotingContainer。一旦修改完毕，我们的投票界面将会使用来自Redux Store的数据： //src/index.jsx import React from &apos;react&apos;; import Router, {Route, DefaultRoute} from &apos;react-router&apos;; import {createStore} from &apos;redux&apos;; import {Provider} from &apos;react-redux&apos;; import reducer from &apos;./reducer&apos;; import App from &apos;./components/App&apos;; import {VotingContainer} from &apos;./components/Voting&apos;; import Results from &apos;./components/Results&apos;; const store = createStore(reducer); store.dispatch({ type: &apos;SET_STATE&apos;, state: { vote: { pair: [&apos;Sunshine&apos;, &apos;28 Days Later&apos;], tally: {Sunshine: 2} } } }); const routes = &lt;Route handler={App}&gt; &lt;Route path=&quot;/results&quot; handler={Results} /&gt; &lt;DefaultRoute handler={VotingContainer} /&gt; &lt;/Route&gt;; Router.run(routes, (Root) =&gt; { React.render( &lt;Provider store={store}&gt; {() =&gt; &lt;Root /&gt;} &lt;/Provider&gt;, document.getElementById(&apos;app&apos;) ); }); 而在对应的测试代码中，我们则需要使用纯Voting组件定义： //test/components/Voting_spec.jsx import React from &apos;react/addons&apos;; import {List} from &apos;immutable&apos;; import {Voting} from &apos;../../src/components/Voting&apos;; import {expect} from &apos;chai&apos;; 其它地方不需要修改了。 现在我们来如法炮制投票结果页面： //src/components/Results.jsx import React from &apos;react/addons&apos;; import {connect} from &apos;react-redux&apos;; import Winner from &apos;./Winner&apos;; export const Results = React.createClass({ mixins: [React.addons.PureRenderMixin], getPair: function() { return this.props.pair || []; }, getVotes: function(entry) { if (this.props.tally &amp;&amp; this.props.tally.has(entry)) { return this.props.tally.get(entry); } return 0; }, render: function() { return this.props.winner ? &lt;Winner ref=&quot;winner&quot; winner={this.props.winner} /&gt; : &lt;div className=&quot;results&quot;&gt; &lt;div className=&quot;tally&quot;&gt; {this.getPair().map(entry =&gt; &lt;div key={entry} className=&quot;entry&quot;&gt; &lt;h1&gt;{entry}&lt;/h1&gt; &lt;div className=&quot;voteCount&quot;&gt; {this.getVotes(entry)} &lt;/div&gt; &lt;/div&gt; )} &lt;/div&gt; &lt;div className=&quot;management&quot;&gt; &lt;button ref=&quot;next&quot; className=&quot;next&quot; onClick={this.props.next}&gt; Next &lt;/button&gt; &lt;/div&gt; &lt;/div&gt;; } }); function mapStateToProps(state) { return { pair: state.getIn([&apos;vote&apos;, &apos;pair&apos;]), tally: state.getIn([&apos;vote&apos;, &apos;tally&apos;]), winner: state.get(&apos;winner&apos;) } } export const ResultsContainer = connect(mapStateToProps)(Results); 同样我们需要修改index.jsx来使用新的ResultsContainer： //src/index.jsx import React from &apos;react&apos;; import Router, {Route, DefaultRoute} from &apos;react-router&apos;; import {createStore} from &apos;redux&apos;; import {Provider} from &apos;react-redux&apos;; import reducer from &apos;./reducer&apos;; import App from &apos;./components/App&apos;; import {VotingContainer} from &apos;./components/Voting&apos;; import {ResultsContainer} from &apos;./components/Results&apos;; const store = createStore(reducer); store.dispatch({ type: &apos;SET_STATE&apos;, state: { vote: { pair: [&apos;Sunshine&apos;, &apos;28 Days Later&apos;], tally: {Sunshine: 2} } } }); const routes = &lt;Route handler={App}&gt; &lt;Route path=&quot;/results&quot; handler={ResultsContainer} /&gt; &lt;DefaultRoute handler={VotingContainer} /&gt; &lt;/Route&gt;; Router.run(routes, (Root) =&gt; { React.render( &lt;Provider store={store}&gt; {() =&gt; &lt;Root /&gt;} &lt;/Provider&gt;, document.getElementById(&apos;app&apos;) ); }); 不要忘记修改测试代码啊： //test/components/Results_spec.jsx import React from &apos;react/addons&apos;; import {List, Map} from &apos;immutable&apos;; import {Results} from &apos;../../src/components/Results&apos;; import {expect} from &apos;chai&apos;; 现在你已经知道如何让纯react组件与Redux Store整合了。 对于一些只有一个根组件且没有路由的小应用，直接连接根组件就足够了。根组件会将状态数据传递给它的子组件。而对于那些使用路由，就像我们的场景，连接每一个路由指向的处理函数是个好主意。但是分别为每个组件编写连接代码并不适合所有的软件场景。我觉得保持组件props尽可能清晰明了是个非常好的习惯，因为它可以让你很容易清楚组件需要哪些数据，你就可以更容易管理那些连接代码。 现在让我们开始把Redux数据对接到UI里，我们再也不需要那些App.jsx中手写的硬编码数据了，这样我们的App.jsx将会变得简单： //src/components/App.jsx import React from &apos;react&apos;; import {RouteHandler} from &apos;react-router&apos;; export default React.createClass({ render: function() { return &lt;RouteHandler /&gt; } }); ###设置socket.io客户端 现在我们已经创建好了客户端的Redux应用，我们接下来将讨论如何让其与我们之前开发的服务端应用进行对接。 服务端已经准备好接受socket连接，并为其进行投票数据的发送。而我们的客户端也已经可以使用Redux Store很方便的接受数据了。我们剩下的工作就是把它们连接起来。 我们需要使用socket.io从浏览器向服务端创建一个连接，我们可以使用socket.io-client库来完成这个目的： npm install --save socket.io-client 这个库赋予了我们连接Socket.io服务端的能力，让我们连接之前写好的服务端，端口号8090（注意使用和后端匹配的端口）： //src/index.jsx import React from &apos;react&apos;; import Router, {Route, DefaultRoute} from &apos;react-router&apos;; import {createStore} from &apos;redux&apos;; import {Provider} from &apos;react-redux&apos;; import io from &apos;socket.io-client&apos;; import reducer from &apos;./reducer&apos;; import App from &apos;./components/App&apos;; import {VotingContainer} from &apos;./components/Voting&apos;; import {ResultsContainer} from &apos;./components/Results&apos;; const store = createStore(reducer); store.dispatch({ type: &apos;SET_STATE&apos;, state: { vote: { pair: [&apos;Sunshine&apos;, &apos;28 Days Later&apos;], tally: {Sunshine: 2} } } }); const socket = io(`${location.protocol}//${location.hostname}:8090`); const routes = &lt;Route handler={App}&gt; &lt;Route path=&quot;/results&quot; handler={ResultsContainer} /&gt; &lt;DefaultRoute handler={VotingContainer} /&gt; &lt;/Route&gt;; Router.run(routes, (Root) =&gt; { React.render( &lt;Provider store={store}&gt; {() =&gt; &lt;Root /&gt;} &lt;/Provider&gt;, document.getElementById(&apos;app&apos;) ); }); 你必须先确保你的服务端已经开启了，然后在浏览器端访问客户端应用，并检查网络监控，你会发现创建了一个WebSockets连接，并且开始传输Socket.io的心跳包了。 ###接受来自服务器端的actions 我们虽然已经创建了个socket.io连接，但我们并没有用它获取任何数据。每当我们连接到服务端或服务端发生状态数据改变时，服务端会发送state事件给客户端。我们只需要监听对应的事件即可，我们在接受到事件通知后只需要简单的对我们的Store指派SET_STATEaction即可： //src/index.jsx import React from &apos;react&apos;; import Router, {Route, DefaultRoute} from &apos;react-router&apos;; import {createStore} from &apos;redux&apos;; import {Provider} from &apos;react-redux&apos;; import io from &apos;socket.io-client&apos;; import reducer from &apos;./reducer&apos;; import App from &apos;./components/App&apos;; import {VotingContainer} from &apos;./components/Voting&apos;; import {ResultsContainer} from &apos;./components/Results&apos;; const store = createStore(reducer); const socket = io(`${location.protocol}//${location.hostname}:8090`); socket.on(&apos;state&apos;, state =&gt; store.dispatch({type: &apos;SET_STATE&apos;, state}) ); const routes = &lt;Route handler={App}&gt; &lt;Route path=&quot;/results&quot; handler={ResultsContainer} /&gt; &lt;DefaultRoute handler={VotingContainer} /&gt; &lt;/Route&gt;; Router.run(routes, (Root) =&gt; { React.render( &lt;Provider store={store}&gt; {() =&gt; &lt;Root /&gt;} &lt;/Provider&gt;, document.getElementById(&apos;app&apos;) ); }); 注意我们移除了SET_STATE的硬编码，我们现在已经不需要伪造数据了。 审视我们的界面，不管是投票还是结果页面，它们都会显示服务端提供的第一对选项。服务端和客户端已经连接上了！ ###从react组件中指派actions 我们已经知道如何从Redux Store获取数据到UI中，现在来看看如何从UI中提交数据用于actions。 思考这个问题的最佳场景是投票界面上的投票按钮。之前在写相关界面时，我们假设Voting组件接受一个回调函数props。当用户点击某个按钮时组件将会调用这个回调函数。但我们目前并没有实现这个回调函数，除了在测试代码中。 当用户投票后应该做什么？投票结果应该发送给服务端，这部分我们稍后再说，客户端也需要执行一些逻辑：组件的hasVoted值应该被设置，这样用户才不会反复对同一对选项投票。 这是我们要创建的第二个客户端Redux Action，我们称之为VOTE： //test/reducer_spec.js it(&apos;handles VOTE by setting hasVoted&apos;, () =&gt; { const state = fromJS({ vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;], tally: {Trainspotting: 1} } }); const action = {type: &apos;VOTE&apos;, entry: &apos;Trainspotting&apos;}; const nextState = reducer(state, action); expect(nextState).to.equal(fromJS({ vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;], tally: {Trainspotting: 1} }, hasVoted: &apos;Trainspotting&apos; })); }); 为了更严谨，我们应该考虑一种情况：不管什么原因，当VOTEaction传递了一个不存在的选项时我们的应用该怎么做： //test/reducer_spec.js it(&apos;does not set hasVoted for VOTE on invalid entry&apos;, () =&gt; { const state = fromJS({ vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;], tally: {Trainspotting: 1} } }); const action = {type: &apos;VOTE&apos;, entry: &apos;Sunshine&apos;}; const nextState = reducer(state, action); expect(nextState).to.equal(fromJS({ vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;], tally: {Trainspotting: 1} } })); }); 下面来看看我们的reducer如何实现的： //src/reducer.js import {Map} from &apos;immutable&apos;; function setState(state, newState) { return state.merge(newState); } function vote(state, entry) { const currentPair = state.getIn([&apos;vote&apos;, &apos;pair&apos;]); if (currentPair &amp;&amp; currentPair.includes(entry)) { return state.set(&apos;hasVoted&apos;, entry); } else { return state; } } export default function(state = Map(), action) { switch (action.type) { case &apos;SET_STATE&apos;: return setState(state, action.state); case &apos;VOTE&apos;: return vote(state, action.entry); } return state; } hasVoted并不会一直保存在状态数据中，每当开始一轮新的投票时，我们应该在SET_STATEaction的处理逻辑中检查是否用户是否已经投票，如果还没，我们应该删除掉hasVoted： //test/reducer_spec.js it(&apos;removes hasVoted on SET_STATE if pair changes&apos;, () =&gt; { const initialState = fromJS({ vote: { pair: [&apos;Trainspotting&apos;, &apos;28 Days Later&apos;], tally: {Trainspotting: 1} }, hasVoted: &apos;Trainspotting&apos; }); const action = { type: &apos;SET_STATE&apos;, state: { vote: { pair: [&apos;Sunshine&apos;, &apos;Slumdog Millionaire&apos;] } } }; const nextState = reducer(initialState, action); expect(nextState).to.equal(fromJS({ vote: { pair: [&apos;Sunshine&apos;, &apos;Slumdog Millionaire&apos;] } })); }); 根据需要，我们新增一个resetVote函数来处理SET_STATE动作： //src/reducer.js import {List, Map} from &apos;immutable&apos;; function setState(state, newState) { return state.merge(newState); } function vote(state, entry) { const currentPair = state.getIn([&apos;vote&apos;, &apos;pair&apos;]); if (currentPair &amp;&amp; currentPair.includes(entry)) { return state.set(&apos;hasVoted&apos;, entry); } else { return state; } } function resetVote(state) { const hasVoted = state.get(&apos;hasVoted&apos;); const currentPair = state.getIn([&apos;vote&apos;, &apos;pair&apos;], List()); if (hasVoted &amp;&amp; !currentPair.includes(hasVoted)) { return state.remove(&apos;hasVoted&apos;); } else { return state; } } export default function(state = Map(), action) { switch (action.type) { case &apos;SET_STATE&apos;: return resetVote(setState(state, action.state)); case &apos;VOTE&apos;: return vote(state, action.entry); } return state; } 我们还需要在修改一下连接逻辑： //src/components/Voting.jsx function mapStateToProps(state) { return { pair: state.getIn([&apos;vote&apos;, &apos;pair&apos;]), hasVoted: state.get(&apos;hasVoted&apos;), winner: state.get(&apos;winner&apos;) }; } 现在我们依然需要为Voting提供一个vote回调函数，用来为Sotre指派我们新增的action。我们依然要尽力保证Voting组件的纯粹性，不应该依赖任何actions或Redux。这些工作都应该在react-redux的connect中处理。 除了连接输入参数属性，react-redux还可以用来连接output actions。开始之前，我们先来介绍一下另一个Redux的核心概念：Action creators。 如我们之前看到的，Redux actions通常就是一个简单的对象，它包含一个固有的type属性和其它内容。我们之前都是直接利用js对象字面量来直接声明所需的actions。其实可以使用一个factory函数来更好的生成actions，如下： function vote(entry) { return {type: &apos;VOTE&apos;, entry}; } 这类函数就被称为action creators。它们就是个纯函数，用来返回action对象，别的没啥好介绍得了。但是你也可以在其中实现一些内部逻辑，而避免将每次生成action都重复编写它们。使用action creators可以更好的表达所有需要分发的actions。 让我们新建一个用来声明客户端所需action的action creators文件： //src/action_creators.js export function setState(state) { return { type: &apos;SET_STATE&apos;, state }; } export function vote(entry) { return { type: &apos;VOTE&apos;, entry }; } 我们当然也可以为action creators编写测试代码，但由于我们的代码逻辑太简单了，我就不再写测试了。 现在我们可以在index.jsx中使用我们刚新增的setStateaction creator了： //src/index.jsx import React from &apos;react&apos;; import Router, {Route, DefaultRoute} from &apos;react-router&apos;; import {createStore} from &apos;redux&apos;; import {Provider} from &apos;react-redux&apos;; import io from &apos;socket.io-client&apos;; import reducer from &apos;./reducer&apos;; import {setState} from &apos;./action_creators&apos;; import App from &apos;./components/App&apos;; import {VotingContainer} from &apos;./components/Voting&apos;; import {ResultsContainer} from &apos;./components/Results&apos;; const store = createStore(reducer); const socket = io(`${location.protocol}//${location.hostname}:8090`); socket.on(&apos;state&apos;, state =&gt; store.dispatch(setState(state)) ); const routes = &lt;Route handler={App}&gt; &lt;Route path=&quot;/results&quot; handler={ResultsContainer} /&gt; &lt;DefaultRoute handler={VotingContainer} /&gt; &lt;/Route&gt;; Router.run(routes, (Root) =&gt; { React.render( &lt;Provider store={store}&gt; {() =&gt; &lt;Root /&gt;} &lt;/Provider&gt;, document.getElementById(&apos;app&apos;) ); }); 使用action creators还有一个非常优雅的特点：在我们的场景里，我们有一个需要vote回调函数props的Vote组件，我们同时拥有一个vote的action creator。它们的名字和函数签名完全一致（都接受一个用来表示选中项的参数）。现在我们只需要将action creators作为react-redux的connect函数的第二个参数，即可完成自动关联： //src/components/Voting.jsx import React from &apos;react/addons&apos;; import {connect} from &apos;react-redux&apos;; import Winner from &apos;./Winner&apos;; import Vote from &apos;./Vote&apos;; import * as actionCreators from &apos;../action_creators&apos;; export const Voting = React.createClass({ mixins: [React.addons.PureRenderMixin], render: function() { return &lt;div&gt; {this.props.winner ? &lt;Winner ref=&quot;winner&quot; winner={this.props.winner} /&gt; : &lt;Vote {...this.props} /&gt;} &lt;/div&gt;; } }); function mapStateToProps(state) { return { pair: state.getIn([&apos;vote&apos;, &apos;pair&apos;]), hasVoted: state.get(&apos;hasVoted&apos;), winner: state.get(&apos;winner&apos;) }; } export const VotingContainer = connect( mapStateToProps, actionCreators )(Voting); 这么配置后，我们的Voting组件的vote参数属性将会与voteaciton creator关联起来。这样当点击某个投票按钮后，会导致触发VOTE动作。 ###使用Redux Middleware发送actions到服务端 最后我们要做的是把用户数据提交到服务端，这种操作一般发生在用户投票，或选择跳转下一轮投票时发生。 让我们讨论一下投票操作，下面列出了投票的逻辑： 当用户进行投票，VOTEaction将产生并分派到客户端的Redux Store中； VOTEactions将触发客户端reducer进行hasVoted状态设置； 服务端监控客户端通过socket.io投递的action，它将接收到的actions分派到服务端的Redux Store; VOTEaction将触发服务端的reducer，其会创建vote数据并更新对应的票数。 这样来说，我们似乎已经都搞定了。唯一缺少的就是让客户端发送VOTEaction给服务端。这相当于两端的Redux Store相互分派action，这就是我们接下来要做的。 那么该怎么做呢？Redux并没有内建这种功能。所以我们需要设计一下何时何地来做这个工作：从客户端发送action到服务端。 Redux提供了一个通用的方法来封装action：Middleware。 Redux中间件是一个函数，每当action将要被指派，并在对应的reducer执行之前会被调用。它常用来做像日志收集，异常处理，修整action，缓存结果，控制何时以何种方式来让store接收actions等工作。这正是我们可以利用的。 注意，一定要分清Redux中间件和Redux监听器的差别：中间件被用于action将要指派给store阶段，它可以修改action对store将带来的影响。而监听器则是在action被指派后，它不能改变action的行为。 我们需要创建一个“远程action中间件”，该中间件可以让我们的action不仅仅能指派给本地的store，也可以通过socket.io连接派送给远程的store。 让我们创建这个中间件，It is a function that takes a Redux store, and returns another function that takes a “next” callback. That function returns a third function that takes a Redux action. The innermost function is where the middleware implementation will actually go（译者注：这句套绕口，请看官自行参悟）： //src/remote_action_middleware.js export default store =&gt; next =&gt; action =&gt; { } 上面这个写法看着可能有点渗人，下面调整一下让大家好理解： export default function(store) { return function(next) { return function(action) { } } } 这种嵌套接受单一参数函数的写法成为currying。这种写法主要用来简化中间件的实现：如果我们使用一个一次性接受所有参数的函数（function(store, next, action) { }），那么我们就不得不保证我们的中间件具体实现每次都要包含所有这些参数。 上面的next参数作用是在中间件中一旦完成了action的处理，就可以调用它来退出当前逻辑： //src/remote_action_middleware.js export default store =&gt; next =&gt; action =&gt; { return next(action); } 如果中间件没有调用next，则该action将丢弃，不再传到reducer或store中。 让我们写一个简单的日志中间件： //src/remote_action_middleware.js export default store =&gt; next =&gt; action =&gt; { console.log(&apos;in middleware&apos;, action); return next(action); } 我们将上面这个中间件注册到我们的Redux Store中，我们将会抓取到所有action的日志。中间件可以通过Redux提供的applyMiddleware函数绑定到我们的store中： //src/components/index.jsx import React from &apos;react&apos;; import Router, {Route, DefaultRoute} from &apos;react-router&apos;; import {createStore, applyMiddleware} from &apos;redux&apos;; import {Provider} from &apos;react-redux&apos;; import io from &apos;socket.io-client&apos;; import reducer from &apos;./reducer&apos;; import {setState} from &apos;./action_creators&apos;; import remoteActionMiddleware from &apos;./remote_action_middleware&apos;; import App from &apos;./components/App&apos;; import {VotingContainer} from &apos;./components/Voting&apos;; import {ResultsContainer} from &apos;./components/Results&apos;; const createStoreWithMiddleware = applyMiddleware( remoteActionMiddleware )(createStore); const store = createStoreWithMiddleware(reducer); const socket = io(`${location.protocol}//${location.hostname}:8090`); socket.on(&apos;state&apos;, state =&gt; store.dispatch(setState(state)) ); const routes = &lt;Route handler={App}&gt; &lt;Route path=&quot;/results&quot; handler={ResultsContainer} /&gt; &lt;DefaultRoute handler={VotingContainer} /&gt; &lt;/Route&gt;; Router.run(routes, (Root) =&gt; { React.render( &lt;Provider store={store}&gt; {() =&gt; &lt;Root /&gt;} &lt;/Provider&gt;, document.getElementById(&apos;app&apos;) ); }); 如果你重启应用，你将会看到我们设置的中间件会抓到应用触发的action日志。 那我们应该怎么利用中间件机制来完成从客户端通过socket.io连接发送action给服务端呢？在此之前我们肯定需要先有一个连接供中间件使用，不幸的是我们已经有了，就在index.jsx中，我们只需要中间件可以拿到它即可。使用currying风格来实现这个中间件很简单： //src/remote_action_middleware.js export default socket =&gt; store =&gt; next =&gt; action =&gt; { console.log(&apos;in middleware&apos;, action); return next(action); } 这样我们就可以在index.jsx中传入需要的连接了： //src/index.jsx const socket = io(`${location.protocol}//${location.hostname}:8090`); socket.on(&apos;state&apos;, state =&gt; store.dispatch(setState(state)) ); const createStoreWithMiddleware = applyMiddleware( remoteActionMiddleware(socket) )(createStore); const store = createStoreWithMiddleware(reducer); 注意跟之前的代码比，我们需要调整一下顺序，让socket连接先于store被创建。 一切就绪了，现在就可以使用我们的中间件发送action了： //src/remote_action_middleware.js export default socket =&gt; store =&gt; next =&gt; action =&gt; { socket.emit(&apos;action&apos;, action); return next(action); } 打完收工。现在如果你再点击投票按钮，你就会看到所有连接到服务端的客户端的票数都会被更新！ 还有个很严重的问题我们要处理：现在每当我们收到服务端发来的SET_STATEaction后，这个action都将会直接回传给服务端，这样我们就造成了一个死循环，这是非常反人类的。 我们的中间件不应该不加处理的转发所有的action给服务端。个别action，例如SET_STATE，应该只在客户端做处理。我们在action中添加一个标识位用于识别哪些应该转发给服务端： //src/remote_action_middleware.js export default socket =&gt; store =&gt; next =&gt; action =&gt; { if (action.meta &amp;&amp; action.meta.remote) { socket.emit(&apos;action&apos;, action); } return next(action); } 我们同样应该修改相关的action creators： //src/action_creators.js export function setState(state) { return { type: &apos;SET_STATE&apos;, state }; } export function vote(entry) { return { meta: {remote: true}, type: &apos;VOTE&apos;, entry }; } 让我们重新审视一下我们都干了什么： 用户点击投票按钮，VOTEaction被分派； 远程action中间件通过socket.io连接转发该action给服务端； 客户端Redux Store处理这个action，记录本地hasVoted属性； 当action到达服务端，服务端的Redux Store将处理该action，更新所有投票及其票数； 设置在服务端Redux Store上的监听器将改变后的状态数据发送给所有在线的客户端； 每个客户端将触发SET_STATEaction的分派； 每个客户端将根据这个action更新自己的状态，这样就保持了与服务端的同步。 为了完成我们的应用，我们需要实现下一步按钮的逻辑。和投票类似，我们需要将数据发送到服务端： //src/action_creator.js export function setState(state) { return { type: &apos;SET_STATE&apos;, state }; } export function vote(entry) { return { meta: {remote: true}, type: &apos;VOTE&apos;, entry }; } export function next() { return { meta: {remote: true}, type: &apos;NEXT&apos; }; } ResultsContainer组件将会自动关联action creators中的next作为props： //src/components/Results.jsx import React from &apos;react/addons&apos;; import {connect} from &apos;react-redux&apos;; import Winner from &apos;./Winner&apos;; import * as actionCreators from &apos;../action_creators&apos;; export const Results = React.createClass({ mixins: [React.addons.PureRenderMixin], getPair: function() { return this.props.pair || []; }, getVotes: function(entry) { if (this.props.tally &amp;&amp; this.props.tally.has(entry)) { return this.props.tally.get(entry); } return 0; }, render: function() { return this.props.winner ? &lt;Winner ref=&quot;winner&quot; winner={this.props.winner} /&gt; : &lt;div className=&quot;results&quot;&gt; &lt;div className=&quot;tally&quot;&gt; {this.getPair().map(entry =&gt; &lt;div key={entry} className=&quot;entry&quot;&gt; &lt;h1&gt;{entry}&lt;/h1&gt; &lt;div className=&quot;voteCount&quot;&gt; {this.getVotes(entry)} &lt;/div&gt; &lt;/div&gt; )} &lt;/div&gt; &lt;div className=&quot;management&quot;&gt; &lt;button ref=&quot;next&quot; className=&quot;next&quot; onClick={this.props.next()}&gt; Next &lt;/button&gt; &lt;/div&gt; &lt;/div&gt;; } }); function mapStateToProps(state) { return { pair: state.getIn([&apos;vote&apos;, &apos;pair&apos;]), tally: state.getIn([&apos;vote&apos;, &apos;tally&apos;]), winner: state.get(&apos;winner&apos;) } } export const ResultsContainer = connect( mapStateToProps, actionCreators )(Results); 彻底完工了！我们实现了一个功能完备的应用。 ###课后练习（不翻译）","tags":[{"name":"react","slug":"react","permalink":"https://blog.kazaff.me/tags/react/"},{"name":"单元测试","slug":"单元测试","permalink":"https://blog.kazaff.me/tags/单元测试/"},{"name":"Immutable-js","slug":"Immutable-js","permalink":"https://blog.kazaff.me/tags/Immutable-js/"},{"name":"Redux","slug":"Redux","permalink":"https://blog.kazaff.me/tags/Redux/"}]},{"title":"［译］Reactjs性能篇","date":"2015-09-10T09:37:12.000Z","path":"2015/09/10/译-Reactjs性能篇/","text":"英文有限，技术一般，海涵海涵，由于不是翻译出身，所以存在大量的瞎胡乱翻译的情况，信不过我的，请看原文～～ 原文地址：https://facebook.github.io/react/docs/advanced-performance.html ###性能优化 每当开发者选择将react用在真实项目中时都会先问一个问题：使用react是否会让项目速度更快，更灵活，更容易维护。此外每次状态数据发生改变时都会进行重新渲染界面的处理做法会不会造成性能瓶颈？而在react内部则是通过使用一些精妙的技巧来最小化每次造成ui更新的昂贵的dom操作从而保证性能的。 ####避免直接作用于DOMreact实现了一层虚拟dom，它用来映射浏览器的原生dom树。通过这一层虚拟的dom，可以让react避免直接操作dom，因为直接操作浏览器dom的速度要远低于操作javascript对象。每当组件的属性或者状态发生改变时，react会在内存中构造一个新的虚拟dom与原先老的进行对比，用来判断是否需要更新浏览器的dom树，这样就尽可能的优化了渲染dom的性能损耗。 在此之上，react提供了组件生命周期函数，shouldComponentUpdate，组件在决定重新渲染（虚拟dom比对完毕生成最终的dom后）之前会调用该函数，该函数将是否重新渲染的权限交给了开发者，该函数默认直接返回true，表示默认直接出发dom更新： shouldComponentUpdate: function(nextProps, nextState) { return true; } 值得注意的是，react会非常频繁的调用该函数，所以如果你打算自己实现该函数的逻辑，请尽可能保证性能。 比方说，你有一个拥有多个帖子的聊天应用，如果此时只有一个发生了变化，如果你如下实现了shouldComponentUpdate，react会根据情况避免重新渲染那些没有发生变化的帖子： shouldComponentUpdate: function(nextProps, nextState) { // TODO: return whether or not current chat thread is different to former one. // 根据实际情况判断当前帖子的状态是否和之前不同 } 总之，react尽可能的避免了昂贵的dom操作，并且允许开发者干涉该行为。 ####shouldComponentUpdate实战这里举个包含子元素的组件例子，如下图： 图中每个圆点表示一个dom节点，当某个dom节点的shouldComponentUpdate返回false时（例如c2），react就无需为其更新dom，注意，react甚至根本不会去调用c4和c5节点的shouldComponentUpdate函数哦～ 图中c1和c3的shouldComponentUpdate返回了true，因此react会检查检查其它们包含的直接子节点。最有趣的是c8节点，虽然调用它的shouldComponentUpdate方法返回的是true，但react检查后发现其dom结构并未发生改变，所以react最终是不会重新渲染其浏览器dom的。 上图的情况下，react最终只会重新渲染c6，原因你应该懂的。 那么我们应该如何实现shouldComponentUpdate函数呢？假设你有一个只包含字符串的组件，如下： React.createClass({ propTypes: { value: React.PropTypes.string.isRequired }, render: function() { return &lt;div&gt;{this.props.value}&lt;/div&gt;; } }); 我们可以简单的直接实现shouldComponentUpdate如下： shouldComponentUpdate: function(nextProps, nextState) { return this.props.value !== nextProps.value; } 目前为止一切都很顺利，处理基础类型的属性和状态是很简单的，我们可以直接使用js语言提供的===比对来实现一个mix并注入到所有组件中，事实上，react自身已经提供了一个类似的：PureRenderMixin。 但是如果你的组件所拥有的属性或状态不是基础类型呢，而是复合类型呢？比方说是一个js对象，{foo: &#39;bar&#39;}： React.createClass({ propTypes: { value: React.PropTypes.object.isRequired }, render: function() { return &lt;div&gt;{this.props.value.foo}&lt;/div&gt;; } }); 这种情况下我们刚才实现的那种shouldComponentUpdate就歇菜了： // 假设 this.props.value 是 { foo: &apos;bar&apos; } // 假设 nextProps.value 是 { foo: &apos;bar&apos; }, // 但是nextProps和this.props对应的引用不相同 this.props.value !== nextProps.value; // true 要想修复这个问题，简单粗暴的方法是我们直接比对foo的值，如下： shouldComponentUpdate: function(nextProps, nextState) { return this.props.value.foo !== nextProps.value.foo; } 我们当然可以通过深比对来确定属性或状态是否确实发生了改变，但是这种深比对是非常昂贵的，还记得我们刚出说过shouldComponentUpdate函数的调用非常频繁么？更何况我们为每个model去单独实现一个匹配的深比对逻辑，对于开发人员来说也是非常痛苦的。最重要的是，如果我们不是很小心的处理对象引用关系的话，还会带来灾难。例如下面这个组件： React.createClass({ getInitialState: function() { return { value: { foo: &apos;bar&apos; } }; }, onClick: function() { var value = this.state.value; value.foo += &apos;bar&apos;; // ANTI-PATTERN! this.setState({ value: value }); }, render: function() { return ( &lt;div&gt; &lt;InnerComponent value={this.state.value} /&gt; &lt;a onClick={this.onClick}&gt;Click me&lt;/a&gt; &lt;/div&gt; ); } }); 起初，InnerComponent组件进行渲染，它得到的value属性为{foo: &#39;bar&#39;}。当用户点击链接后，父组件的状态将会更新为{ value: { foo: &#39;barbar&#39; } }，触发了InnerComponent组件的重新渲染，因为它得到了一个新的属性：{ foo: &#39;barbar&#39; }。 看上去一切都挺好的，其实问题在于，父组件和子组件供用了同一个对象的引用，当用户触发click事件时，InnerComponent的prop将会发生改变，因此它的shouldComponentUpdate函数将会被调用，而此时如果按照我们目前的shouldComponentUpdate比对逻辑的话，this.props.value.foo和nextProps.value.foo是相等的，因为事实上，它们同时引用同一个对象哦～所以，我们将会看到，InnerComponent的ui并没有更新。哎～，不信的话，我贴出完整代码： &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;demo&lt;/title&gt; &lt;!--引入React库--&gt; &lt;script src=&quot;lib/react.min.js&quot;&gt;&lt;/script&gt; &lt;!--引入JSX转换库--&gt; &lt;script src=&quot;lib/JSXTransformer.js&quot;&gt;&lt;/script&gt; &lt;!--组件样式--&gt; &lt;/head&gt; &lt;body&gt; &lt;!--定义容器--&gt; &lt;div id=&quot;content&quot;&gt;&lt;/div&gt; &lt;!--声明脚本类型为JSX--&gt; &lt;script type=&quot;text/jsx&quot;&gt; var InnerComponent = React.createClass({ shouldComponentUpdate: function(nextProps, nextState) { return this.props.value.foo !== nextProps.value.foo; }, render: function() { return ( &lt;div&gt; {this.props.value.foo} &lt;/div&gt; ); } }); var OutComponent = React.createClass({ getInitialState: function() { return { value: { foo: &apos;bar&apos; } }; }, onClick: function() { var value = this.state.value; value.foo += &apos;bar&apos;; // ANTI-PATTERN! this.setState({ value: value }); }, render: function() { return ( &lt;div&gt; &lt;InnerComponent value={this.state.value} /&gt; &lt;a onClick={this.onClick}&gt;Click me&lt;/a&gt; &lt;/div&gt; ); } }); React.render(&lt;OutComponent /&gt;, document.querySelector(&quot;#content&quot;)); &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; ####Immutable-js的救赎Immutable-js是一个javascript集合库，作者是Lee Byron，该项目最近从fb开源。它提供了不可变集合类型： 不可变性：一旦创建，这个集合不允许再更改。 延续性：新集合可以衍生自一个已经创建过的集合，并作一些改动，此时源集合不会受到任何影响。 结构共享：如果新集合衍生自一个老集合，那么新集合中与老集合相同的部份将会共享同一块内存，这样做的好处是节省内存开销，并能在创建新集合对象时减少内存拷贝的性能损耗。 不可变性使得监控状态变化变得可行，每次状态发生变化，将总是返回一个新的对象，我们只需要检查改变前后的对象引用是否相同即可。举个例子： var x = { foo: &quot;bar&quot; }; var y = x; y.foo = &quot;baz&quot;; x === y; // true 尽管变量y被修改，但由于y和x的引用相同，最后的比对仍然返回true。如果上面的代码用immutable-js来实现： var SomeRecord = Immutable.Record({ foo: null }); var x = new SomeRecord({ foo: &apos;bar&apos; }); var y = x.set(&apos;foo&apos;, &apos;baz&apos;); x === y; // false 看到了么，是不是很爽？ 另外一种监控变量的方法是设置一个标识位，但这需要开发者编写额外的代码，哎，反正活着就是麻烦。 总之，不可变集合结构提供了你一个廉价且简单的监控对象改变的方法，你可以放在shouldComponentUpdate中。因此，如果你的model属性和状态是基于immutable-js来实现的，那么你就可以直接使用官方提供的PureRenderMixin哟～ ####Immutable-js和Flux如果你恰巧使用Flux，并且你又基于immutable-js来实现你的store，那你先看一下相关的api吧。 让我们来用个模拟应用演示一下使用一种可行的方案。首先，我们需要定义一下用到的model： var User = Immutable.Record({ id: undefined, name: undefined, email: undefined }); var Message = Immutable.Record({ timestamp: new Date(), sender: undefined, text: &apos;&apos; }); 每个Record接受一个对象，分别定义了字段和默认值。我们的messages store需要使用的是list结构： this.users = Immutable.List(); this.messages = Immutable.List(); 很简单吧，接着，每当store接受到一个新的message时，我们只需要创建一个新的record并把它加入到列表中即可： this.messages = this.messages.push(new Message({ timestamp: payload.timestamp, sender: payload.sender, text: payload.text }); 对比我们刚刚讲过的immutable-js特性，在react的组件中我们只需要直接注入PureRenderMixin即可高枕无忧。","tags":[{"name":"react","slug":"react","permalink":"https://blog.kazaff.me/tags/react/"},{"name":"dom","slug":"dom","permalink":"https://blog.kazaff.me/tags/dom/"},{"name":"shouldComponentUpdate","slug":"shouldComponentUpdate","permalink":"https://blog.kazaff.me/tags/shouldComponentUpdate/"},{"name":"Immutable-js","slug":"Immutable-js","permalink":"https://blog.kazaff.me/tags/Immutable-js/"}]},{"title":"React开发web系统初体验","date":"2015-08-25T09:37:12.000Z","path":"2015/08/25/React开发web系统初体验/","text":"大热React的这段时间里，刚好碰到我找工作，所以画了一些时间学了一下react，用其开发了一个小web系统，下面主要是介绍一下其中的一些收获。 React ＋ ？ 这里我想说的是，只用react你是不足以搭建一个web系统的前端的，保守的说，你还需要： 路由：react-router ajax：我使用的是jQuery 项目文档结构： 你会发现上图中存在两种带描述后缀的文件：XXXAction.js 和 XXXStore.js，之所以这么分，是因为项目里还是用了reflux，这个理念我们之前已经说过了。 除此之外，所有页面文件的后缀名我使用.jsx，其他都是.js，我认为这样有助于快速的了解对应文件的作用。 上述文件结构其实不算很灵活，实际开发中碰到有一些通用需求的页面，又不是太适合放在components里，就显得无法适配了，除此之外，components中的可复用web控件，理应不包含数据获取相关业务逻辑，但由于时间关系，我打破了这个规则，这样这些控件就无法直接用于其他项目了。。。以后开发需要注意～ 组件化思维之前在使用angular的时候，指令其实是推荐使用的，但是你也完全可以不考虑组件模式，而react就不行了，你无时无刻不被强迫思考组件这个概念，因为react提供的语法本身就无时无刻体现了该模式，这一点我认为是极好的。 可以说虽然react不像angular那样提供大量的内容需要开发者掌握，但想要用好react，必须建立起组件思维，官方提供的demo中就强调了如何把原先思考页面的布局改为思考页面的组件，听起来简单但实际做的时候还是会有些不适应。 语法小贴士 由于有babel这样的好东西，所以推荐从今天起就开始使用es6 好好学一下webpack的用法 那些不用来影响视图的变量就不要放在state中 组件嵌套时，props的值不要在组件内部做二次赋值，这样会导致react数据绑定“失效”，下面我给个例子： let Inside = React.createClass({ componentWillMount(){ this.setState({ a: this.props.params.a, }); }, render(){ return ( &lt;div&gt;{this.state.a}&lt;/div&gt; ); }, }); let Outside = React.createClass({ getInitialState(){ return { data: { a: 1, }, }; }, render(){ return ( &lt;Inside params={this.state.data} /&gt; ); }, }); 看上去按说应该在外层每次修改this.state.data.a的时候内层都应该立刻显示最新的a值，但其实不然，原因其实很简单，内层componentWillMount方法只会在组件第一次加载时调用一次，然后就没有然后了。 聪明的童鞋应该知道怎么改了，我就不多说了。 最后我给出我项目的依赖库： &quot;devDependencies&quot;: { &quot;babel-core&quot;: &quot;^5.8.20&quot;, &quot;babel-loader&quot;: &quot;^5.3.2&quot;, &quot;css-loader&quot;: &quot;^0.15.6&quot;, &quot;file-loader&quot;: &quot;^0.8.4&quot;, &quot;html-webpack-plugin&quot;: &quot;^1.6.0&quot;, &quot;react-hot-loader&quot;: &quot;^1.2.8&quot;, &quot;style-loader&quot;: &quot;^0.12.3&quot;, &quot;url-loader&quot;: &quot;^0.5.6&quot;, &quot;webpack&quot;: &quot;^1.10.5&quot;, &quot;webpack-dev-server&quot;: &quot;^1.10.1&quot;, &quot;amazeui&quot;: &quot;^2.4.0&quot;, &quot;amazeui-react&quot;: &quot;latest&quot; }, &quot;dependencies&quot;: { &quot;react&quot;: &quot;^0.13.3&quot;, &quot;react-notification-system&quot;: &quot;^0.1.14&quot;, &quot;react-router&quot;: &quot;^0.13.3&quot;, &quot;reflux&quot;: &quot;^0.2.11&quot; } 下面是项目的webpack配置文件： var path = require(&apos;path&apos;); var webpack = require(&apos;webpack&apos;); var HtmlWebpackPlugin = require(&apos;html-webpack-plugin&apos;); var node_modules = path.resolve(__dirname, &apos;node_modules&apos;); var deps = [ //&apos;react/dist/react.min.js&apos;, &apos;react-router/umd/ReactRouter.min.js&apos;, //&apos;amazeui-react/dist/amazeui.react.min.js&apos;, ]; var config = { entry: [ path.resolve(__dirname, &quot;app/app.jsx&quot;), &quot;webpack/hot/dev-server&quot;, ], resolve: { alias: {} }, output: { path: path.resolve(__dirname, &quot;build&quot;), filename: &quot;bundle.js&quot;, }, plugins: [ /*new webpack.optimize.UglifyJsPlugin({ compress:{ warnings: false, }, }), new webpack.optimize.CommonsChunkPlugin(&apos;common&apos;, &apos;common.js&apos;),*/ new HtmlWebpackPlugin({ inject: true, template: &apos;app/index.html&apos;, }), new webpack.NoErrorsPlugin(), ], module: { loaders: [ { test: /\\.jsx?$/, //loaders: [&apos;react-hot&apos;,&apos;babel&apos;] loader: &apos;babel&apos; }, { test: /\\.css$/, loader: &apos;style!css&apos;, }, { test: /\\.(png|jpg|eot|ttf|woff|woff2)$/, loader: &apos;url&apos;, } ], noParse: [], }, debug: true, devtool: &apos;eval-cheap-module-source-map&apos;, devServer: { contentBase: path.resolve(__dirname, &quot;build&quot;), historyApiFallback: true } }; deps.forEach(function (dep) { var depPath = path.resolve(node_modules, dep); config.resolve.alias[dep.split(path.sep)[0]] = depPath; config.module.noParse.push(depPath); }); module.exports = config; 实际使用下来，感觉react要比ng强很多，同时也非常期待ng2的问世。","tags":[{"name":"react","slug":"react","permalink":"https://blog.kazaff.me/tags/react/"},{"name":"react-router","slug":"react-router","permalink":"https://blog.kazaff.me/tags/react-router/"},{"name":"reflux","slug":"reflux","permalink":"https://blog.kazaff.me/tags/reflux/"}]},{"title":"论Uikit的重要性","date":"2015-08-08T09:37:12.000Z","path":"2015/08/08/论UIkit的重要性/","text":"这年头，最怕的就是上贼船，真的！我才从待了四年的公司离职，谁知刚出粪坑又入“火坑”，真是运气太差啊～我所说的这个火坑，其实是一套无意间发现的ui，好吧，其实是我特意搜的，只恨当时鬼迷心窍被忽悠了啊（当然，也可能是我等级不够，下不了这个副本啊～）。 我所指的ui就是：materialUI。搞开发的，谁不佩服google的技术，之前玩的angular就是出自google，而我说的这套ui代码就是基于Material design设计理念打造而成，确实很容易看对眼啊～ 只可惜小哥我前端道行太浅驾驭不了啊，花了几天的时间硬是给我死扛着做了个demo，进度着实缓慢，哎～ 为什么说它不爽呢？在我看来，它封装的太厚实了，除了暴露出来的一些操作属性外，你很难去控制这些控件的位置，更别提样式了，当然可能作者就是为了避免整体的设计理念被打乱吧。 除此之外，该uikit不支持多国语言，而且datepicker也不是自适应的，导致无法在手机上很好的操作，而且动效还有些许卡顿（可能是我设备档次太低～）。 不过，它的leftNav控件着实不赖，不管是性能还是美观度都非常讨好。 当然，它还缺少了很多我个人认为非常必备的控件，比方说页码，面包屑等，期待它的完善吧～反正小哥我是耗不起了。 我在中文论坛上搜了搜，果不其然找到了一个国人发布的ui库，之前也听说过，不过这次发现竟然还有react版本的，真是大爱啊：amazeui。哥可不是做广告啊，其实原先我是打算使用：react-bootstrap，可不知道是不是审美疲劳了，总之bs2.0已经无法满足我的欲望了。 这几天就容我换个行头重新来过吧～","tags":[{"name":"react","slug":"react","permalink":"https://blog.kazaff.me/tags/react/"},{"name":"ui","slug":"ui","permalink":"https://blog.kazaff.me/tags/ui/"}]},{"title":"这几天的感想","date":"2015-07-19T09:37:12.000Z","path":"2015/07/19/日志/","text":"###总结 今天已是离职的第7天，时间过的真尼玛快，不知不觉就给家待了一周了。从一开始因为会议上沟通吃力的失望，到为了捍卫原则的愤怒，再到离职后经济带来的恐慌，到各种职位把手机打爆的惊叹，再到模拟笔试题带来的精神打击，到朋友喝同事给予我的肯定和安慰，再到家人给予的支持，我的情绪就像在坐过山车一样时好时坏。 不过时间确实是机械的，不管你是快乐还是悲伤，是愤怒还是失望，它都不会为此多停哪怕一亚秒。它以简单粗暴的方式逼着你向前看，这种方式或许也是最行之有效的啦～ 在这家公司工作了少说四年吧，说没有感情那绝壁是气话，即便是对公司早已失望，对产品从没看好，但相处了那么久的同事，打磨了那么久的团队，捍卫了多次的团队文化，总让我感到依依不舍。但事实证明，在企业文化面前，谈团队文化并没有什么卵用，几年前我对这句话还抱着鄙视的态度，但这几天发生的事儿让我无奈，也让我成长。 和一个朋友聊天，本来还带着些许成就感的我，被朋友一个问题问的我竟无言以对。大概的对话如下： 我：这些年至少学了不少技术，做了几个产品。 TA：你们的产品有用户么？你们的产品改变了某些人的生活么？提高了哪怕些许人的生活质量了吗？ 我tm竟然无言以对，静下来思考一下，才恍然大悟，以前口口声声说要改变世界，却花了四年的时间为的只是老板在饭桌上吹牛逼，如果非要说改善了谁的生活质量，那估计也就公司股东吧～ 到底是公司的产品是不是在圈钱，我说不太好，那是市场和营销范畴。我能感觉到的，仅仅是以现在公司的内部状况来说，不太像是做实事儿的，老板的观点是：在中国，想成任何事，都要拉好关系，抱对大腿，领导们的观点是：大老板说的对！，投资人的观点是：技术人员不重要，随时都能招的到。这就是一家创业公司的企业文化，这就是那个我工作了4年之久的公司，这就是一个开发人员的悲哀。 可能我确实太平庸，没有能力和资格去要求更多，能在家门口拿一份体面的工资就应该知足的。但我却忘记了，对每个人来说，时间才是最珍贵的资源。同样的时间，为何要挥霍在那些嗤之以鼻的事情上？同样的时间，为何要浪费在那些不值得的人上？更可悲的是，让我顿悟这一点的，是那些年纪比我还小的朋友。 ###规划 最近一段时间依赖，一直在穷尽最大的努力再拓展自己的技术视野，毕竟我的理想是当一名架构师。但有得有失，长时间的拉宽知识面，却让我对一些基础知识变的生疏，导致的直接结果就是，面试没啥问题（这几天的电话面试基本上都很容易的过了:)），笔试成了问题（我可不想交白卷啊T_T）。 所以接下来的一个月时间，我的计划是恢复一下自己的笔试能力，找回那些原本亲切的算法和教科书式填空题的记忆（ps：智力题真的太伤自尊了啊Q_Q）。 ###下一个五年 人生能有多少个十年，多少个五年，多少个三年，我已经三十了，身边朋友们的下一代都会喊爸爸了，我却还一身负债，这就是对自己不够狠的报应吧～ 下一个五年计划，就是要朝着我的理想加速前行，我猜35岁的我，应该是个合格的架构师，应该是个好父亲，应该是个更懂得生活的好老公，应该是个有担当的好儿子，应该依然是个有棱有角的人，一个坚持原则的人，一个就事论事实事求是的人，一个依然有着正义感的中年人。 我的朋友们，请你们见证！","tags":[{"name":"离职","slug":"离职","permalink":"https://blog.kazaff.me/tags/离职/"}]},{"title":"Logstash整合kafka","date":"2015-06-05T09:37:12.000Z","path":"2015/06/05/logstash整合kafka/","text":"今天要搞一搞的，是把logstash和kafka整合起来，由于我们使用的是logstash1.5.0+版本，此版本下官方已经提拱了plugin用来整合kafka，这篇文章的目的就是简单的搭建这么一个环境。 kafka的安装kafka是基于scala实现的，scala是一种jvm语言，也就是说你得先装jdk，我就不从jdk开始介绍如何安装了，我们直接开始安装kafka，其实这玩意儿官方提供了编译好的版本，你只需要下载，解压，运行即可~当然，如果想搭建集群，还是需要了解一下kafka的配置的，这不是我们关注的重点，so，我们就简单地先跑起来它吧： 下载 解压，我们解压在/usr/local下 运行，为了测试方便，我们需要一共开启四个终端窗口： 首先，我们要运行zookeeper，kafka自带了zookeeper，所以我们无需下载，只需要在/usr/local/kafka目录下执行： bin/zookeeper-server-start.sh config/zookeeper.properties 然后，再开启一个终端，执行： bin/kafka-server-start.sh config/server.properties 这样，我们的kafka就已经运行起来了，不过还不是集群环境，只有一个borker哟~但是，我们测试足够了。 再然后，开启一个新的终端，执行： bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 这样我们就创建了一个用于测试的topic，接下来继续执行： bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 该命令执行完毕后会阻塞终端，你可以随便输入一些数据，每一行都相当于一个消息，会发送给kafka。 最后，再再开启一个新终端，执行： bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning 你会看到你之前输入的那些消息都会显示在终端中，这就完成了kafka的测试环境搭建。 值得注意的是，上面消息生产者的命令，需要参数broker-list，也就是说我们的kafka生产者必须自己知道所有的kafka broker的位置，而其它命令则只需要填写zookeeper的位置即可，我不清楚这样做的用意是什么，我只是隐约感到有些问题（如果broker出现扩容，如何更新应用代码中的borker信息？？），但这不是我们本次的关注的重点。 logstash–&gt;kafka我们现在想要干的，是从logstash的Shipper中收集到的数据发送给kafka，所以我们需要安装logstash-output-kafka插件。 但是由于未知原因，我试图安装插件时却碰到了报错： [root@kazaff logstash-1.5.0]# bin/plugin install logstash-output-kafka Validating logstash-output-kafka Plugin logstash-output-kafka does not exist ERROR: Installation aborted, verification failed for logstash-output-kafka 不像是被墙了的味道，因为提醒的是不存在，而不是网络连接超时。本来还想搭建一个翻墙环境，后来执行了一下这个命令： bin/plugin list 竟然发现kafka插件已经预装好了，我也是醉了。OK，我们可以继续了，接下来就是配置一下logstash： input { stdin{} } output { kafka { broker_list =&gt; &quot;localhost:9092&quot; topic_id =&gt; &quot;test&quot; compression_codec =&gt; &quot;snappy&quot; } } 不多做解释了，在终端运行logstash后，就可以直接输入“helloworld”测试一下了，如果没有问题的话，你将会在之前的kafka消费者终端中看到输出： {&quot;message&quot;:&quot;hello world!&quot;,&quot;@version&quot;:&quot;1&quot;,&quot;@timestamp&quot;:&quot;2015-06-11T10:01:21.183Z&quot;,&quot;host&quot;:&quot;kazaff&quot;} 就是这么简单啦~ 参考Kafka快速入门 Logstash入门教程 - 启动命令行参数及插件安装 logstash-input-file以及logstash-output-kafka插件性能测试","tags":[{"name":"Logstash","slug":"Logstash","permalink":"https://blog.kazaff.me/tags/Logstash/"},{"name":"kafka","slug":"kafka","permalink":"https://blog.kazaff.me/tags/kafka/"},{"name":"logstash-output-kafka","slug":"logstash-output-kafka","permalink":"https://blog.kazaff.me/tags/logstash-output-kafka/"},{"name":"logstash插件","slug":"logstash插件","permalink":"https://blog.kazaff.me/tags/logstash插件/"}]},{"title":"日志收集架构－ELK","date":"2015-06-05T09:37:12.000Z","path":"2015/06/05/日志收集架构--ELK/","text":"本周的工作计划是调研并测试ELK，那什么是ELK呢？简而言之就是开源的主流的日志收集与分析系统。ELK是三个工具的总称： E: ElasticSearch L: Logstash K: Kibana 我这里主要想强调的，是它们三个组合起来以后，提供强大的开箱即用的日志收集和检索功能，这对于创业公司和小团队来说，简直就是完美~ 可能对我来讲，最不理想的就是它基于ruby语言，这么高逼格的语言我不会啊…… 但是也不要太乐观，社区和大牛们表示，想用好这三个工具也不是一件很简单的事儿，我们本次的目的，就是搞清楚这三者之间的关系和它们各自的作用，并最终搭建一个可测试环境，好吧，废话不多说，开始吧。 最为强迫症的我，一向是尽可能用最新版本，也就是说，我们本次尝试搭建的ELK环境，软件版本分别为： ElasticSearch-1.5.2 Logstash-1.5.0 Kibana-4.0.2 安装之前提过“开箱即用”，绝对不是吹牛逼的，安装流程就是： 下载 --&gt; 解压 --&gt; 简单配置 --&gt; 运行 --&gt; 看效果 完全不需要编译安装，真是醉了~我们从logstash开始，下载上面给的版本后，解压并拷贝到/usr/local下： $ cd /usr/local/logstash $ mkdir conf logs 我们创建了两个文件夹，分别用来放配置文件和日志。 接下来我们先简单的配置一下logstash，在conf下创建一个central.conf，内容如下： input { stdin {} } output { stdout {} elasticsearch { cluster =&gt; &quot;elasticsearch&quot; codec =&gt; &quot;json&quot; protocol =&gt; &quot;http&quot; } } 保存，运行下面这个命令启动： $ ./bin/logstash agent --verbose --config ./conf/central.conf --log ./logs/stdout.log 接下来安装elasticsearch，一样简单，解压到/usr/local/下，然后直接启动： $ ./bin/elasticsearch -d 最后来配置kibana，解压到/usr/local/下，然后直接启动： $ ./bin/kibana 我们使用的是kibana默认的配置，现在可以直接在浏览器中访问： http://127.0.0.1:5601。 怎么测试我们安装的是否成功呢？很简单，在刚才我们执行logstash的终端中，直接输入字符串：hello world，然后刷新kibana页面，你将会看到对应的日志信息~~ 知道什么叫开箱即用了么？ 搭建好了，不等于我们就可以直接使用在产品环境下了，毕竟这个例子貌似没有卵用…下面我们来聊一下理论知识。 架构 说明： 多个独立的agent(Shipper)负责收集不同来源的数据，一个中心agent(Indexer)负责汇总和分析数据，在中心agent前的Broker(使用redis实现)作为缓冲区，中心agent后的ElasticSearch用于存储和搜索数据，前端的Kibana提供丰富的图表展示。 Shipper表示日志收集，使用LogStash收集各种来源的日志数据，可以是系统日志、文件、redis、mq等等； Broker作为远程agent与中心agent之间的缓冲区，使用redis实现，一是可以提高系统的性能，二是可以提高系统的可靠性，当中心agent提取数据失败时，数据保存在redis中，而不至于丢失； 中心agent也是LogStash，从Broker中提取数据，可以执行相关的分析和处理(Filter)； ElasticSearch用于存储最终的数据，并提供搜索功能； Kibana提供一个简单、丰富的web界面，数据来自于ElasticSearch，支持各种查询、统计和展示； 上面这个图是参考前辈的（下面的参考列表中有具体链接），需要知道的是上面的这种搭配并非是唯一的，例如你可以不使用redis而是用kafka来做消息队列，你也可以让Shipper直接从你希望的日志源中拉取日志数据，如你喜欢你也可以让数据存到Elasticsearch后再存一份到hdfs中，反正大牛们说logstash的插件非常的多，我是信了~~ 接下来，我们就试试用logstash来获取一些常用的web server的access log，例如：nginx，tomcat等。由于测试环境都是在同一台机器上本地完成，所有就不需要劳烦消息队列中间件了。 Nginx–&gt;Logstash根据上图所示，我们其实就是让logstash去监听nginx的日志文件，首先我们得先修改一下上面的那个logstash的配置文件： input { file { type =&gt; &quot;nginx_access&quot; path =&gt; [&quot;/usr/local/nginx/logs/*.log&quot;] exclude =&gt; [&quot;*.gz&quot;,&quot;error.log&quot;,&quot;nginx.pid&quot;] sincedb_path =&gt; &quot;/dev/null&quot; codec =&gt; &quot;json&quot; } } output { stdout {} elasticsearch { cluster =&gt; &quot;elasticsearch&quot; codec =&gt; &quot;json&quot; protocol =&gt; &quot;http&quot; } } ok，接下来我们就需要安装nginx了，这部分我就不多讲了，安装好nginx后在启动之前，我们需要先修改一下nginx的配置文件（nginx.conf）: ...... log_format json &apos;{&quot;@timestamp&quot;:&quot;$time_iso8601&quot;,&apos; &apos;&quot;@source&quot;:&quot;$server_addr&quot;,&apos; &apos;&quot;@nginx_fields&quot;:{&apos; &apos;&quot;client&quot;:&quot;$remote_addr&quot;,&apos; &apos;&quot;size&quot;:$body_bytes_sent,&apos; &apos;&quot;responsetime&quot;:&quot;$request_time&quot;,&apos; &apos;&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,&apos; &apos;&quot;upstreamaddr&quot;:&quot;$upstream_addr&quot;,&apos; &apos;&quot;request_method&quot;:&quot;$request_method&quot;,&apos; &apos;&quot;domain&quot;:&quot;$host&quot;,&apos; &apos;&quot;url&quot;:&quot;$uri&quot;,&apos; &apos;&quot;http_user_agent&quot;:&quot;$http_user_agent&quot;,&apos; &apos;&quot;status&quot;:$status,&apos; &apos;&quot;x_forwarded_for&quot;:&quot;$http_x_forwarded_for&quot;&apos; &apos;}&apos; &apos;}&apos;; access_log logs/access.log json; ...... 大功告成了，启动nginx后，浏览器访问nginx下的页面，你会看到kibana中的对应更新。 Tomcat–&gt;Logstashtomcat的日志比nginx要复杂一些，打开对应的日志文件（catalina.out）,你会发现类似下面这样的日志结构： Jun 12, 2014 11:17:34 AM org.apache.catalina.core.AprLifecycleListener init INFO: The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: /usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib Jun 12, 2014 11:17:34 AM org.apache.coyote.AbstractProtocol init INFO: Initializing ProtocolHandler [&quot;http-bio-8080&quot;] Jun 12, 2014 11:17:34 AM org.apache.coyote.AbstractProtocol init SEVERE: Failed to initialize end point associated with ProtocolHandler [&quot;http-bio-8080&quot;] java.net.BindException: Address already in use &lt;null&gt;:8080 at org.apache.tomcat.util.net.JIoEndpoint.bind(JIoEndpoint.java:411) at org.apache.tomcat.util.net.AbstractEndpoint.init(AbstractEndpoint.java:640) at org.apache.coyote.AbstractProtocol.init(AbstractProtocol.java:434) at org.apache.coyote.http11.AbstractHttp11JsseProtocol.init(AbstractHttp11JsseProtocol.java:119) at org.apache.catalina.connector.Connector.initInternal(Connector.java:978) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:102) at org.apache.catalina.core.StandardService.initInternal(StandardService.java:559) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:102) at org.apache.catalina.core.StandardServer.initInternal(StandardServer.java:813) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:102) at org.apache.catalina.startup.Catalina.load(Catalina.java:638) at org.apache.catalina.startup.Catalina.load(Catalina.java:663) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.catalina.startup.Bootstrap.load(Bootstrap.java:280) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:454) Caused by: java.net.BindException: Address already in use at java.net.PlainSocketImpl.socketBind(Native Method) at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376) at java.net.ServerSocket.bind(ServerSocket.java:376) at java.net.ServerSocket.&lt;init&gt;(ServerSocket.java:237) at java.net.ServerSocket.&lt;init&gt;(ServerSocket.java:181) at org.apache.tomcat.util.net.DefaultServerSocketFactory.createSocket(DefaultServerSocketFactory.java:49) at org.apache.tomcat.util.net.JIoEndpoint.bind(JIoEndpoint.java:398) ... 17 more 无法直视啊简直，不过还好，logstash提供了强大的插件来帮助我们解析各种各样的日志输出结构，分析一下可以得出，日志结构是：时间戳，后面跟着类名，再后面跟着日志信息，这样，我们就可以根据这种结构来写过滤规则： COMMONAPACHELOG %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] &quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})&quot; %{NUMBER:response} (?:%{NUMBER:bytes}|-) COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent} CATALINA_DATESTAMP %{MONTH} %{MONTHDAY}, 20%{YEAR} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) (?:AM|PM) CATALINALOG %{CATALINA_DATESTAMP:timestamp} %{JAVACLASS:class} %{JAVALOGMESSAGE:logmessage} 我们把这些规则存储在一个文件（/logstash安装路径/patterns/grok-patterns）中，接下来我们要改写logstash的配置文件了： input { file { type =&gt; &quot;tomcat_access&quot; path =&gt; [&quot;/usr/local/tomcat/logs/catalina.out&quot;] exclude =&gt; [&quot;*.log&quot;,&quot;*.txt&quot;] sincedb_path =&gt; &quot;/dev/null&quot; start_position =&gt; &quot;beginning&quot; } file { type =&gt; &quot;apache_access&quot; path =&gt; [&quot;/usr/local/tomcat/logs/*.txt&quot;] exclude =&gt; [&quot;*.log&quot;] sincedb_path =&gt; &quot;/dev/null&quot; start_position =&gt; &quot;beginning&quot; } } filter { if [type] == &quot;tomcat_access&quot; { multiline { patterns_dir =&gt; &quot;/usr/local/logstash/patterns&quot; pattern =&gt; &quot;(^%{CATALINA_DATESTAMP})&quot; negate =&gt; true what =&gt; &quot;previous&quot; } if &quot;_grokparsefailure&quot; in [tags] { drop { } } grok { patterns_dir =&gt; &quot;/usr/local/logstash/patterns&quot; match =&gt; [ &quot;message&quot;, &quot;%{CATALINALOG}&quot; ] } date { match =&gt; [ &quot;timestamp&quot;, &quot;yyyy-MM-dd HH:mm:ss,SSS Z&quot;, &quot;MMM dd, yyyy HH:mm:ss a&quot; ] } } if [type] == &quot;apache&quot; { grok { patterns_dir =&gt; &quot;/usr/local/logstash/patterns&quot; match =&gt; { &quot;message&quot; =&gt; &quot;%{COMBINEDAPACHELOG}&quot; } } date { match =&gt; [ &quot;timestamp&quot;, &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ] } } } output { stdout {} elasticsearch { cluster =&gt; &quot;elasticsearch&quot; protocol =&gt; &quot;http&quot; embedded =&gt; true } } 然后你就可以重启一下logstash看结果啦。注意这里，我配置的是start_position =&gt; &quot;beginning&quot;，字面意思就是从日志文件头部开始解析，这样可以把一些原始日志给收集过来，但是可能会造成一些重复日志~~ Log4j–&gt;Logstash其实通过上面两个场景，你大概也知道是个什么尿性了，对，就是指定好要监控的日志文件（input），然后解析对应的格式（filter），然后导入到对应的存储中（output），而且整个过程是管道化的，前面提到了，由于咱们的测试环境是单机本地化，所以并没有使用消息队列，否则你会感受到更明显的管道化风格。 把log4j产生的日志导入到logstash要做的事儿依然如此，不过官方提供了更简单的方式：log4j-jsonevent-layout，这玩意儿的作用相当于我们在nginx中干的事儿，直接将log4j的日志格式定义成json的，有助于性能提升~ 剩下的事儿就是老样子了，只需要改一下我们的logstash配置文件即可： ...... file { type =&gt; &quot;log4j&quot; path =&gt; [&quot;/usr/local/tomcat/logs/logTest.log&quot;] sincedb_path =&gt; &quot;/dev/null&quot; start_position =&gt; &quot;beginning&quot; } ...... 高可用有经验的童鞋不难看出，上图中存在单点故障，但，其实是可以通过把对应单点集群化部署来增加这套架构的可用性和处理性能，具体可参考ElasticSearch+LogStash+Kibana+Redis日志服务的高可用方案。 总结上面的场景和简单介绍都是非常入门级的，放在线上场景中肯定太粗糙了，肯定还需要考量更多的因素，例如数据量，日志大小，通信方式等，不过我相信到现在大家应该对ELK有了一个基本的认知。 参考Kibana 中文指南 使用ElasticSearch+LogStash+Kibana+Redis搭建日志管理服务 使用Logstash收集Nginx日志 Logstash Multiline Tomcat and Apache Log Parsing Logstash and Log4j","tags":[{"name":"Logstash","slug":"Logstash","permalink":"https://blog.kazaff.me/tags/Logstash/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.kazaff.me/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"https://blog.kazaff.me/tags/Kibana/"}]},{"title":"Https情结","date":"2015-06-01T09:37:12.000Z","path":"2015/06/01/https情结/","text":"早在几年前，我就一直想在自己的博客用上https，不要问我为什么，虽然并没有什么卵用。 随着http2.0的宣传，大量的关于web性能和安全的信息跃然纸上，这让我对https更加的向往~工作了这么多年，一直想给公司开发的系统配置上ssl，最近刚好看到一篇写的不错的文章，跟着大牛测了一下，还真配成了。 首先大家知道，想搭建https，你是需要购买证书的，不然用户访问你的https的时候还需要手动安装证书，这会让一些不明真相的用户感到反感，而且由于你的私人证书没有什么信用可言，所以更多的人会选择放弃访问你的系统，而这是我们最不希望看到的。 不过免费提供证书的机构还是存在的，StartSSL就是这样的公司，下面的两个参考链接分别介绍了如何搭建https服务和如何申请免费的证书，希望大家都能早日用上https~ 参考NGINX 配置 SSL 证书 + 搭建 HTTPS 网站教程 Apache + WordPress + SSL 完全指南","tags":[{"name":"https","slug":"https","permalink":"https://blog.kazaff.me/tags/https/"},{"name":"ssl","slug":"ssl","permalink":"https://blog.kazaff.me/tags/ssl/"},{"name":"证书","slug":"证书","permalink":"https://blog.kazaff.me/tags/证书/"}]},{"title":"Screen命令","date":"2015-05-31T09:37:12.000Z","path":"2015/05/31/screen命令/","text":"今天发现一个好东西，你是不是以前和我一样，当需要执行一个后台常驻进程的命令时，总像下面这样写： nohup command &amp; 但是尴尬的是，一旦你这么做，当你需要查看相关命令输出的时候，你只能扇自己脸了。 有童鞋说可以指定输出到对应文件，是的，然后就是不停的刷新对应文件，你有考虑过文件的感受么？ 今天算是长见识了，你其实可以通过screen命令创建一个平行环境，该环境下运行的终端命令，其父进程不是sshd登录会话，也就是说它可以避免用户退出进程消失的问题，更神奇的是，它还支持随时重新接管回终端继续操作。妈蛋，简直逆天~ 创建独立的screen命令如下： screen -dmS kazaff-Demo 接管连入创建的kazaff-Demo环境命令如下： screen -r kazaff-Demo 想退出当前平行环境，也不难，直接按 Ctrl+A+D 键，如果创建了多个screen，可以查看列表，命令如下： screen -list 好了，从今以后，又可以装逼了~ 如果你觉得这个不够屌，那你不妨看看Tmux。","tags":[{"name":"ssh","slug":"ssh","permalink":"https://blog.kazaff.me/tags/ssh/"},{"name":"tmux","slug":"tmux","permalink":"https://blog.kazaff.me/tags/tmux/"},{"name":"终端","slug":"终端","permalink":"https://blog.kazaff.me/tags/终端/"}]},{"title":"关于Flow","date":"2015-05-27T09:37:12.000Z","path":"2015/05/27/关于Flow/","text":"最近感觉又back2js了，仅仅是几个月的暂别，就感觉js又tmd翻天覆地了￥Q@#@#$～ 这次要科普的是Flow，这玩意儿是FaceBook开源的一个类型检查库，从此写js就再也不”自由”了～都说FB的天才多，现在终于感觉到了，人家玩的都是语言扩展，顿时感觉高端大气档次高了啊！其实为若类型语言增加类型检查，JS并不是第一个语言，早先FB就给PHP改造过了……之所以我突然想起来科普这个，是因为最近在看React的代码，发现有很多看似眼熟又总觉得怪怪的代码风格，一开始以为是ES6的新特性，但是查了一下却没找到对应介绍，谁知道是Flow提供的，真是让我大开眼界啊！ 看一个官方的demo： function add(num1, num2) { return num1 + num2; } var x = add(3, &apos;0&apos;); console.log(x); 你说，x的值是啥？3？30？undefined？不管是啥，其实这都是动态类型带来的连带伤害，我们在以前的js/php编程的日子里上面这种场景屡见不鲜。但是FB的大大们坐不住了，非得改到爽才行： /* @flow */ function add(num1: number, num2: number): number { return num1 + num2; } var x: number = add(3, &apos;0&apos;); console.log(x); 在Flow的世界里，你就得这么老实的写，这样，是个人都应该知道x应该是啥了吧？ 这里我就好奇了，Flow提供的既然不是标准的js语法，那浏览器怎么可能理解？确实不能理解，别说浏览器，我的WebStorm都报错，怎么办？ 我们需要在交给客户端之前转换为标准的js代码，官方提供了对应的转换工具，这样我们的js代码就可以在编写时拥有完善的类型检查，又可以直接在产品环境下运行了，这种思想和less如出一辙。 再往下聊，就需要我们同时对js这门语言和Flow提供的适配规则有很全面的了解了。所以我就不多说了，免得误人子弟～～","tags":[{"name":"flow","slug":"flow","permalink":"https://blog.kazaff.me/tags/flow/"},{"name":"facebook","slug":"facebook","permalink":"https://blog.kazaff.me/tags/facebook/"},{"name":"类型检查","slug":"类型检查","permalink":"https://blog.kazaff.me/tags/类型检查/"}]},{"title":"ReactNative的Image","date":"2015-05-25T09:37:12.000Z","path":"2015/05/25/ReactNative的Image/","text":"今天就聊一个问题，React Native的Image读取本地图片的方法。（甚少见我如此不罗嗦了吧？那是因为真的没时间啊……） 按照官方文档中的描述，RCT（React Native）中考虑到诸多原因，提供了一种快速获取本地图片资源的方法：require(&#39;image!你的图片名称&#39;)。看上去很简单，但对于我这种移动端开发小白来说，简单的有点简陋了，首先，我们要从如何把本地图片导入到项目说起，在xcode的文件导航栏，找到你的项目文件夹，点击其中的“Images.xcassets”，然后你就会看到右边工作区界面，然后直接把你想导入的本地图片直接拖入工作区即可，具体步骤可参见这里，虽然该帖子的xcode版本不是最新的，但步骤没啥问题。 这里注意的是，我们只能导入png，这可能并不是xcode要求的，但是RCT只会去读取png图片～～此外，我在导入的时候，xcode总是提醒我权限不够，你只需要修改“Images.xcassets”文件的权限即可，如下图： 好了，接下来你就可以使用官方提供的代码实例测试了： return ( &lt;View&gt; &lt;Image style={styles.icon} source={require(&apos;image!myIcon&apos;)} /&gt; &lt;/View&gt; ); 你会发现，图片竟然还是没有显示，这是为什么呢？告诉你，你还需要重新编译项目哦，简单的依靠RCT提供的动态刷新是无法加载你刚新增的本地资源的～ ok，按照我的描述，你应该就不会遇到下面这个头疼的报错了：","tags":[{"name":"权限","slug":"权限","permalink":"https://blog.kazaff.me/tags/权限/"},{"name":"ReactNative","slug":"ReactNative","permalink":"https://blog.kazaff.me/tags/ReactNative/"},{"name":"xcode","slug":"xcode","permalink":"https://blog.kazaff.me/tags/xcode/"}]},{"title":"React和flux初学","date":"2015-05-24T09:37:12.000Z","path":"2015/05/24/React和flux初尝心得/","text":"身为一个刚满三十岁的程序猿（还差2个月），我婶婶的体会到技术的无止境，这是一件多么有挑战的工作啊~ 这几天花时间看了一下ReactJS，感觉确实是个不错的思想，之前在公司内部推广AngularJS，虽然基本上已经在团队内部普及了MVVM的理念，但组件化还是做得一塌糊涂啊~初学ReactJS，最大的感触就是组件化的那叫一个彻底~以我目前的了解，尽管无法说出其AngularJS指令的功能差异，但至少在理念上，ReactJS更接近趋势一些~ 老实讲，原先认为可能要花很久来学习ReactJS，但没想到粗略的看了两天文档，就对其有了一个比较全面的了解，相比Angular确实有更好的学习曲线~ 其实原本的目的是为了React-Native，但现在对React本身就产生了非常大的兴趣，入门教程强烈推荐：阮一峰的分享。相信你看完以后，基本上就对React有了一个较为完整的了解，再花点时间啃一下官方文档，基本上就可以看懂简单的项目了…… 为什么说简单的项目呢？因为很少有实际项目是仅使用基础React就完成全部的，原因是因为： React仅仅是UI 其实这也是我一开始的疑惑，相比Angular提供整套的路由，控制器，作用域等概念，React简直太轻了，甚至可以说是缺乏必要的功能（当然，这都是个人观点~）。 正因如此，Facebook又进一步推出了一个思想：Flux。其宣扬单向数据流，让我这个当时震撼于“双向绑定”的童鞋又一次被撼动（小人物就是这样，永远只能追赶大牛）~ 观望了一下社区，发现大家又各自选择自己中意的Flux具体实现（Flux更合适被当做一种思想，而非实现），这里有一个项目，用来对比各种具体实现的差异。简直太宏伟了，感觉一辈子也看不完的资料啊，最后我选择的是：Refluxjs。这个看起来还是非常亲切的，相比Facebook提供的原版Flux，要“简单”很多~ 现在，基本上可以看懂中大型项目了，但貌似还少了点啥，路由，没错，就是每个传统web项目非常重要的一环，不过不要怕，也已经有非常强大的扩展来帮我们完成：React-router。 总结下来，感觉要比angular轻量很多（至少在概念上），下一个小项目中我感觉自己会尝试一下它~ PS：貌似Facebook在推一个新的React框架（应该算框架吧）：Relay，强大的一笔~","tags":[{"name":"react","slug":"react","permalink":"https://blog.kazaff.me/tags/react/"},{"name":"flux","slug":"flux","permalink":"https://blog.kazaff.me/tags/flux/"},{"name":"mvvm","slug":"mvvm","permalink":"https://blog.kazaff.me/tags/mvvm/"}]},{"title":"RequireJS导入cmd模块","date":"2015-05-23T09:37:12.000Z","path":"2015/05/23/RequireJS导入CMD模块/","text":"搞了一段时间的JAVA，又要再一次回归前端技术了，其实这么说也不准确，因为现在的前端技术已经囊括了太多~ 这次回来，是为了征服移动端，也很可能最后一次为目前的公司征战沙场了（有种说不出道不明预感~）！我一直坚信，应用级别的APP开发套件迟早一定是H5的天下，跨平台永远是人类追求的目标，这个应该是毋庸置疑的！谁能做到真的“跨平台”，谁就能笼络更多的人心。目前我看到的发展史是： Native --&gt; Hybrid --&gt; H5 虽然这路程并不平坦，反反复复坎坎坷坷，但大方向还是非常明确的~ 扯了那么多，今次的主题是AMD和CMD，之所以纠结这两个概念，是因为目前项目中计划开发一些用于封装逻辑的SDK，那么，就需要我们提供各种语言和平台下的SDK，当然这个范围一开始并不会太大，毕竟能力有限。 前端，移动端，Node端我打算实现基于JS的SDK，这就要求在项目模块化组织时选择一个通用性更强的标准，一开始打算使用RequireJS，因为目前公司的项目中使用的就是它，不过，它是基于AMD概念的，也就是说更偏重于前端，不过，还好，RequireJS可以兼容基于CMD概念实现的模块，下面我看一个例子： Project |-index.html |-main.js |-scripts | |-require.js |-lib |-foo | |-main.js |-bar |-main.js index.html &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt;&lt;title&gt;&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;script data-main=&quot;./main&quot; src=&quot;scripts/require.js&quot;&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; main.js require.config({ baseUrl: &quot;./lib&quot;, //所有模块的base URL,注意，以.js结尾的文件加载时不会使用该baseUrl packages:[&quot;foo&quot;,&quot;bar&quot;], //需要把所有CMD的模块都声明在这里 waitSeconds: 10 //waitSeconds是指定最多花多长等待时间来加载一个JavaScript文件，用户不指定的情况下默认为7秒 }); require([&quot;foo&quot;],function(foo){ console.log(&quot;test&quot;); foo.log(); }); lib/foo/main.js define(function(require, exports, module){ exports.name = &quot;foo&quot;; exports.log = function(){ console.log(this.name); } var bar = require(&quot;bar&quot;); bar.log(); }); lib/bar/main.js define(function(require, exports, module){ exports.name = &quot;bar&quot;; exports.log = function(){ console.log(this.name); } }); 代码很简单，直接放在web目录下测试即可，运行看一下控制台的输出： bar test foo 留意一下输出的顺序，“bar”甚至再“test”之前，这是为什么呢？理由很简单，因为foo模块被main.js依赖，所以requireJS会先加载并执行它，这个时候发现foo模块又依赖bar模块，所以会先去加载bar模块，加载完毕后执行foo模块中写的逻辑，这个时候会打印出“bar”，接下来，main.js的回调逻辑会执行，打印“test”，然后再调用“foo.log()”打印最后的“foo”。 这样，我们就可以把SDK以CMD规范来编写，将来用于多种场景~ 参考： RequireJS 中文网 CMD 模块定义规范","tags":[{"name":"amd","slug":"amd","permalink":"https://blog.kazaff.me/tags/amd/"},{"name":"cmd","slug":"cmd","permalink":"https://blog.kazaff.me/tags/cmd/"},{"name":"RequireJS","slug":"RequireJS","permalink":"https://blog.kazaff.me/tags/RequireJS/"}]},{"title":"Avro的三种序列化方法","date":"2015-04-30T15:54:30.000Z","path":"2015/04/30/Avro的三种序列化与反序列化方法/","text":"知道Avro有一段时间了，但是并没有过多的动手测试其用法，今天花时间好好找了几个例子来练习Avro，特此记录，供以后查阅。Avro提供了自身的RPC实现，可以更完美的利用Avro的设计思想来完成高性能的跨进程通信，不过我们这里只注重使用Avro的序列化/反序列化的细节。 Avro使用Json来声明Schema，这个预先定义的模式作为通信两端数据协议，在网络传输前后对目标数据进行二进制处理。太多的理论可以从Avro官网上了解到，我们下面就进入代码模式。 测试代码是基于Maven的，你可能会使用下面的pom.xml： &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro&lt;/artifactId&gt; &lt;version&gt;1.7.6-cdh5.2.5&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera-repo-releases&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/repo/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; 方法1 该方法描述的场景是：用指定的json schema对数据进行编码，并用相同/近似的schema对数据进行解码。 import java.io.ByteArrayOutputStream; import java.io.IOException; import org.apache.avro.Schema; import org.apache.avro.generic.GenericData; import org.apache.avro.generic.GenericDatumReader; import org.apache.avro.generic.GenericDatumWriter; import org.apache.avro.generic.GenericRecord; import org.apache.avro.io.DatumReader; import org.apache.avro.io.Decoder; import org.apache.avro.io.DecoderFactory; import org.apache.avro.io.Encoder; import org.apache.avro.io.EncoderFactory; public class AvroTest { public static void main(String[] args) throws IOException { // Schema String schemaDescription = &quot; { \\n&quot; + &quot; \\&quot;name\\&quot;: \\&quot;FacebookUser\\&quot;, \\n&quot; + &quot; \\&quot;type\\&quot;: \\&quot;record\\&quot;,\\n&quot; + &quot; \\&quot;fields\\&quot;: [\\n&quot; + &quot; {\\&quot;name\\&quot;: \\&quot;name\\&quot;, \\&quot;type\\&quot;: \\&quot;string\\&quot;},\\n&quot; + &quot; {\\&quot;name\\&quot;: \\&quot;num_likes\\&quot;, \\&quot;type\\&quot;: \\&quot;int\\&quot;},\\n&quot; + &quot; {\\&quot;name\\&quot;: \\&quot;num_photos\\&quot;, \\&quot;type\\&quot;: \\&quot;int\\&quot;},\\n&quot; + &quot; {\\&quot;name\\&quot;: \\&quot;num_groups\\&quot;, \\&quot;type\\&quot;: \\&quot;int\\&quot;} ]\\n&quot; + &quot;}&quot;; Schema s = Schema.parse(schemaDescription); //parse方法在当前的Avro版本下已不推荐使用 ByteArrayOutputStream outputStream = new ByteArrayOutputStream(); Encoder e = EncoderFactory.get().binaryEncoder(outputStream, null); GenericDatumWriter w = new GenericDatumWriter(s); // Populate data GenericRecord r = new GenericData.Record(s); r.put(&quot;name&quot;, new org.apache.avro.util.Utf8(&quot;kazaff&quot;)); r.put(&quot;num_likes&quot;, 1); r.put(&quot;num_groups&quot;, 423); r.put(&quot;num_photos&quot;, 0); // Encode w.write(r, e); e.flush(); byte[] encodedByteArray = outputStream.toByteArray(); String encodedString = outputStream.toString(); System.out.println(&quot;encodedString: &quot;+encodedString); // Decode using same schema DatumReader&lt;GenericRecord&gt; reader = new GenericDatumReader&lt;GenericRecord&gt;(s); Decoder decoder = DecoderFactory.get().binaryDecoder(encodedByteArray, null); GenericRecord result = reader.read(null, decoder); System.out.println(result.get(&quot;name&quot;).toString()); System.out.println(result.get(&quot;num_likes&quot;).toString()); System.out.println(result.get(&quot;num_groups&quot;).toString()); System.out.println(result.get(&quot;num_photos&quot;).toString()); } } 可以看出，编码解码都需要指定schema，这相当于要求通信两端必须提前通过某种交互来完成schema的同步工作（貌似Avro自带的RPC就实现了这个细节），通过代码中的打印语句，我们可以留意到，这种方式下，编码后的数据是不包含schema的（我不确定对，但相比第二种方式确实有这种差异性）。 方法2 该方法采用的方式是通过指定的json schema把数据进行编码，并同时把schema作为metadata放在编码后的数据头部。解码则使用内嵌在数据中的schema来完成。 import java.io.ByteArrayInputStream; import java.io.ByteArrayOutputStream; import java.io.IOException; import org.apache.avro.Schema; import org.apache.avro.file.DataFileStream; import org.apache.avro.file.DataFileWriter; import org.apache.avro.generic.GenericData; import org.apache.avro.generic.GenericDatumReader; import org.apache.avro.generic.GenericDatumWriter; import org.apache.avro.generic.GenericRecord; import org.apache.avro.io.DatumReader; import org.apache.avro.io.DatumWriter; public class AvroDataFile { /** * @param args * @throws IOException */ public static void main(String[] args) throws IOException { // Schema String schemaDescription = &quot; { \\n&quot; + &quot; \\&quot;name\\&quot;: \\&quot;FacebookUser\\&quot;, \\n&quot; + &quot; \\&quot;type\\&quot;: \\&quot;record\\&quot;,\\n&quot; + &quot; \\&quot;fields\\&quot;: [\\n&quot; + &quot; {\\&quot;name\\&quot;: \\&quot;name\\&quot;, \\&quot;type\\&quot;: \\&quot;string\\&quot;},\\n&quot; + &quot; {\\&quot;name\\&quot;: \\&quot;num_likes\\&quot;, \\&quot;type\\&quot;: \\&quot;int\\&quot;},\\n&quot; + &quot; {\\&quot;name\\&quot;: \\&quot;num_photos\\&quot;, \\&quot;type\\&quot;: \\&quot;int\\&quot;},\\n&quot; + &quot; {\\&quot;name\\&quot;: \\&quot;num_groups\\&quot;, \\&quot;type\\&quot;: \\&quot;int\\&quot;} ]\\n&quot; + &quot;}&quot;; Schema schema = Schema.parse(schemaDescription); //parse方法在当前的Avro版本下已不推荐使用 ByteArrayOutputStream os = new ByteArrayOutputStream(); DatumWriter&lt;GenericRecord&gt; writer = new GenericDatumWriter&lt;GenericRecord&gt;(schema); DataFileWriter&lt;GenericRecord&gt; dataFileWriter = new DataFileWriter&lt;GenericRecord&gt;(writer); dataFileWriter.create(schema, os); // Populate data GenericRecord datum = new GenericData.Record(schema); datum.put(&quot;name&quot;, new org.apache.avro.util.Utf8(&quot;kazaff&quot;)); datum.put(&quot;num_likes&quot;, 1); datum.put(&quot;num_groups&quot;, 423); datum.put(&quot;num_photos&quot;, 0); dataFileWriter.append(datum); dataFileWriter.close(); System.out.println(&quot;encoded string: &quot; + os.toString()); //可以看到，数据是头部携带了schema metadata // Decode DatumReader&lt;GenericRecord&gt; reader = new GenericDatumReader&lt;GenericRecord&gt;(); ByteArrayInputStream is = new ByteArrayInputStream(os.toByteArray()); DataFileStream&lt;GenericRecord&gt; dataFileReader = new DataFileStream&lt;GenericRecord&gt;(is, reader); GenericRecord record = null; while (dataFileReader.hasNext()) { record = dataFileReader.next(record); System.out.println(record.getSchema()); //可以直接获取该数据的json schema定义 System.out.println(record.get(&quot;name&quot;).toString()); System.out.println(record.get(&quot;num_likes&quot;).toString()); System.out.println(record.get(&quot;num_groups&quot;).toString()); System.out.println(record.get(&quot;num_photos&quot;).toString()); } } } 该方法最符合我的预期，这样若我们使用第三方RPC框架（like dubbo），就不需要为schema的数据同步问题花太多精力，但是能想到的问题就是通信成本，毕竟每次数据通信都要携带json schema metadata感觉总是不太好接受，尤其是针对单一长连接场景，似乎更应该去实现类似Avro提供的RPC那样的机制，可以获得更好的性能。 方法3 利用Avro提供的反射机制来完成数据的编码解码，该方法我没有进行实测。 import java.io.ByteArrayOutputStream; import java.io.IOException; import org.apache.avro.Schema; import org.apache.avro.io.DatumWriter; import org.apache.avro.io.Decoder; import org.apache.avro.io.DecoderFactory; import org.apache.avro.io.Encoder; import org.apache.avro.io.EncoderFactory; import org.apache.avro.reflect.ReflectData; import org.apache.avro.reflect.ReflectDatumReader; import org.apache.avro.reflect.ReflectDatumWriter; import dto.AnotherEmployee; import dto.Employee; public class AvroReflect { final static ReflectData reflectData = ReflectData.get(); final static Schema schema = reflectData.getSchema(Employee.class); public static void main(String[] args) throws IOException { ByteArrayOutputStream os = new ByteArrayOutputStream(); Encoder e = EncoderFactory.get().binaryEncoder(os, null); DatumWriter&lt;Employee&gt; writer = new ReflectDatumWriter&lt;Employee&gt;(schema); Employee employee = new Employee(); employee.setName(&quot;Kamal&quot;); employee.setSsn(&quot;000-00-0000&quot;); employee.setAge(29); writer.write(employee, e); e.flush(); System.out.println(os.toString()); ReflectData reflectData = ReflectData.get(); Schema schm = reflectData.getSchema(AnotherEmployee.class); ReflectDatumReader&lt;AnotherEmployee&gt; reader = new ReflectDatumReader&lt;AnotherEmployee&gt;(schm); Decoder decoder = DecoderFactory.get().binaryDecoder(os.toByteArray(), null); AnotherEmployee decodedEmployee = reader.read(null, decoder); System.out.println(&quot;Name: &quot;+decodedEmployee.getName()); System.out.println(&quot;Age: &quot;+decodedEmployee.getAge()); System.out.println(&quot;SSN: &quot;+decodedEmployee.getSsn()); } } 以上代码，均参考自：Apache Avro “HelloWorld” Examples","tags":[{"name":"Avro","slug":"Avro","permalink":"https://blog.kazaff.me/tags/Avro/"},{"name":"序列化","slug":"序列化","permalink":"https://blog.kazaff.me/tags/序列化/"},{"name":"RPC","slug":"RPC","permalink":"https://blog.kazaff.me/tags/RPC/"},{"name":"json","slug":"json","permalink":"https://blog.kazaff.me/tags/json/"},{"name":"schema","slug":"schema","permalink":"https://blog.kazaff.me/tags/schema/"}]},{"title":"JDBC的超时问题","date":"2015-04-28T15:54:30.000Z","path":"2015/04/28/JDBC的超时问题/","text":"java新人，问题多多~今天来聊聊JDBC的连接超时问题。首先，我把mysql的两个相关参数设置为： interactive_timeout=10 wait_timeout=10 单位为秒，也就是说不管是交互还是非交互式的客户端在空闲十秒mysql就会自动断开对应连接。 好，继续，使用org.springframework.jdbc.datasource.DriverManagerDataSource来进行数据库查询，是否会出现超时错误呢？ ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring-config.xml&quot;); JdbcTemplate jdbcTemplate = (JdbcTemplate)context.getBean(&quot;jdbcTemplate&quot;); String sql = &quot;SELECT * FROM diabloo LIMIT 1&quot;; User target = jdbcTemplate.queryForObject(sql, new RowMapper&lt;User&gt;() { public User mapRow(ResultSet rs, int i) throws SQLException { User one = new User(); one.setName(rs.getString(&quot;name&quot;)); return one; } }); System.out.println(target.getName()); //睡眠足够时间，导致mysql连接超时 Thread.sleep(11000); sql = &quot;SELECT * FROM diabloo LIMIT 1&quot;; target = jdbcTemplate.queryForObject(sql, new RowMapper&lt;User&gt;() { public User mapRow(ResultSet rs, int i) throws SQLException { User one = new User(); one.setName(rs.getString(&quot;name&quot;)); return one; } }); System.out.println(target.getName()); 我以为会超时的，但却一切正常，后来gg搜索，才发现原因： DriverManagerDataSource建立连接是只要有连接就新建一个connection，…… 这在我看来，有点吃惊，不过后来就明白了Spring提供这个类的目的，我想更多的理由是用它来充当数据池的connectFactory角色。 再看来直接使用原始的JDBC情况下是否会出现超时： Class.forName(&quot;com.mysql.jdbc.Driver&quot;); // 建立连接 Connection con = DriverManager.getConnection( &quot;jdbc:mysql://localhost:3306/test&quot;, &quot;root&quot;, &quot;&quot;); Statement stmt = con.createStatement(); ResultSet rs = stmt.executeQuery(&quot;SELECT * FROM diabloo LIMIT 1&quot;); while (rs.next()) { String name = rs.getString(&quot;name&quot;); System.out.println(name); } //睡眠足够时间，导致mysql连接超时 Thread.sleep(11000); stmt = con.createStatement(); rs = stmt.executeQuery(&quot;SELECT * FROM diabloo LIMIT 1&quot;); while (rs.next()) { String name = rs.getString(&quot;name&quot;); System.out.println(name); } 这就和猜想一致了，报超时异常了~~ 另附一大神的文章，推荐阅读~","tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.kazaff.me/tags/mysql/"},{"name":"timeout","slug":"timeout","permalink":"https://blog.kazaff.me/tags/timeout/"},{"name":"超时","slug":"超时","permalink":"https://blog.kazaff.me/tags/超时/"},{"name":"jdbc","slug":"jdbc","permalink":"https://blog.kazaff.me/tags/jdbc/"},{"name":"DriverManagerDataSource","slug":"DriverManagerDataSource","permalink":"https://blog.kazaff.me/tags/DriverManagerDataSource/"}]},{"title":"Kafka+Avro的demo","date":"2015-04-27T15:54:30.000Z","path":"2015/04/27/Kafka+Avro的demo/","text":"最近在对消息中间件进行调研，原先项目里使用的是RabbitMQ，理由很简单：对开发语言支持度是最好的，没有之一。但是业界的反馈是，其高并发和分布式支持存在不足~我就打算再了解一下kafka，看看它是怎么个用法~~ 如RabbitMQ一样，kafka也提供了终端脚本来完成基本功能的测试，可以看一下这里。但光玩玩脚本或官方提供的例子，是不能满足我的~ 作为把消息队列，让其使用在项目中，除了解决和业务代码进行互动以外，还要考虑数据传输的格式问题，也就是说如何解决producer和consumer之间通信的协议问题。 官方推荐的就是Avro，只可惜我找了半天，都没有一个现成的kafka+Avro的demo供我测试，那就只能自己试着写个了~~ package me.kazaff.mq; import kafka.consumer.ConsumerConfig; import kafka.consumer.ConsumerIterator; import kafka.consumer.KafkaStream; import kafka.javaapi.consumer.ConsumerConnector; import kafka.javaapi.producer.Producer; import kafka.producer.KeyedMessage; import kafka.producer.ProducerConfig; import me.kazaff.mq.avro.Message; import org.apache.avro.io.*; import org.apache.avro.specific.SpecificDatumReader; import org.apache.avro.specific.SpecificDatumWriter; import java.io.ByteArrayOutputStream; import java.io.IOException; import java.util.*; public class Main { public static void main(String[] args){ try { //消息生产 produce(); //消息消费 consume(); }catch (Exception ex){ System.out.println(ex); } } private static void produce() throws IOException{ Properties props = new Properties(); props.put(&quot;metadata.broker.list&quot;, &quot;localhost:9092&quot;); props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.DefaultEncoder&quot;); props.put(&quot;key.serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;); //key的类型需要和serializer保持一致，如果key是String，则需要配置为kafka.serializer.StringEncoder，如果不配置，默认为kafka.serializer.DefaultEncoder，即二进制格式 props.put(&quot;partition.class&quot;, &quot;me.kazaff.mq.MyPartitioner&quot;); props.put(&quot;request.required.acks&quot;, &quot;1&quot;); ProducerConfig config = new ProducerConfig(props); Producer&lt;String, byte[]&gt; producer = new Producer&lt;String, byte[]&gt;(config); Random rnd = new Random(); for(int index = 0; index &lt;= 10; index++){ Message msg = new Message(); msg.setUrl(&quot;blog.kazaff.me&quot;); msg.setIp(&quot;192.168.137.&quot; + rnd.nextInt(255)); msg.setDate(Long.toString(new Date().getTime())); DatumWriter&lt;Message&gt; msgDatumWriter = new SpecificDatumWriter&lt;Message&gt;(Message.class); ByteArrayOutputStream os = new ByteArrayOutputStream(); try { Encoder e = EncoderFactory.get().binaryEncoder(os, null); msgDatumWriter.write(msg, e); e.flush(); byte[] byteData = os.toByteArray(); KeyedMessage&lt;String, byte[]&gt; data = new KeyedMessage&lt;String, byte[]&gt;(&quot;demo&quot;, &quot;0&quot;, byteData); producer.send(data); }catch (IOException ex){ System.out.println(ex.getMessage()); }finally { os.close(); } } producer.close(); } private static void consume(){ Properties props = new Properties(); props.put(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;); props.put(&quot;group.id&quot;, &quot;1&quot;); props.put(&quot;zookeeper.session.timeout.ms&quot;, &quot;400&quot;); props.put(&quot;zookeeper.sync.time.ms&quot;, &quot;200&quot;); props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); ConsumerConnector consumer = kafka.consumer.Consumer.createJavaConsumerConnector(new ConsumerConfig(props)); Map&lt;String, Integer&gt; topicCountMap = new HashMap&lt;String, Integer&gt;(); topicCountMap.put(&quot;demo&quot;, 1); Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; consumerMap = consumer.createMessageStreams(topicCountMap); List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt; streams = consumerMap.get(&quot;demo&quot;); KafkaStream steam = streams.get(0); ConsumerIterator&lt;byte[], byte[]&gt; it = steam.iterator(); while (it.hasNext()){ try { DatumReader&lt;Message&gt; reader = new SpecificDatumReader&lt;Message&gt;(Message.class); Decoder decoder = DecoderFactory.get().binaryDecoder(it.next().message(), null); Message msg = reader.read(null, decoder); System.out.println(msg.getDate() + &quot;,&quot; + msg.getUrl() + &quot;,&quot; + msg.getIp()); }catch (Exception ex){ System.out.println(ex); } } if(consumer != null) consumer.shutdown(); } } PS：这只是个测试的例子，存在各种问题，不建议直接使用在项目中。 为了方便大家直接测试，我把pom.xml也贴出来： &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;me.kazaff.mq&lt;/groupId&gt; &lt;artifactId&gt;kafkaProducer&lt;/artifactId&gt; &lt;version&gt;0.0.1&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;kafkaProducer&lt;/name&gt; &lt;description&gt;The Producer of message, use Avro to encode data, and send to the kafka.&lt;/description&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt; &lt;version&gt;0.8.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro&lt;/artifactId&gt; &lt;version&gt;1.7.6-cdh5.2.5&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.7.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;schema&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;sourceDirectory&gt;${project.basedir}/src/avro/&lt;/sourceDirectory&gt; &lt;outputDirectory&gt;${project.basedir}/src/&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera-repo-releases&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/repo/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/project&gt; 下面是Avro使用的Schema： { &quot;namespace&quot;: &quot;me.kazaff.mq.avro&quot;, &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Message&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;date&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;url&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;ip&quot;, &quot;type&quot;: &quot;string&quot; } ] } 代码中使用的MyPartitioner其实很sb： package me.kazaff.mq; import kafka.producer.Partitioner; import kafka.utils.VerifiableProperties; /** * Created by kazaff on 2015/4/21. */ public class MyPartitioner implements Partitioner { public MyPartitioner(VerifiableProperties props){} public int partition(Object key, int numPartitions){ return 0; } } 需要注意的是，我是先使用Maven根据配置的plugin，把声明的Schema先处理生成对应的Class文件，然后再进行运行测试的~ 问题 1. java.lang.ClassCastException: [B cannot be cast to java.lang.String 解决方法很简单： props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.DefaultEncoder&quot;); 这样，kafka就默认使用二进制的序列化方案处理Avro的编码结果了。 2. java.lang.ClassCastException: java.lang.String cannot be cast to [B 这个问题是最恶心的，搜了半天都没有找到原因，原因是问题1中那么设置后，Kafka所有的数据序列化方式都成了二进制方式，包括我们后面要使用的“key”（用于kafka选择分区的线索）。 所以你还需要再加一条配置： props.put(&quot;key.serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;); 单独设置一下“key”的序列化方式，这样就可以编译运行了~~ 初尝Kafka和Avro，就这么点儿要记录的，不要嫌少哟~~","tags":[{"name":"Avro","slug":"Avro","permalink":"https://blog.kazaff.me/tags/Avro/"},{"name":"序列化","slug":"序列化","permalink":"https://blog.kazaff.me/tags/序列化/"},{"name":"Kafka","slug":"Kafka","permalink":"https://blog.kazaff.me/tags/Kafka/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://blog.kazaff.me/tags/RabbitMQ/"}]},{"title":"前后端分离的契约：接口文档","date":"2015-04-25T15:54:30.000Z","path":"2015/04/25/基于REST的接口文档/","text":"最近在公司开发部举办了一次关于前后端配合为主题的研（si）讨(b)会，主要矛盾还是针对前后端协作所使用的接口文档随意变更问题。其实，接口文档是在我们前后端分离后产生的一个文档，现在的前端基于Angularjs，以Ajax的方式向后端暴露的RESTful接口进行调用，这就要求，在前后端各自开发的过程中如何尽可能的摆脱跨端的依赖。 最直观的方案，就是在设计阶段中产生一个接口文档，在编码阶段开发人员根据分工各自根据接口文档进行功能的实现。这本来是一套简单可爱的流程，不过由于我们在设计阶段所暴露的问题导致了混乱产生： 经验不足，无法一次性就设计出完美的接口 时间不够 认知问题，对接口文档重视程度不够 经过几个小时的撕逼，基本上大家统一了一些基础观念，把接口文档当做是契约，在确定后严格履行其声明。 不过，只是解决了认知问题远远不够，光有接口文档也是不能达到理想状态： 拒绝写文档的繁琐 尽可能的摆脱依赖 好，第一个问题，我们需要一个足够“程序员化”的接口文档编写工具，找了找，发现一个轻量级的工具不错：APIDOC。 该工具针对RESTful风格的api，提供了多种语言的支持，而且基于码农擅长的注解方式完成接口的声明，十分贴心。而且，可以自动导出成html，方便部署在各种环境下。 美中不足的是，它没有提供MOCK功能，自带的模拟请求也需要依赖后端服务。不过我依然推荐的理由是：真的很简单。 跟我们的开发人员商量了一下，发现前端人员对mock数据的需求非常强烈，这让我不得不忍痛放弃APIDOC，继续寻找油腻的师姐。 知乎上我看有人推荐一款工具：RAP，阿里吊炸天啊~ RAP不仅提供了对Mock的完美支持，而且还加入了一些组织结构的功能用于满足团队的使用。 不过就是复杂一些，还好官方提供了视频教学。目前能想到的遗憾之处，就是不像APIDOC那样，可以方便的直接作为接口文档开放给所有第三方使用。","tags":[{"name":"api","slug":"api","permalink":"https://blog.kazaff.me/tags/api/"},{"name":"RAP","slug":"RAP","permalink":"https://blog.kazaff.me/tags/RAP/"},{"name":"RESTful","slug":"RESTful","permalink":"https://blog.kazaff.me/tags/RESTful/"},{"name":"mock","slug":"mock","permalink":"https://blog.kazaff.me/tags/mock/"}]},{"title":"Dubbo-Admin与多注册中心","date":"2015-04-05T15:54:30.000Z","path":"2015/04/05/dubbo-admin与多注册中心/","text":"dubbo-admin是否支持同时关联多个注册中心统一管理不同注册中心上注册的服务？ 答案是：不支持！ 这里指的多注册中心，并不是zookeeper集群，两者的差别看这里和这里。 dubbo-admin提供了运维界面，辅助我们更好的管理和维护服务之间的依赖等信息。刚认识dubbo时确实被这么逆天的功能给震惊了（现在也是~）。 那么该怎么办呢？目前想到的就只是为每一个注册中心部署一套admin管理后台。确实听起来有些土鳖，但应该是最省事儿的了吧~ 当然，也可以改造一下现有的dubbo-admin逻辑，只不过以现在我的道行还做不到啊~坐等高手！ 最后还要叮嘱的是，如果你的注册中心和服务之间的网络质量比较差，建议你配置一个较长的timeout时间，否则会出现： ... nested exception is org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 5000 ... 这么写： &lt;dubbo:registry protocol=&quot;zookeeper&quot; address=&quot;192.168.76.138:2182,192.168.76.138:2181,192.168.76.138:2183&quot; timeout=&quot;100000&quot;/&gt;","tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://blog.kazaff.me/tags/zookeeper/"}]},{"title":"Dubbo的通信模型","date":"2015-04-01T15:54:30.000Z","path":"2015/04/01/dubbo的通信模型/","text":"上接dubbo的编解码，序列化和通信。 在上面那一篇文章中，大致的介绍了一下dubbo的通信机制。不过感觉说的有点乱，总结了下面一张图： 图中的从“外到内”对应着“从dubbo底层到应用的业务逻辑层”，我把在这个过程中起到关键作用的类都标注了出来，注意：这里还是基于dubbo的默认协议dubbo，默认通信框架netty，以及默认的序列化方式dubbocodec。 这里我想讨论的是，注意看图中的“AllChannelHandler”类的职责之一： 把对应事件（connected、disconnected、received、caught）的执行业务分配给线程池中可使用线程 dubbo处理handler所使用的线程并非来自netty提供的I/O Work线程，而是dubbo自身来维护的一个java原生线程池，源码见com.alibaba.dubbo.remoting.transport.dispatcher.WrappedChannelHandler。why？ 但从netty线程模型的分析中，可以认为netty提供的那些nio工作线程主要被用于消息链路的读取、解码、编码和发送。而dubbo把业务逻辑的执行放在自身维护的线程池中是否就是为了贯彻netty的这一原则呢？ 从上面给的链接中可以注意到下面这段话： Netty是个异步高性能的NIO框架，它并不是个业务运行容器，因此它不需要也不应该提供业务容器和业务线程。合理的设计模式是Netty只负责提供和管理NIO线程，其它的业务层线程模型由用户自己集成，Netty不应该提供此类功能，只要将分层划分清楚，就会更有利于用户集成和扩展。 正如文中所说，dubbo这么做有利于分离通信层，方便的替换掉netty。至于是否还有更高深的理由，我就不清楚了，希望大牛赐教。 另外dubbo在进行数据的读取和解析上做了很多工作，体现出了开发人员的功底，由于我在这部分没有经验，勉强看得懂就已经不错了，谈不上分析，就不瞎掰了。","tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"dubbox","slug":"dubbox","permalink":"https://blog.kazaff.me/tags/dubbox/"},{"name":"netty","slug":"netty","permalink":"https://blog.kazaff.me/tags/netty/"},{"name":"nio","slug":"nio","permalink":"https://blog.kazaff.me/tags/nio/"}]},{"title":"基于Dubbo的文件上传","date":"2015-03-18T15:54:30.000Z","path":"2015/03/18/基于dubbo的文件上传/","text":"之前写过基于WebUploader+Java/Php/Nodejs的文章，今天来结合dubbo聊一下文件上传的问题。目前我们的项目中，文件上传功能是以独立服务的方式部署的，也就是说多个不同的应用可以向同一个文件上传API发起请求（可能需要跨域支持）。我个人是比较喜欢这样的部署方式，不过任何事都是有利弊的，这里就不展开讨论了。 复习完上面的内容后，我们开始今天的主题，那就是基于dubbo来完成文件上传服务的开发，这么做的动机也很单纯，就是尽可能保持统一的设计哲学和实现方案。统一的好处，从运维、监控等多方面也是非常有必要的，任何自动化的前提是足够的规范。 要想做到把之前写的java版本的文件上传功能集成到dubbo中，我们就得先了解一下dubbo的协议细节。不难发现，凡是以单一长连接提供通信的协议都不太适合大数据的传输，原因很简单，凡是单个客户端会长期占用通道传输的场景都会造成通道阻塞，你可以脑补高架上堵车的景象。 而我们这里提供文件上传服务要使用的协议是REST，也就是dubbox新增的一个方式，具体细节可以看这里和这里。其实和上面文档描述的http://协议很相似。 之前的请求流程如下图： client ---&gt; tomcat ---&gt; servlet ---&gt; spring ---&gt; springMVC ---&gt; business code dubbo化后的如下图： client ---&gt; tmocat embed ---&gt; servlet ---&gt; dubbo/spring ---&gt; resteasy ---&gt; business code 不确定这么描述是否合适，但大致就是这样了，其中springMVC和resteasy做的事儿是一样的。我之所以犹豫，是因为不确定dubbo放在哪合适~~完成我们的预期需求，我们可以按照以下几步来走： 编写Filter实现跨域请求（因为我们的前端全部都是Ajax调用）； 配置以REST协议暴露dubbo服务； 使用RESTEasy及JAX-RS规范提供的注解完成把请求参数映射成Bean； 实现文件上传的业务逻辑（其实就是把原先的springMVC版的相关代码移植到RESTEasy对应位置） 测试 剩下的任务就是实操了，我会更新原先的github项目，有兴趣的盆宇可以去关注一下。","tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"dubbox","slug":"dubbox","permalink":"https://blog.kazaff.me/tags/dubbox/"},{"name":"RESTEasy","slug":"RESTEasy","permalink":"https://blog.kazaff.me/tags/RESTEasy/"},{"name":"webuploader","slug":"webuploader","permalink":"https://blog.kazaff.me/tags/webuploader/"},{"name":"跨域","slug":"跨域","permalink":"https://blog.kazaff.me/tags/跨域/"},{"name":"JAX-RS","slug":"JAX-RS","permalink":"https://blog.kazaff.me/tags/JAX-RS/"}]},{"title":"Dubbo的缓存分析","date":"2015-03-13T15:54:30.000Z","path":"2015/03/13/dubbo的缓存实现/","text":"这次的目标是缓存，没错，绝壁常用的一个知识点，我们怎么能不了解一下它的内部实现源码呢？！ dubbo的官方描述很简洁，好的封装就是这么强大， 让你用起来丝毫不费力。我们今天就费力的看一下dubbo是如何提供cache功能的。有想直接使用的童鞋，就可以跳过下面内容直面看官方提供的简单例子。 按照SPI的要求，我们从配置文件中可以看到dubbo提供的三种缓存接口的入口： threadlocal=com.alibaba.dubbo.cache.support.threadlocal.ThreadLocalCacheFactory lru=com.alibaba.dubbo.cache.support.lru.LruCacheFactory jcache=com.alibaba.dubbo.cache.support.jcache.JCacheFactory 先来看一下dubbo提供的AbstractCacheFactory的细节： public abstract class AbstractCacheFactory implements CacheFactory { private final ConcurrentMap&lt;String, Cache&gt; caches = new ConcurrentHashMap&lt;String, Cache&gt;(); public Cache getCache(URL url) { String key = url.toFullString(); Cache cache = caches.get(key); if (cache == null) { caches.put(key, createCache(url)); cache = caches.get(key); } return cache; } protected abstract Cache createCache(URL url); } 很直观的看得出，该类完成了具体cache实现的实例化工作（注意getCache的返回类型Cache，该接口规范了不同缓存的实现），接下来我们就分三部分来具体看一下不同的缓存接口的具体实现。 ThreadLocal如果你的配置如下： &lt;dubbo:reference interface=&quot;com.foo.BarService&quot; cache=&quot;threadlocal&quot; /&gt; 那就表明你使用的是该类型的缓存，根据SPI机制，会执行下面这个工厂类： public class ThreadLocalCacheFactory extends AbstractCacheFactory { protected Cache createCache(URL url) { return new ThreadLocalCache(url); } } 注意该类继承了上面提到的AbstractCacheFactory。可以看出，真正实例化的具体缓存层实现是ThreadLocalCache类型。由于此类型是基于线程本地变量的，所以非常简单： public class ThreadLocalCache implements Cache { private final ThreadLocal&lt;Map&lt;Object, Object&gt;&gt; store; public ThreadLocalCache(URL url) { this.store = new ThreadLocal&lt;Map&lt;Object, Object&gt;&gt;() { @Override protected Map&lt;Object, Object&gt; initialValue() { return new HashMap&lt;Object, Object&gt;(); } }; } public void put(Object key, Object value) { store.get().put(key, value); } public Object get(Object key) { return store.get().get(key); } } 这里注意的是，为了遵循接口定义才需要初始化时传入url参数，但其实该类型的缓存实现是完全不需要额外参数的。 最后要叮嘱的是，该缓存应用场景为： 比如一个页面渲染，用到很多portal，每个portal都要去查用户信息，通过线程缓存，可以减少这种多余访问。 场景描述的核心内容是当前请求的上下文，可以结合dubbo的线程模型来更好的消化这一点。也许我们以后还会单独来分析这个主题。 LRU类似ThreadLocal，我们就不再重复列举对应的工厂方法了，直接看LruCache类的实现： public class LruCache implements Cache { private final Map&lt;Object, Object&gt; store; public LruCache(URL url) { final int max = url.getParameter(&quot;cache.size&quot;, 1000); //定义了缓存的容量 this.store = new LinkedHashMap&lt;Object, Object&gt;() { private static final long serialVersionUID = -3834209229668463829L; @Override protected boolean removeEldestEntry(Entry&lt;Object, Object&gt; eldest) { //jdk提供的接口，用于移除最旧条目的需求 return size() &gt; max; } }; } public void put(Object key, Object value) { synchronized (store) { //注意这里的同步条件 store.put(key, value); } } public Object get(Object key) { synchronized (store) { //注意这里的同步条件 return store.get(key); } } } 相比ThreadLocal，可以看出，该类型的缓存是跨线程的，也匹配我们常见的缓存场景。 JCache对于我这种java新手，什么是JCache，显然需要科普一下，这里给出了我找到的几篇不错的文章：官府，草根，小栗子，注解篇，中文完美篇。由于内容太多，我就不胡乱翻译了~~ 由于这部分的代码太简单，节省篇幅就不列源码了。不过我们的项目缓存是基于redis的，而我并没有找到支持JCache的redis客户端，不知道大家有没有推荐的啊~？？ 如何解析“cache”属性那么，cache层的逻辑是如何一步一步“注入”到我们的业务逻辑里呢？这还是要追溯到dubbo的过滤器上，我们知道在dubbo初始化指定protocol的时候，会使用装饰器模式把所有需要加载的过滤器封装到目标protocol上，这个细节指引我来查看ProtocolFilterWrapper类： refer() ---&gt; buildInvokerChain（） | V private static &lt;T&gt; Invoker&lt;T&gt; buildInvokerChain(final Invoker&lt;T&gt; invoker, String key, String group) { Invoker&lt;T&gt; last = invoker; List&lt;Filter&gt; filters = ExtensionLoader.getExtensionLoader(Filter.class).getActivateExtension(invoker.getUrl(), key, group); if (filters.size() &gt; 0) { for (int i = filters.size() - 1; i &gt;= 0; i --) { final Filter filter = filters.get(i); final Invoker&lt;T&gt; next = last; last = new Invoker&lt;T&gt;() { public Class&lt;T&gt; getInterface() { return invoker.getInterface(); } public URL getUrl() { return invoker.getUrl(); } public boolean isAvailable() { return invoker.isAvailable(); } public Result invoke(Invocation invocation) throws RpcException { return filter.invoke(next, invocation); } public void destroy() { invoker.destroy(); } @Override public String toString() { return invoker.toString(); } }; } } return last; } 注意ExtensionLoader.getExtensionLoader(Filter.class).getActivateExtension(invoker.getUrl(), key, group);这一行，单步调试可以得知它会返回所有需要“注入”的Filter逻辑，当然也包含我们关注的缓存：com.alibaba.dubbo.cache.filter.CacheFilter。 注意看该类声明的开头： @Activate(group = {Constants.CONSUMER, Constants.PROVIDER}, value = Constants.CACHE_KEY) 这一行是关键哟，上面提到的getActivateExtension方法就是靠这一行注解工作的。dubbo以这种设计风格完成了大多数的功能，所以对于研究dubbo源码的童鞋，一定要多多注意。 经历了这一圈下来，所有过滤器就已经注入到我们的服务当中了。 业务层如何使用cache最后再来仔细看一下com.alibaba.dubbo.cache.filter.CacheFilter类的invoke方法： public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException { if (cacheFactory != null &amp;&amp; ConfigUtils.isNotEmpty(invoker.getUrl().getMethodParameter(invocation.getMethodName(), Constants.CACHE_KEY))) { Cache cache = cacheFactory.getCache(invoker.getUrl().addParameter(Constants.METHOD_KEY, invocation.getMethodName())); if (cache != null) { String key = StringUtils.toArgumentString(invocation.getArguments()); if (cache != null &amp;&amp; key != null) { Object value = cache.get(key); if (value != null) { return new RpcResult(value); } Result result = invoker.invoke(invocation); if (! result.hasException()) { cache.put(key, result.getValue()); } return result; } } } return invoker.invoke(invocation); } 可以看出，这里根据不同的配置会初始化并使用不同的缓存实现，好了，关于缓存的分析就到此为止。","tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"dubbox","slug":"dubbox","permalink":"https://blog.kazaff.me/tags/dubbox/"},{"name":"cache","slug":"cache","permalink":"https://blog.kazaff.me/tags/cache/"},{"name":"threadlocal","slug":"threadlocal","permalink":"https://blog.kazaff.me/tags/threadlocal/"},{"name":"JCache","slug":"JCache","permalink":"https://blog.kazaff.me/tags/JCache/"},{"name":"Filter","slug":"Filter","permalink":"https://blog.kazaff.me/tags/Filter/"}]},{"title":"dubbox新增的REST协议分析","date":"2015-03-12T15:54:30.000Z","path":"2015/03/12/dubbox新增的REST协议分析/","text":"我们今天来看一下dubbox多出来的那个“x”都包含什么，当然一定会存在遗落，毕竟我是从一个第三方使用者的角度来总结的。之前也写了几篇关于dubbo的文章，虽然都加了dubbox的tag，但这一篇才是真正的只与dubbox相关的哟~先从业务应用的角度来看，其实dangdang给dubbo嫁接的rest协议是基于RESTEasy的，并且增加了序列化的方式，还有额外的servlet容器。前两个新增是开发者需要非常熟悉的，毕竟是要常打交道的。servlet容器更多的是和运维人员有交集。 RESTEasy不熟悉的童鞋（和我一样），不妨先看一些相关的技术文档，我这里粗糙的翻译了一篇，尽请笑纳。随着你对RESTEasy的熟悉，你会发现dubbox中很多的新增特性都是RESTEasy带来的，遵循JAX-RS规范。当然dangdang的大牛们也做了很多工作，后面我们一点一点分析。 这里还要再次说一个关于RESTEasy的小问题，这也是最终有这篇文章的原因。 序列化部分，dangdang为dubbox新增了fst和kryo两种方式，使用方式和dubbo原有的保持一致，这一部分几乎是对业务透明的，之所以说是几乎，是因为不少大牛提到序列化这部分还是存在不少坑，这就需要我们在开发时进行比较全面的测试，由于我们目前还没正式投入使用，所以暂时就说这么多。 关于新增的servlet容器，目前知道的就是tomcat-embed，另外如果使用REST协议，还可以使用Tjws容器，不过RESEasy官方推荐这个玩意儿还是建议在测试环境中使用。当然，我们也可以选择使用server=&quot;servlet&quot;来使用外部的servlet容器，例如和其他应用使用同一个tomcat。但是文档上也叮嘱，即便是打算使用外部tomcat，也尽可能不要和其他应用混合部署，而应该用单独的实例。 其实上面说的这些，在dubbox提供的指南中都有详细的说明，多说只是重复。 源码解析我们接下来看一下新增的RestProtocol的具体实现。基于之前的分析结果，我们可以直接定位到核心的代码块（doExport）： protected &lt;T&gt; Runnable doExport(T impl, Class&lt;T&gt; type, URL url) throws RpcException { String addr = url.getIp() + &quot;:&quot; + url.getPort(); //这么处理其实是为了配合dubbo的延迟暴露机制，延迟暴露的原理是创建新线程，所以ServiceImplHolder靠ThreadLocal来记录对应的类信息，而不是靠公共变量，避免竞争问题 Class implClass = ServiceImplHolder.getInstance().popServiceImpl().getClass(); //根据url中的地址创建server容器，相同地址的服务使用相同的容器 RestServer server = servers.get(addr); if (server == null) { server = serverFactory.createServer(url.getParameter(Constants.SERVER_KEY, &quot;jetty&quot;)); //默认使用jetty，当当推荐使用tomcat server.start(url); servers.put(addr, server); } //查看implClass是否包含jax-rs注解 final Class resourceDef = GetRestful.getRootResourceClass(implClass) != null ? implClass : type; server.deploy(resourceDef, impl, getContextPath(url)); //在server中部署指定服务 final RestServer s = server; //注意这样的写法，java要求线程中只能使用外部final变量 return new Runnable() { public void run() { // TODO due to dubbo&apos;s current architecture, // it will be called from registry protocol in the shutdown process and won&apos;t appear in logs s.undeploy(resourceDef); } }; } 该方法是继承自dubbo原有的AbstractProxyProtocol类： 按照dubbo的逻辑，注意该方法最终返回的是一个可异步执行的callback类，完成的逻辑也很简单，就是从对应的server中把指定资源服务给“卸载”掉。 我们再来关注一下关于server的逻辑，这部分其实很简单，根据你的配置直接创建对应的server： public class RestServerFactory { private HttpBinder httpBinder; //提供http服务的容器 //该方法为SPI注入时调用，会注入一个httpBinder自适应扩展实例 public void setHttpBinder(HttpBinder httpBinder) { this.httpBinder = httpBinder; } public RestServer createServer(String name) { // TODO move names to Constants if (&quot;servlet&quot;.equalsIgnoreCase(name) || &quot;jetty&quot;.equalsIgnoreCase(name) || &quot;tomcat&quot;.equalsIgnoreCase(name)) { return new DubboHttpServer(httpBinder); // } else if (&quot;tjws&quot;.equalsIgnoreCase(name)) { // return new TjwsServer(); } else if (&quot;netty&quot;.equalsIgnoreCase(name)) { return new NettyServer(); } else if (&quot;sunhttp&quot;.equalsIgnoreCase(name)) { return new SunHttpServer(); } else { throw new IllegalArgumentException(&quot;Unrecognized server name: &quot; + name); } } } DubboHttpServer类主要贡酒是完成了RESTEasy和servlet容器的绑定，上面代码的httpBinder代表的就是你要使用的容器，例如tomcat。这部分逻辑你需要结合RESTEasy的初始化步骤来理解，我就不多说了。 tomcat-embed的使用方式，也非常的直观，结合官方提供的例子即可，也不多说了。 总之，这篇文章还是很水的，不管怎么说，这就是我这几天来的工作内容。更多内容，敬请期待~~","tags":[{"name":"rest","slug":"rest","permalink":"https://blog.kazaff.me/tags/rest/"},{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"dubbox","slug":"dubbox","permalink":"https://blog.kazaff.me/tags/dubbox/"},{"name":"协议","slug":"协议","permalink":"https://blog.kazaff.me/tags/协议/"},{"name":"RESTEasy","slug":"RESTEasy","permalink":"https://blog.kazaff.me/tags/RESTEasy/"},{"name":"tomcat-embed","slug":"tomcat-embed","permalink":"https://blog.kazaff.me/tags/tomcat-embed/"}]},{"title":"RESTEasy指南","date":"2015-03-10T15:54:30.000Z","path":"2015/03/10/看看RESTEasy/","text":"由于dubbox的这个问题，我决定先了解一下这个框架。为了快速认识RESTEasy，我决定翻译一篇看似还不错的文章。下面就开始，不过我英文能力确实不如我的中文能力，请大家见谅。 RESTEasy是一个实现了JAX-RS规范的轻量实现，该规范是针对基于http协议的RESTful Web Service而提供标准的JAVA API定义。这篇指南我们来手把手演示如何使用Resteasy来创建一些简单的RESTful Web Service。 #安装RESTEasy根据你所使用的不同版本的JBoss，安装RESTEasy需要不同的步骤。 ##JBoss 6/7 在此版本下安装RESTEasy你不需要下载任何包，RESTEasy类已经内嵌在server中。应用服务器会自动发现导出成RESTful的服务资源。 更多关于RESTEasy和JBoss 7的例子可以看这里。 你唯一需要做的只是在web.xml中插入正确的命名空间： &lt;web-app version=&quot;3.0&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot;&gt; &lt;/web-app&gt; ##旧JBoss版本 如果你工作在旧的JBoss版本下，安装RESTEasy你只需要做下面几步： ###1.下载RESTEasy 你需要先从这里下载最新的RESTEasy稳定版。 ###2.把RESTEasy类加入你的web应用 ###3.定义listenser和bootstrap 在web.xml中添加下面的配置： &lt;?xml version=&quot;1.0&quot;?&gt; &lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot;&gt; &lt;web-app&gt; &lt;display-name&gt;RestEasy sample Web Application&lt;/display-name&gt; &lt;listener&gt; &lt;listener-class&gt; org.jboss.resteasy.plugins.server.servlet.ResteasyBootstrap &lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;Resteasy&lt;/servlet-name&gt; &lt;servlet-class&gt; org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher &lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;javax.ws.rs.Application&lt;/param-name&gt; &lt;param-value&gt;sample.HelloWorldApplication&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;Resteasy&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;/web-app&gt; org.jboss.resteasy.plugins.server.servlet.ResteasyBootstrap类是一个ServletContextListener，它负责配置实例化 ResteasyProviderFactory 和 Registry。 然后，你还需要设置 javax.ws.rs.core.Application 参数，赋值一个被用于提供给你的应用去作为所有JAX-RS的根资源和提供者的入口单例实现（工厂模式）。不过在JBoss 6-M4或更高版本的环境下可以省略这个设置，因为在你部署应用时应用服务器已经帮你自动处理过了。 最终你还需要指定一个url模式，用于RESTEasy来识别对应服务，如上面的配置例子，/*表示所有资源都会被RESTEasy servlet匹配。 ###4.创建一个单例类 现在，我们来创建一个单例类，该类就是前面提到的 javax.ws.rs.core.Application 所需要的实现： package sample; import java.util.HashSet; import java.util.Set; import javax.ws.rs.core.Application; import sample.HelloWorld; public class HelloWorldApplication extends Application { private Set&lt;Object&gt; singletons = new HashSet(); private Set&lt;Class&lt;?&gt;&gt; empty = new HashSet(); public HelloWorldApplication() { // 把你的rest资源添加到这个属性中 this.singletons.add(new HelloWorld()); } public Set&lt;Class&lt;?&gt;&gt; getClasses() { return this.empty; } public Set&lt;Object&gt; getSingletons() { return this.singletons; } } 在启动时上面声明的这个类会被用于初始化你想提供的RESTful服务，在这个例子中，我们加了一个 HelloWorld 服务。 ####资源的自动发现 如果你更希望让RESTEasy自动去发现你的资源，而不是像刚才那样手动添加一个单例bean来完成这个目的，你可以在web.xml中添加下面的配置： &lt;context-param&gt; &lt;param-name&gt;resteasy.scan&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/context-param&gt; &lt;context-param&gt; &lt;param-name&gt;resteasy.servlet.mapping.prefix&lt;/param-name&gt; &lt;param-value&gt;/&lt;/param-value&gt; &lt;/context-param&gt; #创建你的第一个RESTful服务 基于REST风格的架构，一切都是资源。 资源可以通过http提供的通用接口（POST,GET,PUT,DELETE）来操作。所有资源都应该支持这些通用接口，并且资源都应该有一个全局唯一的ID（通常指的就是URL）。 在我们的第一个例子里，我们创建一个RESTful服务，每当它被以get方式访问请求时，会返回一个字符串： package sample; import javax.ws.rs.*; @Path(&quot;tutorial&quot;) public class HelloWorld { @GET @Path(&quot;helloworld&quot;) public String helloworld() { return &quot;Hello World!&quot;; } } 在类上添加的那个@Path注解的作用是指定了该资源的统一URL前缀，而方法上的@Path注解则是定义了更具体的某一个资源操作。（译者：原文中的“action”违背了RESTful规范，资源url应该以名词为主） 在这个例子中，如果你想调用我们的helloworld服务，你需要访问下面这个URL（我们假设应用打包为：RestEasy.war）: 访问后在你的浏览器会显示“Hello World!”字符串。现在我们添加一个参数： @GET @Path(&quot;helloname/{name}&quot;) public String hello(@PathParam(&quot;name&quot;) final String name) { return &quot;Hello &quot; +name; } 现在，我们定义了一个“helloname”服务，它接受一个{name}参数。服务会从请求url中获取对应参数并返回，如下： http://localhost:8080/RestEasy/tutorial/helloname/francesco 将返回： Hello Francesco #使你的RESTEasy服务返回XML 根据JAX-RS规范要求，RESTEasy支持多个JAXB注解，并提供JAXB Providers。 RESTEasy会根据资源的参数类型或返回类型来选择不同的provider。当方法返回的对象的类声明加上了JAXB注解（@XmlRootEntity或@XmlType）时，RESTEasy会选择注解指定的JAXB Provider。 举个例子，下面将返回xml： @GET @Path(&quot;item&quot;) @Produces({&quot;application/xml&quot;}) public Item getItem() { Item item = new Item(&quot;computer&quot;,2500); return item; } 现在把这个方法加到你的 HelloWorld 服务中，然后调用这个服务（http://localhost:8080/RestEasy/tutorial/item），将返回下面这个xml： &lt;item&gt; &lt;description&gt;computer&lt;/description&gt; &lt;price&gt;2500&lt;/price&gt; &lt;/item&gt; 如果你需要返回一个数组，你可以简单的这么写： @GET @Path(&quot;itemArray&quot;) @Produces({&quot;application/xml&quot;}) public Item[] getItem() { Item item[] = new Item[2]; item[0] = new Item(&quot;computer&quot;,2500); item[1] = new Item(&quot;chair&quot;,100); return item; } 调用后将会返回： &lt;collection&gt; &lt;item&gt; &lt;description&gt;computer&lt;/description&gt; &lt;price&gt;2500&lt;/price&gt; &lt;/item&gt; &lt;item&gt; &lt;description&gt;computer&lt;/description&gt; &lt;price&gt;2500&lt;/price&gt; &lt;/item&gt; &lt;/collection&gt; 有时候，你需要使用标准的java集合，这时候你就不得不提供一个包装类： @GET @Path(&quot;itemList&quot;) @Produces({&quot;application/xml&quot;}) public ItemList getCollItems() { ArrayList list = new ArrayList(); Item item1 = new Item(&quot;computer&quot;,2500); Item item2 = new Item(&quot;chair&quot;,100); Item item3 = new Item(&quot;table&quot;,200); list.add(item1); list.add(item2); list.add(item3); return new ItemList(list); } 我们来看一下 ItemList 包装类的定义： package sample; import javax.xml.bind.annotation.XmlRootElement; import javax.xml.bind.annotation.XmlElement; import java.util.List; @XmlRootElement(name=&quot;listing&quot;) public class ItemList { private List&lt;Item&gt; items; public ItemList(){} public ItemList(List&lt;Item&gt; items) { this.items = items; } @XmlElement(name=&quot;items&quot;) public List&lt;Item&gt; getItems() { return items; } } 返回的xml如下： &lt;listing&gt; &lt;items&gt; &lt;description&gt;computer&lt;/description&gt; &lt;price&gt;2500&lt;/price&gt; &lt;/items&gt; &lt;items&gt; &lt;description&gt;chair&lt;/description&gt; &lt;price&gt;100&lt;/price&gt; &lt;/items&gt; &lt;items&gt; &lt;description&gt;table&lt;/description&gt; &lt;price&gt;200&lt;/price&gt; &lt;/items&gt; &lt;/listing&gt; #使用JSON providers 根据文档，RESTEasy支持多个不同的providers去创建xml，而JSON是一个非常有用的返回集合的结构，它支持lists，sets，arrays，如下： @GET @Path(&quot;items&quot;) @Produces(&quot;application/json&quot;) @Mapped public ItemList getJSONItems() { ArrayList list = new ArrayList(); Item item1 = new Item(&quot;computer&quot;,2500); Item item2 = new Item(&quot;chair&quot;,100); Item item3 = new Item(&quot;table&quot;,200); list.add(item1); list.add(item2); list.add(item3); return new ItemList(list); } 返回的数据将依照下面的格式： {xtypo_code} {&quot;listing&quot;:{&quot;items&quot;:[{&quot;description&quot;:{&quot;$&quot;:&quot;computer&quot;},&quot;price&quot;:{&quot;$&quot;:&quot;2500&quot;}},{&quot;description&quot;:{&quot;$&quot;:&quot;chair&quot;},&quot;price&quot;:{&quot;$&quot;:&quot;100&quot;}},{&quot;description&quot;:{&quot;$&quot;:&quot;table&quot;},&quot;price&quot;:{&quot;$&quot;:&quot;200&quot;}}]}} {/xtypo_code} 你可以使用下面的ajax客户端来调用这个json服务： &lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt; &lt;html&gt; &lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot;&gt; &lt;script type=&quot;text/javascript&quot;&gt; function createXHR() { var request = false; try { request = new ActiveXObject(&apos;Msxml2.XMLHTTP&apos;); } catch (err2) { try { request = new ActiveXObject(&apos;Microsoft.XMLHTTP&apos;); } catch (err3) { try { request = new XMLHttpRequest(); } catch (err1) { request = false; } } } return request; } function loadJSON(fname) { var xhr = createXHR(); xhr.open(&quot;GET&quot;, fname, true); xhr.onreadystatechange = function() { if (xhr.readyState == 4) { if (xhr.status != 404) { var data = eval(&quot;(&quot; + xhr.responseText + &quot;)&quot;); document.getElementById(&quot;zone&quot;).innerHTML = &quot;&lt;h2&gt;Items:&lt;/h2&gt;&quot;; for (i = 0; i &lt; 3; i++) { document.getElementById(&quot;zone&quot;).innerHTML += data.listing.items[i].description + &apos;, price &lt;i&gt;&apos; + data.listing.items[i].price + &quot;&lt;/i&gt;&lt;br/&gt;&quot;; } } else { document.getElementById(&quot;zone&quot;).innerHTML = fname + &quot; not found&quot;; } } } xhr.send(null); } &lt;/script&gt; &lt;title&gt;Ajax Get JSON Demo&lt;/title&gt; &lt;/head&gt; &lt;body bgcolor=&quot;#FFFFFF&quot;&gt; &lt;p&gt; &lt;font size=&quot;+3&quot;&gt;Ajax JSON/JAXB Demo&lt;/font&gt; &lt;/p&gt; &lt;hr&gt; &lt;FORM name=&quot;ajax&quot; method=&quot;POST&quot; action=&quot;&quot;&gt; &lt;p&gt; &lt;INPUT type=&quot;BUTTON&quot; value=&quot; Click to load the JSON file &quot; ONCLICK=&quot;loadJSON(&apos;resteasy/tutorial/items&apos;)&quot;&gt; &lt;/p&gt; &lt;/FORM&gt; &lt;div id=&quot;zone&quot;&gt;&lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 注意上面javascript部分，它负责从json中拿到数据并放到“zone”DIV中。 document.getElementById(&quot;zone&quot;).innerHTML += data.listing.items[i].description + &apos;, price &lt;i&gt;&apos; + data.listing.items[i].price + &quot;&lt;/i&gt;&lt;br/&gt;&quot;; 可以阅读这个指南的第二部分：RESTEasy web parameters handling。","tags":[{"name":"json","slug":"json","permalink":"https://blog.kazaff.me/tags/json/"},{"name":"rest","slug":"rest","permalink":"https://blog.kazaff.me/tags/rest/"},{"name":"dubbox","slug":"dubbox","permalink":"https://blog.kazaff.me/tags/dubbox/"},{"name":"xml","slug":"xml","permalink":"https://blog.kazaff.me/tags/xml/"}]},{"title":"如何把项目SOA化系列之三：架构设计","date":"2015-03-05T15:54:30.000Z","path":"2015/03/05/如何把项目soa化系列之三：架构设计/","text":"已经到了这一步，进度还算比较快。只是悲剧的是，当我整理完项目调用关系后才“恍然大悟”，妈蛋，年前某个时间我已经做过一次这样的工作了。而且更夸张的是，那一次做的比现在还彻底，哇哈哈哈哈，我这种记性，也是醉了。值得庆幸的是，两次流程的大致方向还是一致的（不然就麻烦了，精神分裂~）。言归正传，本来计划接着上一篇，直接开始写服务的详细设计呢，但是感觉还是缺少什么，应该就是对平台的整体设计规划吧。先上一张图： 图解： 客户端和运营平台之间的通信统一基于Http的REST API，这类API尽可能保证完整业务逻辑，以粗颗粒度提供（意味着复用性较差） Rest API内部会负责业务编排，以dubbo提供的高性能协议与运营平台中的数据应用服务进行通信 数据应用服务之间允许相互依赖，同样基于dubbo 数据应用服务之间的依赖应该尽可能的少，应该尽可能放在Rest API层来做处理这种依赖 后方的各个数据系统之间不存在直接依赖，所有依赖都需要通过运营平台的数据应用服务来提供，通信同样基于dubbo 计费和权限认证等逻辑尽可能的异步化或并行（后面详细讨论） 这是年前设计出的一张图，现在来看依然是很满意的，唯一值得考虑的就是“微服务”概念，我一直比较赞同这个概念，尤其是在做SOA筹备的时候更是如此。上图中，数据应用服务是比较适合以微服务方式设计的，不过还需要结合dubbo更仔细的琢磨琢磨。 接下来再来说一下关于上图中一些特殊的服务：计费，认证授权等。我提到了异步和并行，如图： 图解： 虚线表示异步调用，实线表示并行调用 计费和余额查询分离，会产生一个不一致的时间窗口，这需要结合业务来衡量可行性 余额查询，权限认证和获取数据并行调用，最终会在API层合并后给予最终裁决 图中省略了SLB的相关部分 最后，我们讨论一下系统中另外一种场景的通信：消息通知。稍微复杂一些的系统，都会对消息队列有强的需求。这是因为，系统之间的通信在排除了人类参与的情况下，并不需要非常之高的即时响应指标，相反，对消息的最终可达性要求确实100%的。这也是消息中间件的战场，各种各样的开源产品可供我们选择。 我们跳出具体的消息中间件，从业务层面来梳理一下我们目前的平台中要如何设计这部分场景，如下图： 图解： 没什么要解释的啦 目前为止，我们已经从整体上描述了一下要做的事情，不管你们看完后有啥感受，我自己是清晰了不少，哇哈哈。这一篇谈的有点虚，都是围绕着实际业务来谈概念的，应该就是所谓的概念架构设计了吧~~ 下一篇开始，就要深入到代码微观视角了，尽请期待！","tags":[{"name":"rest","slug":"rest","permalink":"https://blog.kazaff.me/tags/rest/"},{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"soa","slug":"soa","permalink":"https://blog.kazaff.me/tags/soa/"},{"name":"队列","slug":"队列","permalink":"https://blog.kazaff.me/tags/队列/"}]},{"title":"如何把项目SOA化系列之二：业务梳理","date":"2015-03-02T15:54:30.000Z","path":"2015/03/02/如何把项目soa化系列之二：业务梳理和服务发现/","text":"好吧，按照计划，我们接下来要做的就是根据项目现有的功能，依照相关的标准抽离出相关的服务。 把这个任务根据我们公司的实际情况，还可以细分，不过不确定是否对大家有参考价值，但是为了记录下每一步重构流程，我还是坚持要写出来的，所以如果你觉得不耐烦，请移步到该系列其它文章，谢谢合作。 服务发现首先，小弟花了三天的时间，把公司相关的项目文档和代码粗略的滤了一遍，方法简单粗暴，谈不上合理，但个人感觉还是有点用的，根据之前提到的基准，整理出了个文档，如下： 其中每个维度的满分为5（不建议选用过大的分值范围），粗略的解释一下每个分值的含义： 重要度： 1分：作用程度几乎可以不考虑 2分：出现问题不影响业务正常工作 3分：普通，出现问题允许在1天的时间内给予修复即可 4分：重要，不允许失效 5分：核心，不允许失效，且要保证高的执行效率 复杂度： 1分：简单的直接操作一张表，或表达为入职一个月的普通应届生就可完成的任务（这么说可能容易被拍） 2分：需要同时操作多张表，存在一对多关系的数据组合 3分：包含较多的业务逻辑，但全部操作都在同一个上下文中完成（单进程单线程） 4分：产生RPC调用 5分：使用了多线程 复用度： 1分：只有自己使用，这里的“自己”表示自身所在的单个系统中的个别操作使用 2分：自身所在的单个系统中不超过5个操作使用 3分：自身所在单个系统中超过5个操作使用 4分：2个以上的系统都使用 5分：可预见的所有系统都会使用 频度： 1分：偶尔会用，或叫几乎不用（其实大多数操作都属于这一类，又一次验证了二八定理） 2分：普通操作，但并非日常必须 3分：通用操作，正常用户日常会经常使用的 4分：热门操作，所有用户日常会经常使用的 5分：必经操作，所有用户一定会频繁使用的 类型： 原子：基础服务，可以理解为自包含，有明确边界的功能 复合：组合服务，需要由1个以上的原子服务组合出来的功能 以上打分并不是非常的严谨，只是争取做到了有个粗略的标准来区分服务的目的。 服务关系梳理粗略的数了数，我们公司的在使用项目大概也有十四个之多，上一篇文章介绍过，由于初期设计时就考虑到了系统间的依赖，所以单独在某个项目自身上去发现服务，是不准确的，很可能A系统中的一些功能是依赖B系统的，这样上述的列表中就会有大量的重复内容，所以我们下一步需要做的，就是梳理服务之间的关系。 既然要研究关系，视角就不能只局限在某个系统，而是应该上升到平台上，自嘲的说，我们团队现有的项目用”平台”二字有点小题大做，不过意思到了就行了，不要在意细节，咱们聊的是方法论。 其实在服务发现时，根据基准，就可以第一轮筛选出一部分较为合适的服务集合了，我们只需要把选中的服务的关系整理出来即可。依然采用图的方式，如下： PS:哥知道画的很丑，但确实没有美化的能力啊，求推荐工具。 这样下来，目标就明确很多了。但，这还不够，接下来就是要深入到每一个服务中，根据实际情况来设计该服务的实现细节。我们放在下一篇来继续。","tags":[{"name":"soa","slug":"soa","permalink":"https://blog.kazaff.me/tags/soa/"},{"name":"服务","slug":"服务","permalink":"https://blog.kazaff.me/tags/服务/"}]},{"title":"如何把项目SOA化系列之一：计划","date":"2015-02-25T15:54:30.000Z","path":"2015/02/25/如何把项目soa化系列之一：计划/","text":"过年回来了，也休息够了，去年年会上该吹的牛逼也都吹的很到位了，年假回来，是该着手兑现的时候了。废话不多说，走起！ 背景公司的项目，在设计之初其实就一定程度的引入了服务的概念，当时的出发点更多的是因为：团队协同和项目进度分期，不过还有一个重要的原因，是因为要完成的目标有点太大太虚，很多细节不可能一次性全部想到位，所以就采用了把大系统拆成一个一个的小系统来完成，这样来做，系统之间自然就产生了通信的必要性。 期初为了快速搭建原型和团队成员的技能分布，选择使用php作为开发语言。确实也在初期达到了快速开发的目的，拿到了一血后也踏上了小系统逐渐变大的不归路。系统和系统之间的通信使用的就是简单的http+cache，简单粗暴能运行，是当时的重要哲学。 工期的紧张，人员流动，再加上员工心态和情绪的波动等原因，映射在项目上就会成为功能缺失，代码质量下降，依赖混乱，抽象不足等。除此之外，开发人员毫无节制的重复发明服务，使前面说到的问题进一步放大，一度成为毫无解决希望的技术债务。 由于公司业务变更，团队大部分成员投入到了一个新项目的开发上，这给这个项目和我都留出了一个非常完美的空档期，在获得了领导的授权后，我调整状态，踏上了系统重构的大道，也就产生了该系列文章。 其实早在13年后半年，项目组开会时就已经提到了关于服务化的设想，无奈当时并没有如今这样好的条件，也就不了了之。经历了14年的php转java，项目soa的条件逐渐成熟，是时候大展身手了。 计划依照前辈大神们的心得体会，需要在项目中首先确定一些有价值的业务进行SOA重构，什么叫有价值的？就是那些项目的核心模块么？ 其实不全对，为了保证开门红，建议从项目中选择一些复杂度相对较低，重要度适中的一些模块来下刀，避免造成骑虎难下的尴尬局面，而且也可以更快的积累经验和信心，当然，也不要选择太简单太无所谓的模块来搞，这样既无法积累经验，也不利于团队推广。 那么我们该怎么选呢？我考虑了一下，决定以下面三个维度来作为选择的基准： 重要度：■ ■ □ □ □ 复杂度：■ ■ □ □ □ 频度：■ ■ ■ □ □ 确定了这个标准后，我又进一步给自己安排了一个重构流程： 从需求书中寻找服务，并依照基准分类 梳理服务之间的关系，确定哪些是原子服务，哪些是复合服务 设计服务接口，以及协议类型（RPC？REST？） 代码实现 技术选型了解soa的童鞋，自然会问：你基于哪些技术来实现SOA？其实SOA的技术骨架也无非就是：服务发现，服务治理，数据通信等几大块，商业的解决方案中还会提供强大的服务总线，工作流等功能（这些仅代表我个人理解，不喜勿拍）。 相信关注我博客的朋友肯定知道我要使用的框架是什么，没错，就是dubbox，其细节请参看我博客中的相关文章。 指标我们引入dubbox的目标，不仅是改善系统间依赖的混乱局面，还要标准化一些开发流程，提升开发效率，提升系统的可用性，性能，可伸缩，扩展性，重用性等。 下面我们具体来定一些指标，由于缺乏经验和相关数据，所以列出的指标会过于笼统，不过有生于无嘛。 标准化服务开发流程，从流程视角杜绝服务的重复，以及确保服务质量； 提高服务的可用性，确保每个服务都有冗余，并提供针对应用的监控工具； 提高服务响应时间，根据实际的复杂度来度量，尽可能保证服务响应时间在100ms–500ms之间（不启用缓存的情况下）； 提升代码的重用性，并尽可能降低代码的耦合度； 提升代码的质量，提供更完善的代码检测工具和流程。 好了，日后根据工作的进行，再逐步细化指标，总之大的方向就这些。","tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"soa","slug":"soa","permalink":"https://blog.kazaff.me/tags/soa/"}]},{"title":"重构团队的开发工作流","date":"2015-02-14T15:54:30.000Z","path":"2015/02/14/重构团队的开发工作流/","text":"靠近年根儿，手上的工作其实还有很多，不过还是要总结一下，展望一下，这就是人类。 在公司的年会上，我代表开发部做了一个简短的工作汇报，在PPT里写了很多14年的业绩和不足，也包含了15年的计划和方案。与往年不同，今年不仅是公司发展的一年，也是团队壮大的一年，也是个人成长最快的一年。其实自己确实有思考过很多，也收获了很多，找到了团队的痛点，也拿到了公司的授权，所以在15年初，要落实一些新的计划。想要强调的是，这些改革并不是我个人的主观意愿，并非我个人想为而为之的，而是包含了开发部各个层面的景愿的。我把我ppt中相关的一页截了个图，给大家展示一下： 你可能会诧异，这些都是围绕这工具的，是的没错，我们的团队以往一直都是靠人海战术来完成这些工作的，尽管团队也就才11个人而已。也许身在大公司的你觉得不可思议的是，没有这些工具，那开发人员要怎么活？项目质量怎么可能有保证？产品怎么可能如期交付？ 这些问题其实在任何一家小公司，创业公司都很常见，像我们这种小城市的创业小团队就更习以为常了，我这么说并非妄自菲薄，纯粹是尊重事实而已！刚到公司，刚走上这个岗位的时候，也曾看过一些敏捷开发，摩拳擦掌想要试试，甚至激进的和大领导申请SOHO办公。 后来在InfoQ上看了一些关于团队协作的分享后，确实得到了很多启发，也迫使自己真的第一次思考了一下所处的真实环境。尊重现实吧骚年，妄想那些远在天边的不切实际，最终只是浪费青春。 废话不多说，我们回过头来继续聊主题，如何完成幻灯片上提到的那些点。 代码质量深入思考以后，我把代码质量分为两个方面： 业务 代码本身 判断程序员写的代码的质量高低，以我们现在的能力和认知，从上述两个方面最容易做到。业务指的是程序员完成项目需求的程度，是否存在功能缺失，是否存在业务理解错误等方面，而代码本身则指的是程序员提交的代码是否满足团队的代码规范等相关要求。 可以看出来，前者需要有经验的人来检验团队中的其他参与者，有点像师傅带徒弟的模式，在我们的团队（我相信很多公司都如此），师傅带徒弟模式反而是一种非常高效的培养骨干的方式。 后者则可以交给一些现成的工具来完成，下面我就简单聊一下我脑子里的一些计划。首先要引入两个开源工具： GitLab Sonar 前者是类似github那样的一个基于git的协同开发平台，提供了功能完善的web界面。之所以选择它，而不是直接使用git，是因为gtilab提供了许多辅助文档化的功能，比方说可以让问题和代码更好的关联在一起，具体做法，可以参见gitlab flow。 后者提供了强大的代码质量监督功能，不仅支持多种语言，而且也有很好的统计以及社交界面。不过也有些许的不如意，可能是用的不熟的原因吧，具体问题可以看这里。 这两个工具都支持与持续集成套件协作，这就为日后的工作提供了衔接。 在借鉴了一些成熟的work flow后，结合我们团队目前的真实情况，我草拟了一个工作流，如下图： 有兴趣的童鞋可以留言一起讨论哇~ 只有解决了这些问题，才可以为PPT上后面两项最好工具基础。 自动化测试还在我写垃圾网站的那些岁月了，我就已经对持续集成这个概念热血沸腾了，不过那个时候更多的只是被其逼格所吸引。随着团队的壮大，项目的成长，慢慢觉得持续集成不仅仅是搞个工具，定个流程那么简单地问题。这玩意儿是需要很多理论支撑的，需要团队成员有相当的职业素养和扎实的技能。甚至也需要其他部门的大力配合，毕竟是一件有得有失的事儿。 我们团队目前的痛点集中在测试环节上，他们已经被手工测试作业折腾的苦不堪言了，更不要说每次修改完bug后的回归测试了，那感觉简直就像是被轮奸，特酸爽。 当然，也不是说自动化就意味着能够把测试完全交给机器来做，凭借公司现在的实力也不可能像模像样的成立一个专门的测试部门，而且我比较认可一个观点：己所不欲勿施于人。如果只是因为程序员不喜欢做测试，而把测试工作交给别的部门来做，那问题依然存在，其他部门也会恶心于这份工作的枯燥乏味。 那么如何能够增加测试的趣味性呢？这个恐怕真的很难做到，不过倒是可以让测试变得不再那么枯燥，引入测试驱动是一个很好的方法，不过这其实并不是减轻了工作量，从某种角度来看反而是加大了代码量，毕竟测试代码也是要交给程序员来完成的。 优势在于回归测试，如果上升到理论的话，程序员还会因为这种思维模式而在编程思想上得到升华。omg~~ 自动化测试中包括的关键词有：单元测试，代码覆盖率等，早在以前我也花了一些时间去了解这些玩意儿，不过一直没有投入使用。15年要迫使自己带头推行测试驱动开发。 测试又要分为：前端，后端。前端又分为：交互，兼容，逻辑。当然，这种分类是源于我个人的粗浅理解。至于使用什么具体的工具来搭建自动化测试平台和工作流程，目前无解。 目前目标锁定在：Junit，Jenkins，Mockito等框架。 哇哈哈哈哈哈哈哈哈~实践后再来补充吧。 运维监控这里想说的主要还是监控预警，至于日志分析什么的，并不在这个主题下。我也不曾做过运维相关的职位，也没有太多使用云平台的经验，在这个主题下，我只能说自己的认知很片面，停留在架设一套开源运维监控系统来搞定，目前目标锁定在：Nagios。 至于应用的监控，可能会尝试花一些时间看一下：kafka，flume，kibana等工具。 好啦，这篇算是个伏笔吧，等在上面三个方向上尝试过以后再来补充。","tags":[{"name":"git","slug":"git","permalink":"https://blog.kazaff.me/tags/git/"},{"name":"gitlab","slug":"gitlab","permalink":"https://blog.kazaff.me/tags/gitlab/"},{"name":"code review","slug":"code-review","permalink":"https://blog.kazaff.me/tags/code-review/"},{"name":"sonar","slug":"sonar","permalink":"https://blog.kazaff.me/tags/sonar/"},{"name":"运维","slug":"运维","permalink":"https://blog.kazaff.me/tags/运维/"}]},{"title":"Dubbo的编解码，序列化和通信","date":"2015-02-11T15:54:30.000Z","path":"2015/02/11/dubbo的编解码，序列化和通信/","text":"dubbo的调研已经快完结了（按照我自己拟定的计划），计划内剩下的内容就只有： 序列化 编解码 通信实现 打算写在一篇里，年前彻底搞定dubbo[x]的调研，过完年来了就要投入使用了，好紧张哇~~哟呵呵呵呵！其实前两块的内容并没有啥好讲的，毕竟咱目的是了解源码来辅佐如何使用，而非像当当网的团队那样做dubbo的升级开发。 按照源码的阅读习惯，我们按照上面列表的逆序来一个一个的分析。废话不多说，走着~ 通信实现我们主要基于dubbo推荐默认使用的通信框架：netty，来了解一下dubbo是如何完成两端通信的。我们直接从DubboProtocol类开始看起： export() --&gt; openServer() --&gt; createServer() | +--&gt; server = Exchangers.bind(url, requestHandler); //创建服务 dubbo从要暴漏的服务的URL中取得相关的配置（host，port等）进行服务端server的创建，并且保证相同的配置（host+port）下只会开启一个server，这和netty提供的模型有关（NIO），这个我们后面再说。 我们先来继续看Exchangers的相关部分， ...... public static ExchangeServer bind(URL url, ExchangeHandler handler) throws RemotingException { if (url == null) { throw new IllegalArgumentException(&quot;url == null&quot;); } if (handler == null) { throw new IllegalArgumentException(&quot;handler == null&quot;); } url = url.addParameterIfAbsent(Constants.CODEC_KEY, &quot;exchange&quot;); //这里尝试配置了编解码的方式 return getExchanger(url).bind(url, handler); } ...... public static Exchanger getExchanger(URL url) { String type = url.getParameter(Constants.EXCHANGER_KEY, Constants.DEFAULT_EXCHANGER); //默认使用HeaderExchanger return getExchanger(type); } ...... public static Exchanger getExchanger(String type) { return ExtensionLoader.getExtensionLoader(Exchanger.class).getExtension(type); } ...... 可以看出，Exchangers只是根据URL的参数提供了策略模式。我们依然以dubbo默认的处理方式为主，接下来代码执行到HeaderExchanger类： public class HeaderExchanger implements Exchanger { public static final String NAME = &quot;header&quot;; public ExchangeClient connect(URL url, ExchangeHandler handler) throws RemotingException { return new HeaderExchangeClient(Transporters.connect(url, new DecodeHandler(new HeaderExchangeHandler(handler)))); } public ExchangeServer bind(URL url, ExchangeHandler handler) throws RemotingException { return new HeaderExchangeServer(Transporters.bind(url, new DecodeHandler(new HeaderExchangeHandler(handler)))); } } 这些代码看起来非常的设计模式： return new HeaderExchangeServer(Transporters.bind(url, new DecodeHandler(new HeaderExchangeHandler(handler)))); | | | | | V | | | V 1.提供统一的服务操作接口 | | | 利用装饰器模式，这个才是最靠近业务的逻辑（直接调用相关的invoker） 2.创建心跳定时任务 V | | 1.利于扩展点机制选择通信框架 | | 2.格式化回调函数 | | V V 消息的解码??? 处理dubbo的通信模型：单向，双向，异步等通信模型 要想理解现在的内容，就得先搞清楚JAVA NIO channel概念，搞清楚netty的NIO线程模型。 了解了这两个基础知识点，那么我们就可以继续分析源码了，上面那一行代码中Transporters.bind()默认会调用NettyTransporter： public class NettyTransporter implements Transporter { public static final String NAME = &quot;netty&quot;; public Server bind(URL url, ChannelHandler listener) throws RemotingException { return new NettyServer(url, listener); } public Client connect(URL url, ChannelHandler listener) throws RemotingException { return new NettyClient(url, listener); } } 接下来我们就真正进入到了netty的世界，我们先来看一下NettyServer的家谱： 要时刻记着，dubbo是一个非常灵活的框架，我们不仅可以使用netty作为底层通信组件，也可以仅靠url参数就可以改变底层通信的实现，这种架构设计彰显了开发人员对代码的驾驭能力。 AbstractServer抽象父类把创建server所需的公共逻辑抽离出来集中完成，而需要根据特定通信框架的逻辑则交给特定子类（NettyServer）利用重载（doOpen）完成，这样的代码结构在dubbo中随处可见。 @Override protected void doOpen() throws Throwable { NettyHelper.setNettyLoggerFactory(); ExecutorService boss = Executors.newCachedThreadPool(new NamedThreadFactory(&quot;NettyServerBoss&quot;, true)); ExecutorService worker = Executors.newCachedThreadPool(new NamedThreadFactory(&quot;NettyServerWorker&quot;, true)); ChannelFactory channelFactory = new NioServerSocketChannelFactory(boss, worker, getUrl().getPositiveParameter(Constants.IO_THREADS_KEY, Constants.DEFAULT_IO_THREADS)); bootstrap = new ServerBootstrap(channelFactory); final NettyHandler nettyHandler = new NettyHandler(getUrl(), this); channels = nettyHandler.getChannels(); // https://issues.jboss.org/browse/NETTY-365 // https://issues.jboss.org/browse/NETTY-379 // final Timer timer = new HashedWheelTimer(new NamedThreadFactory(&quot;NettyIdleTimer&quot;, true)); bootstrap.setPipelineFactory(new ChannelPipelineFactory() { public ChannelPipeline getPipeline() { NettyCodecAdapter adapter = new NettyCodecAdapter(getCodec() ,getUrl(), NettyServer.this); ChannelPipeline pipeline = Channels.pipeline(); /*int idleTimeout = getIdleTimeout(); if (idleTimeout &gt; 10000) { pipeline.addLast(&quot;timer&quot;, new IdleStateHandler(timer, idleTimeout / 1000, 0, 0)); }*/ pipeline.addLast(&quot;decoder&quot;, adapter.getDecoder()); //Upstream pipeline.addLast(&quot;encoder&quot;, adapter.getEncoder()); //Downstream pipeline.addLast(&quot;handler&quot;, nettyHandler); //Upstream &amp; Downstream return pipeline; } }); // bind channel = bootstrap.bind(getBindAddress()); } 如果是熟悉netty的童鞋，肯定早已习惯这个方法的写法，就是创建了netty的server嘛，不过需要注意的是，netty本身是基于事件的，留意一下上面的NettyServer的继承关系，其中ChannelHandler并不是netty的那个ChannelHandler，这就意味着要让前者转换成后者，才可以供netty使用，这也就是NettyHandler的意义，同样，类似这样的做法也可以在dubbo中找到多处。 同时也要注意，NettyServer和NettyHandler都有同一个用于记录打开中的channel的集合： private final Map&lt;String, Channel&gt; channels = new ConcurrentHashMap&lt;String, Channel&gt;(); // &lt;ip:port, channel&gt;，其中ip:port指的是调用端的ip和端口号 其中的Channel类型也并非netty的Channel，而是dubbo的NettyChannel，该类负责把netty的Channel，dubbo自身的url和handler映射起来，依赖这样的设计思想，就可以完全把业务和底层基础实现很好的隔离开来，灵活性大大提高，当然，复杂度也随之增加了，这是架构师需要权衡的一个哲学问题。 dubbo封装netty就介绍到这里，我们的分析并没有深入到netty太多，因为小弟我对netty的了解也是非常的皮毛，为了避免误人子弟，所以更多的细节就留给高手来分享吧。 编解码socket通信中有一个很好玩儿的部分，就是定义消息头，作用非常重大，例如解决粘包问题。dubbo借助netty这样的第三方框架来完成底层通信，这样一部分工作就委托出去了，不过还是有一些工作是需要dubbo好好规划的，我们来看一张官方提供的消息头格式： 只有搞清楚了消息头结构设计，才能完成消息体的编码解码，才能交给底层通信框架去收发。上图中我们其实只需要关注Dubbo部分，其部分意义已经在这篇文章中阐述过了，我们这里只关注代码实现，再来看一下NettyServer类： @Override protected void doOpen() throws Throwable { ...... NettyCodecAdapter adapter = new NettyCodecAdapter(getCodec() ,getUrl(), NettyServer.this); ChannelPipeline pipeline = Channels.pipeline(); pipeline.addLast(&quot;decoder&quot;, adapter.getDecoder()); //Upstream pipeline.addLast(&quot;encoder&quot;, adapter.getEncoder()); //Downstream pipeline.addLast(&quot;handler&quot;, nettyHandler); //Upstream &amp; Downstream return pipeline; ...... } 注意看我在每一行后面加的注释，参见这一篇关于netty的流处理顺序的文章，我们就可以理解dubbo的编码解码是如何配置的。下面接着看一下getCodec()方法： protected static Codec2 getChannelCodec(URL url) { String codecName = url.getParameter(Constants.CODEC_KEY, &quot;telnet&quot;); //这里的codecName值为dubbo if (ExtensionLoader.getExtensionLoader(Codec2.class).hasExtension(codecName)) { return ExtensionLoader.getExtensionLoader(Codec2.class).getExtension(codecName); } else { //应该是向下兼容 或者 阿里内部才会执行的代码 return new CodecAdapter(ExtensionLoader.getExtensionLoader(Codec.class) .getExtension(codecName)); } } 这里又一次尝试根据url中的codec参数来确定最终使用的编解码类，不过我们可以在DubboProtocol类的定义中看到，其实这个参数已经被硬编码了： //这里强行设置编码方式，有点硬啊 url = url.addParameter(Constants.CODEC_KEY, Version.isCompatibleVersion() ? COMPATIBLE_CODEC_NAME : DubboCodec.NAME); 注意这里Version.isCompatibleVersion()会去查找是否存在”com/taobao/remoting/impl/ConnectionRequest.class”，但我们知道，这是taobao内部的实现。 根据参数，我们看一下对应的配置文件： transport=com.alibaba.dubbo.remoting.transport.codec.TransportCodec telnet=com.alibaba.dubbo.remoting.telnet.codec.TelnetCodec exchange=com.alibaba.dubbo.remoting.exchange.codec.ExchangeCodec dubbo=com.alibaba.dubbo.rpc.protocol.dubbo.DubboCountCodec #使用的是这个 thrift=com.alibaba.dubbo.rpc.protocol.thrift.ThriftCodec 再看回来，NettyCodecAdapter完成了把netty和dubbo隔离的任务，使在后面进行编码解码时使用的channel不再是特定的通信框架提供的，而是dubbo提供的抽象实现。 再往下深挖，就会看到dubbo是如何处理数据包的拆装，由于过于琐碎，我决定暂时不继续下去了，日后如果在使用时出现问题，会单独拿出来讲讲。 序列化dubbo本身支持多种序列化方式，当当的duubox也在序列化方面做了新的工作，PRC中要解决跨进程通信的一个首要问题就是对象的系列化问题，业界各大佬公司和开源组织也都开源了很多优秀的项目，而要了解所有的序列化库是需要花大量时间的，我们依旧只关注dubbo是如何在代码层面触发序列化工作的。只有序列化算法本身，还是交给大家去对应官网进行深度学习吧。 序列化是在向对端发送数据前的重要工作，事实上我是在DubboCodec类中发现序列化工作的入口的： protected Object decodeBody(Channel channel, InputStream is, byte[] header) throws IOException { ...... Serialization s = CodecSupport.getSerialization(channel.getUrl(), proto); //获取对应的序列化库 ...... decodeEventData(channel, deserialize(s, channel.getUrl(), is)); ...... } private ObjectInput deserialize(Serialization serialization, URL url, InputStream is) throws IOException { return serialization.deserialize(url, is); } //该方法继承自ExchangeCodec父类 protected void encodeRequest(Channel channel, ChannelBuffer buffer, Request req) throws IOException { Serialization serialization = getSerialization(channel); ...... ChannelBufferOutputStream bos = new ChannelBufferOutputStream(buffer); ObjectOutput out = serialization.serialize(channel.getUrl(), bos); ...... } 而从dubbo的配置文件中看，dubbo[x]支持的序列化方式包括： dubbo=com.alibaba.dubbo.common.serialize.support.dubbo.DubboSerialization hessian2=com.alibaba.dubbo.common.serialize.support.hessian.Hessian2Serialization java=com.alibaba.dubbo.common.serialize.support.java.JavaSerialization compactedjava=com.alibaba.dubbo.common.serialize.support.java.CompactedJavaSerialization json=com.alibaba.dubbo.common.serialize.support.json.JsonSerialization fastjson=com.alibaba.dubbo.common.serialize.support.json.FastJsonSerialization nativejava=com.alibaba.dubbo.common.serialize.support.nativejava.NativeJavaSerialization kryo=com.alibaba.dubbo.common.serialize.support.kryo.KryoSerialization fst=com.alibaba.dubbo.common.serialize.support.fst.FstSerialization jackson=com.alibaba.dubbo.common.serialize.support.json.JacksonSerialization 好吧，到此为止，我们就算了解dubbo啦，如果有什么遗漏的地方，可以留言提醒小弟，一起学习进步。","tags":[{"name":"序列化","slug":"序列化","permalink":"https://blog.kazaff.me/tags/序列化/"},{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"dubbox","slug":"dubbox","permalink":"https://blog.kazaff.me/tags/dubbox/"},{"name":"编码","slug":"编码","permalink":"https://blog.kazaff.me/tags/编码/"},{"name":"netty","slug":"netty","permalink":"https://blog.kazaff.me/tags/netty/"}]},{"title":"Dubbo的拦截器和监听器","date":"2015-02-06T15:54:30.000Z","path":"2015/02/06/dubbo的拦截器和监听器/","text":"今天要聊一个可能被其他dubbo源码研究的童鞋容易忽略的话题：Filter和Listener。我们先来看一下这两个概念的官方手册： 拦截器 监听器：引用监听器和暴露监听器 老实说，依赖之前的源码分析经验，导致我饶了很大的弯路，一直找不到filter和listener被使用的位置。看过前几篇文章的朋友应该也有这个疑惑，为什么按照url参数去匹配框架的执行流程，死活找不到dubbo注入拦截器和监听器的位置呢？ ReferenceConfig --&gt; RegistryProtocol --&gt; DubboProtocol --&gt; invoker --&gt; exporter 按照这个调用流程，没错啊，可每一个环节都没有使用filter和listener属性的痕迹，有点抓瞎了啊。要说用好IDE确实很重要啊，光靠脑子想真的很伤身，下面来看一下谜底。 先来回忆一下dubbo的SPI机制，根据接口类型，dubbo会去读取并解析对应的配置文件，从中拿到对应的扩展点实现，好，我们先来看一下Protocol接口对应的配置文件： registry=com.alibaba.dubbo.registry.integration.RegistryProtocol dubbo=com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol filter=com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper #注意这一行 listener=com.alibaba.dubbo.rpc.protocol.ProtocolListenerWrapper #注意这一行 mock=com.alibaba.dubbo.rpc.support.MockProtocol injvm=com.alibaba.dubbo.rpc.protocol.injvm.InjvmProtocol rmi=com.alibaba.dubbo.rpc.protocol.rmi.RmiProtocol hessian=com.alibaba.dubbo.rpc.protocol.hessian.HessianProtocol com.alibaba.dubbo.rpc.protocol.http.HttpProtocol com.alibaba.dubbo.rpc.protocol.webservice.WebServiceProtocol thrift=com.alibaba.dubbo.rpc.protocol.thrift.ThriftProtocol memcached=com.alibaba.dubbo.rpc.protocol.memcached.MemcachedProtocol redis=com.alibaba.dubbo.rpc.protocol.redis.RedisProtocol rest=com.alibaba.dubbo.rpc.protocol.rest.RestProtocol 我们已经找到了filter和listener对应的扩展点了。接下来看一下它们是怎么一步一步的被注入到上面的流程里的。 在ReferenceConfig类中我们会引用和暴露对应的服务，我们以服务引用为场景来分析： get() --&gt; init() --&gt; createProxy() | +---&gt; invoker = refprotocol.refer(interfaceClass, urls.get(0)); 注意上面提到的这一行代码，这里的refprotocol是引用的Protocol$Adpative，这个类是dubbo的SPI机制动态创建的自适应扩展点，我们在之前的文章中已经介绍过，看一下它的refer方法细节： public com.alibaba.dubbo.rpc.Invoker refer(java.lang.Class arg0, com.alibaba.dubbo.common.URL arg1) throws java.lang.Class { if (arg1 == null) throw new IllegalArgumentException(&quot;url == null&quot;); com.alibaba.dubbo.common.URL url = arg1; String extName = ( url.getProtocol() == null ? &quot;dubbo&quot; : url.getProtocol() ); if(extName == null) throw new IllegalStateException(&quot;Fail to get extension(com.alibaba.dubbo.rpc.Protocol) name from url(&quot; + url.toString() + &quot;) use keys([protocol])&quot;); //注意这一行，根据url的协议名称选择对应的扩展点实现 com.alibaba.dubbo.rpc.Protocol extension = (com.alibaba.dubbo.rpc.Protocol)ExtensionLoader.getExtensionLoader(com.alibaba.dubbo.rpc.Protocol.class).getExtension(extName); return extension.refer(arg0, arg1); } 乍一看，并没有感觉有什么蹊跷，不过在单步调试中就会出现”诡异”现象（由于该类是动态创建的，所以该方法并不会被单步到，所以为分析带来了一定的干扰），我们得再往回倒一下，之前在dubbo中SPI的基础中曾经分析过ExtensionLoader的源码，但是当时由于了解的不够确实忽略了一些细节。 我们再来看一下它的执行流程： getExtension() --&gt; createExtension() | +--&gt; ...... Set&lt;Class&lt;?&gt;&gt; wrapperClasses = cachedWrapperClasses; if (wrapperClasses != null &amp;&amp; wrapperClasses.size() &gt; 0) { for (Class&lt;?&gt; wrapperClass : wrapperClasses) { //装饰器模式 instance = injectExtension((T) wrapperClass.getConstructor(type).newInstance(instance)); } } ...... 一看到这行代码，就知道关键点在这里，这种写法刚好就是和常见的拦截器和监听器的实现方法吻合，而且事实证明也确实是在这个地方完成的注入，那么我们就需要看一下这个cachedWrapperClasses到到底存了什么？ 我们最后看一下ExtensionLoader.loadFile方法，它是负责解析我们开头提到的那个SPI扩展点配置文件的，它会依次扫描配置文件的每一行，然后根据配置内容完成等号两边的键值对应关系，例如： test=com.alibaba.dubbo.rpc.filter.TestFilter loadFile的任务就是把test和解析过以后的TestFilter类关系对应上，供以后的getExtension查找使用。注意看其中的这几行代码： ...... clazz.getConstructor(type); //判断是否为wrapper实现 Set&lt;Class&lt;?&gt;&gt; wrappers = cachedWrapperClasses; if (wrappers == null) { cachedWrapperClasses = new ConcurrentHashSet&lt;Class&lt;?&gt;&gt;(); wrappers = cachedWrapperClasses; } wrappers.add(clazz); ...... 这里就完成了cachedWrapperClasses的初始化，它根据查看配置文件中定义的扩展点实现是否包含一个带有当前类型的构造方法为条件，确定哪些是wrapper，这样我们就可以发现： filter=com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper listener=com.alibaba.dubbo.rpc.protocol.ProtocolListenerWrapper 这两行命中了。这也是之后在真正获取protocol扩展点时会动态注入的两个重要包装类，前者完成拦截器，后者完成监听器。 至于拦截器和监听器的使用方法，我实在不知道除了官方提到的内容以外还有什么好补充的了，那就写到这里吧~","tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"dubbox","slug":"dubbox","permalink":"https://blog.kazaff.me/tags/dubbox/"},{"name":"soa","slug":"soa","permalink":"https://blog.kazaff.me/tags/soa/"},{"name":"拦截器","slug":"拦截器","permalink":"https://blog.kazaff.me/tags/拦截器/"},{"name":"监听器","slug":"监听器","permalink":"https://blog.kazaff.me/tags/监听器/"},{"name":"SPI","slug":"SPI","permalink":"https://blog.kazaff.me/tags/SPI/"}]},{"title":"Dubbo的服务治理细节","date":"2015-02-02T15:54:30.000Z","path":"2015/02/02/dubbo的服务治理细节/","text":"如果说单单只完成远程调用的话，dubbo还算不上是一个合格的SOA服务架构，而它之所以那么碉堡，是因为它还提供了服务治理的功能，今天就让我们来研究一下关于服务治理，dubbo都做了什么。听起来服务治理挺高大上的，但其实做的都是一些非常琐碎的事儿，了解了dubbo的做法，你就会发觉其实一切并没有想的那么复杂。远程调用要解决的最本质的问题是通信，通信就好像人和人之间的互动，有效的沟通建立在双方彼此了解的基础上（我们团队在沟通上就有死穴），同样道理，服务提供方和消费方之间要相互了解对方的基本情况，才能做到更好的完成远程调用。这里面就要提到dubbo的做法：URL。 前几篇中大量提到dubbo的分层之间是依靠什么纽带工作的：invoker，没错，比invoker更low的就是URL，这是dubbo带给我的另一个非常重要的经验。才疏学浅，并不知道dubbo是借鉴的哪里，但影响了全世界的WEB就是依赖URL机制建立了互联网帝国的！ 依赖URL机制，dubbo不仅打通了通信两端，而且还靠URL机制完成了服务治理的任务。我们可以先看一下这些内容： 路由规则 配置规则 服务降级 负载均衡 其实dubbo的路由和集群是在服务暴露，服务发现，服务引用中透明完成的，暴露给其他层的是同一个接口类型：Invoker。dubbo官方提供了一张巨清晰无比的图： 这张图是站在服务消费方的视角来看的（dubbo的服务治理都是针对服务消费方的），当业务逻辑中需要调用一个服务时，你真正调用的其实是dubbo创建的一个proxy，该proxy会把调用转化成调用指定的invoker（cluster封装过的）。而在这一系列的委托调用的过程里就完成了服务治理的逻辑，最终完成调用。 集群当相同服务由多个提供方同时提供时，消费方就需要有个选择的步骤，就好比你去电商平台买一本书，你自然会看一下哪儿买的最便宜。同样，消费方也需要根据需求选择到底使用哪个提供方的服务，而集群的主要作用就是从容错的维度来帮我们选择合适的服务提供方。 我们需要从Protocol接口的部分定义开始： /** * 引用远程服务：&lt;br&gt; * 1. 当用户调用refer()所返回的Invoker对象的invoke()方法时，协议需相应执行同URL远端export()传入的Invoker对象的invoke()方法。&lt;br&gt; * 2. refer()返回的Invoker由协议实现，协议通常需要在此Invoker中发送远程请求。&lt;br&gt; * 3. 当url中有设置check=false时，连接失败不能抛出异常，并内部自动恢复。&lt;br&gt; * * @param &lt;T&gt; 服务的类型 * @param type 服务的类型 * @param url 远程服务的URL地址 * @return invoker 服务的本地代理 * @throws RpcException 当连接服务提供方失败时抛出 */ @Adaptive &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; type, URL url) throws RpcException; 注意这个方法的返回值，根据我们这一系列文章一直使用的场景（有注册中心），看一下RegistryProtocol.doRefer方法的最后一行： return cluster.join(directory); 之前的文章提到过这个directory，它在后面我们会再次提到，这里你只需要知道它不是我们需要的invoker类型，那么这个cluster对象又是什么呢？根据dubbo的SPI机制，我们知道，这里的cluster是动态创建的自适应扩展点： package com.alibaba.dubbo.rpc.cluster; import com.alibaba.dubbo.common.extension.ExtensionLoader; public class Cluster$Adpative implements com.alibaba.dubbo.rpc.cluster.Cluster { public com.alibaba.dubbo.rpc.Invoker join(com.alibaba.dubbo.rpc.cluster.Directory arg0) throws com.alibaba.dubbo.rpc.cluster.Directory { if (arg0 == null) throw new IllegalArgumentException(&quot;com.alibaba.dubbo.rpc.cluster.Directory argument == null&quot;); if (arg0.getUrl() == null) throw new IllegalArgumentException(&quot;com.alibaba.dubbo.rpc.cluster.Directory argument getUrl() == null&quot;); com.alibaba.dubbo.common.URL url = arg0.getUrl(); String extName = url.getParameter(&quot;cluster&quot;, &quot;failover&quot;); //默认使用failover实现 if(extName == null) throw new IllegalStateException(&quot;Fail to get extension(com.alibaba.dubbo.rpc.cluster.Cluster) name from url(&quot; + url.toString() + &quot;) use keys([cluster])&quot;); com.alibaba.dubbo.rpc.cluster.Cluster extension = (com.alibaba.dubbo.rpc.cluster.Cluster)ExtensionLoader.getExtensionLoader(com.alibaba.dubbo.rpc.cluster.Cluster.class).getExtension(extName); return extension.join(arg0); } } 我们再来看一下默认使用的FailoverCluster定义： /** * 失败转移，当出现失败，重试其它服务器，通常用于读操作，但重试会带来更长延迟。 * * &lt;a href=&quot;http://en.wikipedia.org/wiki/Failover&quot;&gt;Failover&lt;/a&gt; * * @author william.liangf */ public class FailoverCluster implements Cluster { public final static String NAME = &quot;failover&quot;; public &lt;T&gt; Invoker&lt;T&gt; join(Directory&lt;T&gt; directory) throws RpcException { return new FailoverClusterInvoker&lt;T&gt;(directory); } } 看到了吗，这就是一开始图上的所表明的，cluster把存有多个invoker的directory对象封装成了单个的invoker。我们在来看一下FailoverClusterInvoker类的UML图： 根据官方文档的说明，dubbo提供了多种集群容错方案供我们直接使用，至于各种集群容错模式算法可以交给大家自己阅读源码来消化了，后面只会以FailoverClusterInvoker为基准来讨论。 路由和配置如果说集群帮我们以容错的维度来完成选择，那么路由和配置是在更细颗粒度的层面做的选择，具体有多细，可以从官方文档和dubbo-admin管理后台来了解，如下多图： 总之很细吧，这么多配置参数最终都会交给谁来管理呢？ 我们需要从Directory接口出发，你应该想到了该接口的一个实现类： 没错，就是这个RegistryDirectory，它在服务引用时被创建，用于充当url与多invoer的代理（或者叫目录类更合适），从源码可以看出，当服务引用时，对应该服务的目录类实例会负责向注册中心（zookeeper）订阅该服务，第一次订阅会同步拿到当前服务节点的详细信息（也就是所有提供服务的提供方信息，包括：地址，配置，路由等），然后该目录实例会根据这些信息来为后续的服务调用提供支撑。 根据描述我们可以锁定代码位置，RegistryDirectory.notify： ...... // configurators 更新缓存的服务提供方动态配置规则 if (configuratorUrls != null &amp;&amp; configuratorUrls.size() &gt;0 ){ this.configurators = toConfigurators(configuratorUrls); } // routers 更新缓存的路由配置规则 if (routerUrls != null &amp;&amp; routerUrls.size() &gt;0 ){ List&lt;Router&gt; routers = toRouters(routerUrls); if(routers != null){ // null - do nothing setRouters(routers); } } ...... 这些配置在什么时候发挥作用呢？往下看~ 前面说到当调用invoker时，其实调用的是集群模块封装过的代理invoker，那么以我们的场景为例，最终会被调用的是FailoverClusterInvoker.invoke： public Result invoke(final Invocation invocation) throws RpcException { checkWheatherDestoried(); LoadBalance loadbalance; //这里就是路由，配置等发挥作用地方，返回所有合法的invoker供集群做下一步的筛选 List&lt;Invoker&lt;T&gt;&gt; invokers = list(invocation); if (invokers != null &amp;&amp; invokers.size() &gt; 0) { loadbalance = ExtensionLoader.getExtensionLoader(LoadBalance.class).getExtension(invokers.get(0).getUrl() .getMethodParameter(invocation.getMethodName(),Constants.LOADBALANCE_KEY, Constants.DEFAULT_LOADBALANCE)); } else { loadbalance = ExtensionLoader.getExtensionLoader(LoadBalance.class).getExtension(Constants.DEFAULT_LOADBALANCE); } RpcUtils.attachInvocationIdIfAsync(getUrl(), invocation); return doInvoke(invocation, invokers, loadbalance); } 再来看一下这个list方法的定义： protected List&lt;Invoker&lt;T&gt;&gt; list(Invocation invocation) throws RpcException { List&lt;Invoker&lt;T&gt;&gt; invokers = directory.list(invocation); return invokers; } 很直接的把选择合法invoker的工作交给了我们的目录类实例，再来看一下directory是怎么list的： public List&lt;Invoker&lt;T&gt;&gt; list(Invocation invocation) throws RpcException { if (destroyed){ throw new RpcException(&quot;Directory already destroyed .url: &quot;+ getUrl()); } //根据请求服务的相关参数（方法名等）返回对应的invoker列表 List&lt;Invoker&lt;T&gt;&gt; invokers = doList(invocation); List&lt;Router&gt; localRouters = this.routers; // local reference if (localRouters != null &amp;&amp; localRouters.size() &gt; 0) { for (Router router: localRouters){ try { //是否在每次调用时执行路由规则，否则只在提供者地址列表变更时预先执行并缓存结果，调用时直接从缓存中获取路由结果。 //如果用了参数路由，必须设为true，需要注意设置会影响调用的性能，可不填，缺省为flase。 if (router.getUrl() == null || router.getUrl().getParameter(Constants.RUNTIME_KEY, true)) { invokers = router.route(invokers, getConsumerUrl(), invocation); } } catch (Throwable t) { logger.error(&quot;Failed to execute router: &quot; + getUrl() + &quot;, cause: &quot; + t.getMessage(), t); } } } return invokers; } 到这里我们就已经把路由和配置的相关流程介绍完了，至于路由和配置的具体参数是如何发挥效果的，这个大家可以结合文档提供的实例直接阅读源码即可。 负载均衡到了负载均衡环节，维度就成了性能，这个词你可以从gg里搜索大量的相关文献，我就不在这里卖弄了。把焦点拉回到FailoverClusterInvoker.invoke方法： ...... if (invokers != null &amp;&amp; invokers.size() &gt; 0) { loadbalance = ExtensionLoader.getExtensionLoader(LoadBalance.class).getExtension(invokers.get(0).getUrl() .getMethodParameter(invocation.getMethodName(),Constants.LOADBALANCE_KEY, Constants.DEFAULT_LOADBALANCE)); } else { //todo 如果invokers为空，还有必要往下走么？ loadbalance = ExtensionLoader.getExtensionLoader(LoadBalance.class).getExtension(Constants.DEFAULT_LOADBALANCE); } RpcUtils.attachInvocationIdIfAsync(getUrl(), invocation); return doInvoke(invocation, invokers, loadbalance); ...... 可以看到，这里就创建了要使用的负载均衡算法，我们接下来看一下到底是怎么使用这个loadbalance对象的，一路跟踪到AbstractClusterInvoker.doselect方法： ...... Invoker&lt;T&gt; invoker = loadbalance.select(invokers, getUrl(), invocation); ...... （其实本人并不喜欢这样截取部分代码展示，因为会让读者很窘迫，不过请相信我，这里这么做可以很好的排除干扰。）可见，最终是依靠负载均衡这最后一道关卡我们总算拿到了要调用的invoker。我们依然不去过多在意算法细节，到目前为止，负载均衡的流程也介绍完了。 其实dubbo服务治理相关的内容还有很多，官方文档也提供了详细的说明，希望大家都能成为dubbo大牛，bye~","tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"dubbox","slug":"dubbox","permalink":"https://blog.kazaff.me/tags/dubbox/"},{"name":"soa","slug":"soa","permalink":"https://blog.kazaff.me/tags/soa/"},{"name":"负载均衡","slug":"负载均衡","permalink":"https://blog.kazaff.me/tags/负载均衡/"},{"name":"路由","slug":"路由","permalink":"https://blog.kazaff.me/tags/路由/"},{"name":"集群","slug":"集群","permalink":"https://blog.kazaff.me/tags/集群/"}]},{"title":"Dubbo的服务发现细节","date":"2015-01-31T15:54:30.000Z","path":"2015/01/31/dubbo的服务发现细节/","text":"对于分布式服务架构，解决服务的发现问题，引入了注册中心中间件，从而很好的解决了服务双方（消费方和提供方）的直接依赖问题。这种解耦的意义是非凡的，不仅在程序运行时保证了灵活性，在开发阶段也使得快速迭代成为了可能，甚至在运维层面也提供了非常好的自由度。夸了这么多，但要实现一个完美的注册中心系统却不是一件那么容易的事儿，你必须时刻注意关注它的可用性（包括稳定，实时和高效），这一点在任何一款分布式系统中都是件很复杂的事儿。当然这篇文章并不是打算摆平这么个庞然大物，我们只是从dubbo和zookeeper之间的关系来了解一下在dubbo架构中注册中心的相关知识： 上图是官方给出的一张描述服务提供方、服务消费方和注册中心的关系图，其实dubbo提供多种注册中心实现，不过常用的就是zookeeper，我们也就拿它来当例子来分析。从图中可见，消费方远程调用服务方是不通过注册中心的，这有效的降低了注册中心的负载，也不会存在明显的单点瓶颈（尽管可以搭建注册中心的集群，但每次调用都走注册中心的话肯定对性能产生较大的伤害）。 官方提供的规则是： 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小； 注册中心，服务提供者，服务消费者三者之间均为长连接； 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者； 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者； 注册中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表； 注册中心是可选的，服务消费者可以直连服务提供者； 注册中心对等集群，任意一台宕掉后，将自动切换到另一台。 好啦，更多的理论我就不转载了，官方已经描述的非常详细了，我们按照老套路，从代码级别看一下dubbo到底是怎样实现的。 register我们需要承接之前的文章里的例子，从拿到需要暴露成服务的url开始： registry://127.0.0.1:2181/com.alibaba.dubbo.registry.RegistryService?application=demo-provider&amp;dubbo=2.0.0&amp;export=dubbo%3A%2F%2F192.168.153.1%3A20880%2Fcom.alibaba.dubbo.demo.bid.BidService%3Fanyhost%3Dtrue%26application%3Ddemo-provider%26dubbo%3D2.0.0%26generic%3Dfalse%26interface%3Dcom.alibaba.dubbo.demo.bid.BidService%26methods%3DthrowNPE%2Cbid%26optimizer%3Dcom.alibaba.dubbo.demo.SerializationOptimizerImpl%26organization%3Ddubbox%26owner%3Dprogrammer%26pid%3D3872%26serialization%3Dkryo%26side%3Dprovider%26timestamp%3D1422241023451&amp;organization=dubbox&amp;owner=programmer&amp;pid=3872&amp;registry=zookeeper&amp;timestamp=1422240274186 以这个url为基准暴露服务的话，dubbo会首先会根据指定协议（registry）拿到对应的protocol（RegistryProtocol），这部分是怎么做到的呢？还是之前通过IDE拿到的dubbo动态创建的protocol自适应扩展点，我们重点看export方法： package com.alibaba.dubbo.rpc; import com.alibaba.dubbo.common.extension.ExtensionLoader; public class Protocol$Adpative implements com.alibaba.dubbo.rpc.Protocol { ...... public com.alibaba.dubbo.rpc.Exporter export(com.alibaba.dubbo.rpc.Invoker arg0) throws com.alibaba.dubbo.rpc.Invoker { if (arg0 == null) throw new IllegalArgumentException(&quot;com.alibaba.dubbo.rpc.Invoker argument == null&quot;); if (arg0.getUrl() == null) throw new IllegalArgumentException(&quot;com.alibaba.dubbo.rpc.Invoker argument getUrl() == null&quot;); com.alibaba.dubbo.common.URL url = arg0.getUrl(); String extName = ( url.getProtocol() == null ? &quot;dubbo&quot; : url.getProtocol() ); //注意这句，根据我们的例子，extName=registry if(extName == null) throw new IllegalStateException(&quot;Fail to get extension(com.alibaba.dubbo.rpc.Protocol) name from url(&quot; + url.toString() + &quot;) use keys([protocol])&quot;); com.alibaba.dubbo.rpc.Protocol extension = (com.alibaba.dubbo.rpc.Protocol)ExtensionLoader.getExtensionLoader(com.alibaba.dubbo.rpc.Protocol.class).getExtension(extName); //根据扩展点加载规则，最终拿到RegistryProtocol实例。 return extension.export(arg0); } ...... } 我们需要注意RegistryProtocol的私有属性： private Protocol protocol; public void setProtocol(Protocol protocol) { this.protocol = protocol; //由SPI机制为其赋予一个protocol的自适应扩展点（动态创建的） } 这个属性真正被赋值的地方是在SPI机制中为扩展点注入的阶段（injectExtension方法）： private T injectExtension(T instance) { try { if (objectFactory != null) { for (Method method : instance.getClass().getMethods()) { if (method.getName().startsWith(&quot;set&quot;) &amp;&amp; method.getParameterTypes().length == 1 &amp;&amp; Modifier.isPublic(method.getModifiers())) { Class&lt;?&gt; pt = method.getParameterTypes()[0]; try { String property = method.getName().length() &gt; 3 ? method.getName().substring(3, 4).toLowerCase() + method.getName().substring(4) : &quot;&quot;; Object object = objectFactory.getExtension(pt, property); //注意这里，我们的例子中，这个object会是SPI动态创建的自适应扩展点实例：Protocol$Adpative if (object != null) { method.invoke(instance, object); } } catch (Exception e) { logger.error(&quot;fail to inject via method &quot; + method.getName() + &quot; of interface &quot; + type.getName() + &quot;: &quot; + e.getMessage(), e); } } } } } catch (Exception e) { logger.error(e.getMessage(), e); } return instance; } 有点乱，回到RegistryProtocol类，我们知道，在服务暴露阶段，会调用它的export方法，在这个方法里会完成服务的注册逻辑： public &lt;T&gt; Exporter&lt;T&gt; export(final Invoker&lt;T&gt; originInvoker) throws RpcException { //export invoker final ExporterChangeableWrapper&lt;T&gt; exporter = doLocalExport(originInvoker); //完成真正的服务暴露逻辑：默认以netty创建server服务来处理远程调用，打算回头专门写一下dubbo使用netty的细节 //registry provider final Registry registry = getRegistry(originInvoker); //根据url参数获取对应的注册中心服务实例，这里就是ZookeeperRegistry final URL registedProviderUrl = getRegistedProviderUrl(originInvoker); registry.register(registedProviderUrl); //向注册中心注册当前暴露的服务的URL // 订阅override数据 // FIXME 提供者订阅时，会影响同一JVM既暴露服务，又引用同一服务的的场景，因为subscribed以服务名为缓存的key，导致订阅信息覆盖。 final URL overrideSubscribeUrl = getSubscribedOverrideUrl(registedProviderUrl); final OverrideListener overrideSubscribeListener = new OverrideListener(overrideSubscribeUrl); overrideListeners.put(overrideSubscribeUrl, overrideSubscribeListener); registry.subscribe(overrideSubscribeUrl, overrideSubscribeListener); //保证每次export都返回一个新的exporter实例 return new Exporter&lt;T&gt;() { public Invoker&lt;T&gt; getInvoker() { return exporter.getInvoker(); } public void unexport() { try { exporter.unexport(); } catch (Throwable t) { logger.warn(t.getMessage(), t); } try { registry.unregister(registedProviderUrl); } catch (Throwable t) { logger.warn(t.getMessage(), t); } try { overrideListeners.remove(overrideSubscribeUrl); registry.unsubscribe(overrideSubscribeUrl, overrideSubscribeListener); } catch (Throwable t) { logger.warn(t.getMessage(), t); } } }; } 到这里，主线轮廓已经勾勒出来了，我们接下来看一下dubbo和zookeeper之间在服务注册阶段的通信细节，要从上面这个方法中的下面三行下手： //registry provider final Registry registry = getRegistry(originInvoker); //根据url参数获取对应的注册中心服务实例，这里就是ZookeeperRegistry final URL registedProviderUrl = getRegistedProviderUrl(originInvoker); registry.register(registedProviderUrl); //向注册中心注册当前暴露的服务的URL 正如注释标明的，第一行会获取invoker中url指定的注册中心实例，我们的情况就是拿到zookeeperRegistry。第二行其实就是过滤掉url中的注册中心相关参数，以及过滤器，监控中心等参数，按照我们上面的例子，registedProviderUrl大概应该如下： dubbo://192.168.153.1:20880/com.alibaba.dubbo.demo.bid.BidService?anyhost=true&amp;application=demo-provider&amp;dubbo=2.0.0&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.bid.BidService&amp;methods=throwNPE,bid&amp;optimizer=com.alibaba.dubbo.demo.SerializationOptimizerImpl&amp;organization=dubbox&amp;owner=programmer&amp;pid=3872&amp;serialization=kryo&amp;side=provider&amp;timestamp=1422241023451 我们主要看第三行，真正完成向zookeeper中注册的工作就是靠register方法完成的，先来看一下zookeeperRegistry的继承关系： 真正声明register方法的是zookeeperRegistry的父类：FailbackRegistry，从名字就能直观的看出它的作用，主要就是负责注册中心失效重试逻辑的。我们不打算在这里展开说这个话题。好吧，我们继续看zookeeperRegistry的doRegister方法（FailbackRegistry的register方法会调用zookeeperRegistry的doRegister的方法）： protected void doRegister(URL url) { try { zkClient.create(toUrlPath(url), url.getParameter(Constants.DYNAMIC_KEY, true)); //参见：http://alibaba.github.io/dubbo-doc-static/Zookeeper+Registry-zh.htm } catch (Throwable e) { throw new RpcException(&quot;Failed to register &quot; + url + &quot; to zookeeper &quot; + getUrl() + &quot;, cause: &quot; + e.getMessage(), e); } } 到这里就已经可以告一段落了，需要叮嘱的是toUrlPath方法，它的作用就是把url格式化成最终存储在zookeeper中的数据格式，尤其要注意category参数，它表示注册类型，如下图： 在我们的例子中，最终这次注册就会在对应serverInterface下的providers下创建一个url节点。 subscribe我们再来看看服务消费方对所引用服务的订阅细节，与服务提供方大致一样（忽略集群逻辑），只不过到达RegistryProtocol后调用的是refer方法： public &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; type, URL url) throws RpcException { //处理注册中心的协议，用url中registry参数的值作为真实的注册中心协议 url = url.setProtocol(url.getParameter(Constants.REGISTRY_KEY, Constants.DEFAULT_REGISTRY)).removeParameter(Constants.REGISTRY_KEY); Registry registry = registryFactory.getRegistry(url); //拿到真正的注册中心实例，我们的例子中就是zookeeperRegistry if (RegistryService.class.equals(type)) { //todo 不太理解，貌似是注册中心服务本身的暴露 return proxyFactory.getInvoker((T) registry, type, url); } //分组聚合处理，http://alibaba.github.io/dubbo-doc-static/Merge+By+Group-zh.htm // group=&quot;a,b&quot; or group=&quot;*&quot; Map&lt;String, String&gt; qs = StringUtils.parseQueryString(url.getParameterAndDecoded(Constants.REFER_KEY)); String group = qs.get(Constants.GROUP_KEY); if (group != null &amp;&amp; group.length() &gt; 0 ) { if ( ( Constants.COMMA_SPLIT_PATTERN.split( group ) ).length &gt; 1 || &quot;*&quot;.equals( group ) ) { return doRefer( getMergeableCluster(), registry, type, url ); } } return doRefer(cluster, registry, type, url); } 真正完成订阅是在doRefer方法中： private &lt;T&gt; Invoker&lt;T&gt; doRefer(Cluster cluster, Registry registry, Class&lt;T&gt; type, URL url) { RegistryDirectory&lt;T&gt; directory = new RegistryDirectory&lt;T&gt;(type, url); //这个directory把同一个serviceInterface对应的多个invoker管理起来提供概念上的化多为单一，供路由、均衡算法等使用 directory.setRegistry(registry); directory.setProtocol(protocol); URL subscribeUrl = new URL(Constants.CONSUMER_PROTOCOL, NetUtils.getLocalHost(), 0, type.getName(), directory.getUrl().getParameters()); //注册自己 if (! Constants.ANY_VALUE.equals(url.getServiceInterface()) &amp;&amp; url.getParameter(Constants.REGISTER_KEY, true)) { registry.register(subscribeUrl.addParameters(Constants.CATEGORY_KEY, Constants.CONSUMERS_CATEGORY, Constants.CHECK_KEY, String.valueOf(false))); } //订阅目标服务提供方 directory.subscribe(subscribeUrl.addParameter(Constants.CATEGORY_KEY, Constants.PROVIDERS_CATEGORY + &quot;,&quot; + Constants.CONFIGURATORS_CATEGORY + &quot;,&quot; + Constants.ROUTERS_CATEGORY)); return cluster.join(directory); //合并所有相同invoker } 可见代码和上面给的那个图很吻合，服务消费方不仅会订阅相关的服务，也会注册自身供其他层使用（服务治理）。特别要注意的是订阅时，同时订阅了三个分类类型：providers，routers，configurators。目前我们不打算说另外两种类型的意义（因为我也不清楚），后面分析道路由和集群的时候再来扯淡。 继续深挖dubbo中服务消费方订阅服务的细节，上面方法中最终把订阅细节委托给RegistryDirectory.subscribe方法，注意，这个方法接受的参数，此时的url已经把category设置为providers，routers，configurators： public void subscribe(URL url) { setConsumerUrl(url); registry.subscribe(url, this); } 这里registry就是zookeeperRegistry，这在doRefer方法可以看到明确的注入。然后和注册服务时一样，订阅会先由FailbackRegistry完成失效重试的处理，最终会交给zookeeperRegistry.doSubscribe方法。zookeeperRegistry实例拥有ZookeeperClient类型引用，该类型对象封装了和zookeeper通信的逻辑（默认是使用zkclient客户端），这里需要注意的一点，小爷我就被这里的一个数据结构卡住了一整天： private final ConcurrentMap&lt;URL, ConcurrentMap&lt;NotifyListener, ChildListener&gt;&gt; zkListeners = new ConcurrentHashMap&lt;URL, ConcurrentMap&lt;NotifyListener, ChildListener&gt;&gt;(); 一开始很不理解，为何要在url和NotifyListener之间再搞一个ChildListener接口出来，后来反复查看zkclient的文档说明和dubbo注册中心的设计，才悟出来点门道。这个ChildListener接口用于把zkclient的事件（IZkChildListener）转换到registry事件（NotifyListener）。这么做的深意不是特别的理解，可能是因为我并没有太多zookeeper的使用经验导致的，这里的做法可以更好的把zkclient的api和dubbo真身的注册中心逻辑分离开，毕竟dubbo除了zkclient以外还可以选择curator。从dubbo源码中可以看出，架构师和开发人员对面向对象和设计模式的理解非常的深刻，合理的运用继承和组合，打造了非常灵活的一套系统，保证概念统一的前提下展现了非常强大的多态性，感叹！ 这样走一圈下来，关于服务订阅的大致流程就描述清楚了，部分问题需要留到未来再解决了。 notify最后看一下注册推送细节，在订阅时你会注意到，订阅真正操作的是用RegistryDirectory类型封装过的对象，这个类型实现了一个接口NotifyListener（前面我们已经提到这个接口了），该接口用于描述支持推送通知逻辑： public interface NotifyListener { /** * 当收到服务变更通知时触发。 * * 通知需处理契约：&lt;br&gt; * 1. 总是以服务接口和数据类型为维度全量通知，即不会通知一个服务的同类型的部分数据，用户不需要对比上一次通知结果。&lt;br&gt; * 2. 订阅时的第一次通知，必须是一个服务的所有类型数据的全量通知。&lt;br&gt; * 3. 中途变更时，允许不同类型的数据分开通知，比如：providers, consumers, routers, overrides，允许只通知其中一种类型，但该类型的数据必须是全量的，不是增量的。&lt;br&gt; * 4. 如果一种类型的数据为空，需通知一个empty协议并带category参数的标识性URL数据。&lt;br&gt; * 5. 通知者(即注册中心实现)需保证通知的顺序，比如：单线程推送，队列串行化，带版本对比。&lt;br&gt; * * @param urls 已注册信息列表，总不为空，含义同{@link com.alibaba.dubbo.registry.RegistryService#lookup(URL)}的返回值。 */ void notify(List&lt;URL&gt; urls); } 前面提到了ChildListener接口，dubbo靠它把zkclient的事件转换成自己的事件类型，如果从代码上来看确实有点绕，事件的流程我手绘了一下： 我们主要看一下RegistryDirectory的notify方法： public synchronized void notify(List&lt;URL&gt; urls) { List&lt;URL&gt; invokerUrls = new ArrayList&lt;URL&gt;(); List&lt;URL&gt; routerUrls = new ArrayList&lt;URL&gt;(); List&lt;URL&gt; configuratorUrls = new ArrayList&lt;URL&gt;(); for (URL url : urls) { String protocol = url.getProtocol(); //允许不同类型的数据分开通知，比如：providers, consumers, routers, overrides，允许只通知其中一种类型，但该类型的数据必须是全量的，不是增量的。 String category = url.getParameter(Constants.CATEGORY_KEY, Constants.DEFAULT_CATEGORY); if (Constants.ROUTERS_CATEGORY.equals(category) || Constants.ROUTE_PROTOCOL.equals(protocol)) { routerUrls.add(url); } else if (Constants.CONFIGURATORS_CATEGORY.equals(category) || Constants.OVERRIDE_PROTOCOL.equals(protocol)) { configuratorUrls.add(url); } else if (Constants.PROVIDERS_CATEGORY.equals(category)) { invokerUrls.add(url); } else { logger.warn(&quot;Unsupported category &quot; + category + &quot; in notified url: &quot; + url + &quot; from registry &quot; + getUrl().getAddress() + &quot; to consumer &quot; + NetUtils.getLocalHost()); } } // configurators 更新缓存的服务提供方配置规则 if (configuratorUrls != null &amp;&amp; configuratorUrls.size() &gt;0 ){ this.configurators = toConfigurators(configuratorUrls); } // routers 更新缓存的路由配置规则 if (routerUrls != null &amp;&amp; routerUrls.size() &gt;0 ){ List&lt;Router&gt; routers = toRouters(routerUrls); if(routers != null){ // null - do nothing setRouters(routers); } } // 合并override参数 List&lt;Configurator&gt; localConfigurators = this.configurators; // local reference this.overrideDirectoryUrl = directoryUrl; if (localConfigurators != null &amp;&amp; localConfigurators.size() &gt; 0) { for (Configurator configurator : localConfigurators) { this.overrideDirectoryUrl = configurator.configure(overrideDirectoryUrl); } } // providers refreshInvoker(invokerUrls); } dubbo提供了强大的服务治理功能，所以这里在每次消费方接受到注册中心的通知后，大概会做下面这些事儿： 更新服务提供方配置规则 更新路由规则 重建invoker实例 前两件事儿我们放在分析路由，过滤器，集群的时候再讲，我们这里主要看dubbo如何“重建invoker实例”，也就是最后一行代码调用的方法refreshInvoker： private void refreshInvoker(List&lt;URL&gt; invokerUrls){ if (invokerUrls != null &amp;&amp; invokerUrls.size() == 1 &amp;&amp; invokerUrls.get(0) != null &amp;&amp; Constants.EMPTY_PROTOCOL.equals(invokerUrls.get(0).getProtocol())) { //如果传入的参数只包含一个empty://协议的url，表明禁用当前服务 this.forbidden = true; // 禁止访问 this.methodInvokerMap = null; // 置空列表 destroyAllInvokers(); // 关闭所有Invoker } else { this.forbidden = false; // 允许访问 Map&lt;String, Invoker&lt;T&gt;&gt; oldUrlInvokerMap = this.urlInvokerMap; // local reference if (invokerUrls.size() == 0 &amp;&amp; this.cachedInvokerUrls != null){ //如果传入的invokerUrl列表是空，则表示只是下发的override规则或route规则，需要重新交叉对比，决定是否需要重新引用 invokerUrls.addAll(this.cachedInvokerUrls); } else { this.cachedInvokerUrls = new HashSet&lt;URL&gt;(); this.cachedInvokerUrls.addAll(invokerUrls);//缓存invokerUrls列表，便于交叉对比 } if (invokerUrls.size() ==0 ){ return; } Map&lt;String, Invoker&lt;T&gt;&gt; newUrlInvokerMap = toInvokers(invokerUrls) ;// 将URL列表转成Invoker列表 Map&lt;String, List&lt;Invoker&lt;T&gt;&gt;&gt; newMethodInvokerMap = toMethodInvokers(newUrlInvokerMap); // 换方法名映射Invoker列表 // state change //如果计算错误，则不进行处理. if (newUrlInvokerMap == null || newUrlInvokerMap.size() == 0 ){ logger.error(new IllegalStateException(&quot;urls to invokers error .invokerUrls.size :&quot;+invokerUrls.size() + &quot;, invoker.size :0. urls :&quot;+invokerUrls.toString())); return ; } this.methodInvokerMap = multiGroup ? toMergeMethodInvokerMap(newMethodInvokerMap) : newMethodInvokerMap; this.urlInvokerMap = newUrlInvokerMap; try{ destroyUnusedInvokers(oldUrlInvokerMap,newUrlInvokerMap); // 关闭未使用的Invoker }catch (Exception e) { logger.warn(&quot;destroyUnusedInvokers error. &quot;, e); } } } 好吧，到这里我们已经完成了服务通知的业务逻辑，有兴趣的童鞋可以深究一下toInvokers方法，它又会走一遍url-&gt;invoker的逻辑（服务引用）。 那么，就先到这里吧，再会~","tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://blog.kazaff.me/tags/zookeeper/"},{"name":"dubbox","slug":"dubbox","permalink":"https://blog.kazaff.me/tags/dubbox/"},{"name":"soa","slug":"soa","permalink":"https://blog.kazaff.me/tags/soa/"},{"name":"分布式","slug":"分布式","permalink":"https://blog.kazaff.me/tags/分布式/"},{"name":"注册中心","slug":"注册中心","permalink":"https://blog.kazaff.me/tags/注册中心/"}]},{"title":"Dubbo的服务暴露细节","date":"2015-01-27T10:54:30.000Z","path":"2015/01/27/dubbo中服务暴露的细节/","text":"前一篇文章只是分析了一下从xml到service的代码流程细节，从中我们发现了一些架构层面的设计，小弟我非常的在意。所以我们这次在原先的基础上再深挖一点，看看能否出油。 这篇文字里会出现大量本尊的瞎掰（其实每篇都挺能瞎掰的），希望大家多多提醒，打脸什么的我丫根本就不怕。 服务接口类型的Wrapper处理在ServiceConfig.java中的doExportUrlsFor1Protocol方法中，我们看到，在得到最终URL之前，会执行下面的代码逻辑： String[] methods = Wrapper.getWrapper(interfaceClass).getMethodNames(); if(methods.length == 0) { logger.warn(&quot;NO method found in service interface &quot; + interfaceClass.getName()); map.put(&quot;methods&quot;, Constants.ANY_VALUE); } else { map.put(&quot;methods&quot;, StringUtils.join(new HashSet&lt;String&gt;(Arrays.asList(methods)), &quot;,&quot;)); } 那么这个Wrapper.getWrapper()的作用是什么呢？从代码层面来看，它按照dubbo自身的需求完成了类似java反射的工作，无非就是根据给定的服务实现接口类型，按照dubbo的要求提供读取该类型的相关类型信息的方法，有些绕口。这么做，可以让dubbo更自由的控制获取类型信息的相关操作，同时也一定程度的统一了调用方式。举个例子，wrapper后的对象可以在其他业务调时有效的屏蔽那些不希望被感知的原类型数据信息，对应设计模式的“适配器模式”。 除此之外，Wrapper还提供了对象缓存池的概念来提升性能： Wrapper ret = WRAPPER_MAP.get(c); if( ret == null ) { ret = makeWrapper(c); WRAPPER_MAP.put(c,ret); } return ret; 至于还有没有其他更深层次的作用，期待您的补充。 暴露前的proxy处理回顾之前提过的bean转service过程，我们当时提到了url，它作为不同层之间通信的keyword起到了重要的作用，但仅仅有key是不够的，如何通过key找到实际提供服务的bean才是本质。 dubbo的Invoker模型是非常关键的概念，看下图： 图中可以很清楚的看到url，ref，interface，ProxyFactory，Protocol和Invoker之间的关系，代码上来看，就是下面这两行： ...... Invoker&lt;?&gt; invoker = proxyFactory.getInvoker(ref, (Class) interfaceClass, url); Exporter&lt;?&gt; exporter = protocol.export(invoker); ...... 看似简单的两次调用之中，其实执行了非常多的逻辑。我们先来看一下这里proxyFactory对象是怎么拿到的（在ServiceConfig中声明了该静态属性）： private static final ProxyFactory proxyFactory = ExtensionLoader.getExtensionLoader(ProxyFactory.class).getAdaptiveExtension(); 对，没错，扩展点加载规则。不过较为麻烦的是从ProxyFactory接口中只能看出其使用的默认扩展点为”javassist”，可代码中却指定使用的是自适应扩展点，看一下配置文件中定义了什么： stub=com.alibaba.dubbo.rpc.proxy.wrapper.StubProxyFactoryWrapper jdk=com.alibaba.dubbo.rpc.proxy.jdk.JdkProxyFactory javassist=com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory 依次查阅这三个类的源码，并没有发现自适应扩展点的痕迹，也就是说最终dubbo会动态创建一个自适应扩展点类，这里作为外行新手，需要吐槽一下，dubbo中大量的动态类生产方式采用的是字符串拼接源码方式，这给代码审阅带来了非常大的困难，我相信即便是项目开发人员很难一次就写正确所有的逻辑。 不过幸好，我们有聪明的IDE，在辅助工具的帮助下，我们可以还原出dubbo针对ProxyFactory所动态创建的自适应扩展点类的完整代码： package com.alibaba.dubbo.rpc; import com.alibaba.dubbo.common.extension.ExtensionLoader; public class ProxyFactory$Adpative implements com.alibaba.dubbo.rpc.ProxyFactory { public java.lang.Object getProxy(com.alibaba.dubbo.rpc.Invoker arg0) throws com.alibaba.dubbo.rpc.Invoker { if (arg0 == null) throw new IllegalArgumentException(&quot;com.alibaba.dubbo.rpc.Invoker argument == null&quot;); if (arg0.getUrl() == null) throw new IllegalArgumentException(&quot;com.alibaba.dubbo.rpc.Invoker argument getUrl() == null&quot;); com.alibaba.dubbo.common.URL url = arg0.getUrl(); String extName = url.getParameter(&quot;proxy&quot;, &quot;javassist&quot;); if(extName == null) throw new IllegalStateException(&quot;Fail to get extension(com.alibaba.dubbo.rpc.ProxyFactory) name from url(&quot; + url.toString() + &quot;) use keys([proxy])&quot;); com.alibaba.dubbo.rpc.ProxyFactory extension = (com.alibaba.dubbo.rpc.ProxyFactory)ExtensionLoader.getExtensionLoader(com.alibaba.dubbo.rpc.ProxyFactory.class).getExtension(extName); return extension.getProxy(arg0); } public com.alibaba.dubbo.rpc.Invoker getInvoker(java.lang.Object arg0, java.lang.Class arg1, com.alibaba.dubbo.common.URL arg2) throws java.lang.Object { if (arg2 == null) throw new IllegalArgumentException(&quot;url == null&quot;); com.alibaba.dubbo.common.URL url = arg2; String extName = url.getParameter(&quot;proxy&quot;, &quot;javassist&quot;); if(extName == null) throw new IllegalStateException(&quot;Fail to get extension(com.alibaba.dubbo.rpc.ProxyFactory) name from url(&quot; + url.toString() + &quot;) use keys([proxy])&quot;); com.alibaba.dubbo.rpc.ProxyFactory extension = (com.alibaba.dubbo.rpc.ProxyFactory)ExtensionLoader.getExtensionLoader(com.alibaba.dubbo.rpc.ProxyFactory.class).getExtension(extName); return extension.getInvoker(arg0, arg1, arg2); } } 最终我们可以看出，dubbo会默认调用com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory作为proxyFactory的实际逻辑： public &lt;T&gt; Invoker&lt;T&gt; getInvoker(T proxy, Class&lt;T&gt; type, URL url) { // TODO Wrapper类不能正确处理带$的类名 final Wrapper wrapper = Wrapper.getWrapper(proxy.getClass().getName().indexOf(&apos;$&apos;) &lt; 0 ? proxy.getClass() : type); return new AbstractProxyInvoker&lt;T&gt;(proxy, type, url) { @Override protected Object doInvoke(T proxy, String methodName, Class&lt;?&gt;[] parameterTypes, Object[] arguments) throws Throwable { return wrapper.invokeMethod(proxy, methodName, parameterTypes, arguments); } }; } 哇塞，一下子就通透了。之所以命名为JavassistProxyFactory，就是因为它使用的是前面提到的Wrapper实例。逻辑很简单，直接完成了Invoker实例的创建，我们前面说了，Invoker是个很关键的概念，它的一个抽象定义如下： public abstract class AbstractProxyInvoker&lt;T&gt; implements Invoker&lt;T&gt; { private final T proxy; private final Class&lt;T&gt; type; private final URL url; public AbstractProxyInvoker(T proxy, Class&lt;T&gt; type, URL url){ if (proxy == null) { throw new IllegalArgumentException(&quot;proxy == null&quot;); } if (type == null) { throw new IllegalArgumentException(&quot;interface == null&quot;); } if (! type.isInstance(proxy)) { throw new IllegalArgumentException(proxy.getClass().getName() + &quot; not implement interface &quot; + type); } this.proxy = proxy; this.type = type; this.url = url; } public Class&lt;T&gt; getInterface() { return type; } public URL getUrl() { return url; } public boolean isAvailable() { return true; } public void destroy() { } public Result invoke(Invocation invocation) throws RpcException { try { return new RpcResult(doInvoke(proxy, invocation.getMethodName(), invocation.getParameterTypes(), invocation.getArguments())); } catch (InvocationTargetException e) { return new RpcResult(e.getTargetException()); } catch (Throwable e) { throw new RpcException(&quot;Failed to invoke remote proxy method &quot; + invocation.getMethodName() + &quot; to &quot; + getUrl() + &quot;, cause: &quot; + e.getMessage(), e); } } protected abstract Object doInvoke(T proxy, String methodName, Class&lt;?&gt;[] parameterTypes, Object[] arguments) throws Throwable; @Override public String toString() { return getInterface() + &quot; -&gt; &quot; + getUrl()==null?&quot; &quot;:getUrl().toString(); } } 也挺简单的，值得关注的是invoke方法，该方法是Invoker真正可以被其他对象调用的方法，逻辑也很简单，主要注意它的参数和返回值类型。 目前为止，我们已经完成了Invoker的转换，剩下的就是暴露服务的底层实现了。 服务暴露的底层实现老规矩，为了更好的理解代码意图，我们利用IDE把dubbo动态创建的protocol自适应扩展点类的代码还原： package com.alibaba.dubbo.rpc; import com.alibaba.dubbo.common.extension.ExtensionLoader; public class Protocol$Adpative implements com.alibaba.dubbo.rpc.Protocol { public int getDefaultPort() { throw new UnsupportedOperationException(&quot;method public abstract int com.alibaba.dubbo.rpc.Protocol.getDefaultPort() of interface com.alibaba.dubbo.rpc.Protocol is not adaptive method!&quot;); } public void destroy() { throw new UnsupportedOperationException(&quot;method public abstract void com.alibaba.dubbo.rpc.Protocol.destroy() of interface com.alibaba.dubbo.rpc.Protocol is not adaptive method!&quot;); } public com.alibaba.dubbo.rpc.Exporter export(com.alibaba.dubbo.rpc.Invoker arg0) throws com.alibaba.dubbo.rpc.Invoker { if (arg0 == null) throw new IllegalArgumentException(&quot;com.alibaba.dubbo.rpc.Invoker argument == null&quot;); if (arg0.getUrl() == null) throw new IllegalArgumentException(&quot;com.alibaba.dubbo.rpc.Invoker argument getUrl() == null&quot;); com.alibaba.dubbo.common.URL url = arg0.getUrl(); String extName = ( url.getProtocol() == null ? &quot;dubbo&quot; : url.getProtocol() ); if(extName == null) throw new IllegalStateException(&quot;Fail to get extension(com.alibaba.dubbo.rpc.Protocol) name from url(&quot; + url.toString() + &quot;) use keys([protocol])&quot;); com.alibaba.dubbo.rpc.Protocol extension = (com.alibaba.dubbo.rpc.Protocol)ExtensionLoader.getExtensionLoader(com.alibaba.dubbo.rpc.Protocol.class).getExtension(extName); return extension.export(arg0); } public com.alibaba.dubbo.rpc.Invoker refer(java.lang.Class arg0, com.alibaba.dubbo.common.URL arg1) throws java.lang.Class { if (arg1 == null) throw new IllegalArgumentException(&quot;url == null&quot;); com.alibaba.dubbo.common.URL url = arg1; String extName = ( url.getProtocol() == null ? &quot;dubbo&quot; : url.getProtocol() ); if(extName == null) throw new IllegalStateException(&quot;Fail to get extension(com.alibaba.dubbo.rpc.Protocol) name from url(&quot; + url.toString() + &quot;) use keys([protocol])&quot;); com.alibaba.dubbo.rpc.Protocol extension = (com.alibaba.dubbo.rpc.Protocol)ExtensionLoader.getExtensionLoader(com.alibaba.dubbo.rpc.Protocol.class).getExtension(extName); return extension.refer(arg0, arg1); } } 目前我们主要关注export方法，可以看出这个自适应扩展点的逻辑也很简单，从url中找出适配的协议参数，并获取指定协议的扩展点实现，并调用其export方法，一气呵成。 我们主要分析默认协议dubbo，所以接下来要分析的是DubboProtocol.java： public &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException { URL url = invoker.getUrl(); //拿到url，可见url的重要性 // export service. String key = serviceKey(url); //根据url中的设置拿到能够唯一标识该exporter的key DubboExporter&lt;T&gt; exporter = new DubboExporter&lt;T&gt;(invoker, key, exporterMap); //其实export只是简单的在invoker上封装了一层，提供了更“语义”的接口 exporterMap.put(key, exporter); //export an stub service for dispaching event Boolean isStubSupportEvent = url.getParameter(Constants.STUB_EVENT_KEY,Constants.DEFAULT_STUB_EVENT); Boolean isCallbackservice = url.getParameter(Constants.IS_CALLBACK_SERVICE, false); if (isStubSupportEvent &amp;&amp; !isCallbackservice){ String stubServiceMethods = url.getParameter(Constants.STUB_EVENT_METHODS_KEY); if (stubServiceMethods == null || stubServiceMethods.length() == 0 ){ if (logger.isWarnEnabled()){ logger.warn(new IllegalStateException(&quot;consumer [&quot; +url.getParameter(Constants.INTERFACE_KEY) + &quot;], has set stubproxy support event ,but no stub methods founded.&quot;)); } } else { stubServiceMethodsMap.put(url.getServiceKey(), stubServiceMethods); } } openServer(url); //根据url中定义的相关参数（协议，host，port等）创建服务，默认使用的是netty // modified by lishen optimizeSerialization(url); return exporter; } 其实exporter从代码层面来看只是对invoker封装了一层调用接口而已，并没有做其他什么转化操作，当然可以利用这一层封装来完成一些自定义逻辑，例如DubboExporter只是做了一层缓存处理。 到目前为止，我们可以知道，每个serviceConfig实例会根据配置中定义的注册中心和协议最终得到多个exporter实例。当有调用过来时，dubbo会通过请求消息中的相关信息来确定调用的exporter，并最终调用其封装的invoker的invoke方法完成业务逻辑。 更多的细节，推荐看一下之前发的这篇文章：dubbo协议下的单一长连接与多线程并发如何协同工作 不打算继续挖下去了，因为打算另起一篇专门聊dubbo中使用netty的文章。那就先瞎扯到这里吧，希望大牛能对上面的错误之处能够无情的给予打击。","tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"dubbox","slug":"dubbox","permalink":"https://blog.kazaff.me/tags/dubbox/"}]},{"title":"Dubbo如何一步一步拿到bean","date":"2015-01-26T18:54:30.000Z","path":"2015/01/26/dubbo如何一步一步拿到bean/","text":"dubbo依赖了spring提供的现成机制完成了bean的创建，我们来看一下这其中的汰渍。 配置关于dubbo的配置相关细节，官方已经给了一个无比详细的文档，文档2。不过由于dubbo可供配置的参数非常多，这也是让我们新手一开始感到最为头疼的，这也是SOA复杂的表象之一。 xml -&gt; beanDefinition对于我这种小学生，需要先补习一个基础知识点：基于Spring可扩展Schema提供自定义配置支持。dubbo是依赖spring提供的这种机制来处理配置文件解析的，理解起来没什么难度。 看一下dubbo-congfig的目录结构： 我们来看一下dubbo是如何按照spring提供的机制来处理配置文件的： #spring.handlers http\\://code.alibabatech.com/schema/dubbo=com.alibaba.dubbo.config.spring.schema.DubboNamespaceHandler #spring.schemas http\\://code.alibabatech.com/schema/dubbo/dubbo.xsd=META-INF/dubbo.xsd 这样我们就锁定了要分析的类： package com.alibaba.dubbo.config.spring.schema; public class DubboNamespaceHandler extends NamespaceHandlerSupport { static { Version.checkDuplicate(DubboNamespaceHandler.class); //确保系统中只存在一份解析处理器类定义 } public void init() { registerBeanDefinitionParser(&quot;application&quot;, new DubboBeanDefinitionParser(ApplicationConfig.class, true)); registerBeanDefinitionParser(&quot;module&quot;, new DubboBeanDefinitionParser(ModuleConfig.class, true)); registerBeanDefinitionParser(&quot;registry&quot;, new DubboBeanDefinitionParser(RegistryConfig.class, true)); registerBeanDefinitionParser(&quot;monitor&quot;, new DubboBeanDefinitionParser(MonitorConfig.class, true)); registerBeanDefinitionParser(&quot;provider&quot;, new DubboBeanDefinitionParser(ProviderConfig.class, true)); registerBeanDefinitionParser(&quot;consumer&quot;, new DubboBeanDefinitionParser(ConsumerConfig.class, true)); registerBeanDefinitionParser(&quot;protocol&quot;, new DubboBeanDefinitionParser(ProtocolConfig.class, true)); registerBeanDefinitionParser(&quot;service&quot;, new DubboBeanDefinitionParser(ServiceBean.class, true)); registerBeanDefinitionParser(&quot;reference&quot;, new DubboBeanDefinitionParser(ReferenceBean.class, false)); registerBeanDefinitionParser(&quot;annotation&quot;, new DubboBeanDefinitionParser(AnnotationBean.class, true)); } } 按照spring提供的机制，dubbo把每个自定义的可使用配置元素和对应的解析器绑定到一起。而真正负责把配置文件中声明的内容解析成对应的BeanDefinition（可以想象为Bean的模子）是靠DubboBeanDefinitionParser.parse类完成，我们就来严肃的分析一下这个方法。 /** * AbstractBeanDefinitionParser * * @author william.liangf * @export */ public class DubboBeanDefinitionParser implements BeanDefinitionParser { private static final Logger logger = LoggerFactory.getLogger(DubboBeanDefinitionParser.class); private final Class&lt;?&gt; beanClass; private final boolean required; public DubboBeanDefinitionParser(Class&lt;?&gt; beanClass, boolean required) { this.beanClass = beanClass; this.required = required; } public BeanDefinition parse(Element element, ParserContext parserContext) { return parse(element, parserContext, beanClass, required); } @SuppressWarnings(&quot;unchecked&quot;) private static BeanDefinition parse(Element element, ParserContext parserContext, Class&lt;?&gt; beanClass, boolean required) { //初始化BeanDefiniion RootBeanDefinition beanDefinition = new RootBeanDefinition(); beanDefinition.setBeanClass(beanClass); beanDefinition.setLazyInit(false); String id = element.getAttribute(&quot;id&quot;); if ((id == null || id.length() == 0) &amp;&amp; required) { String generatedBeanName = element.getAttribute(&quot;name&quot;); if (generatedBeanName == null || generatedBeanName.length() == 0) { if (ProtocolConfig.class.equals(beanClass)) { //如果当前解析的类型是ProtocolConfig，则设置默认id为dubbo generatedBeanName = &quot;dubbo&quot;; } else { generatedBeanName = element.getAttribute(&quot;interface&quot;); //其他情况，默认id为接口类型 } } if (generatedBeanName == null || generatedBeanName.length() == 0) { generatedBeanName = beanClass.getName(); //如果该节点没有interface属性（包含：registry,monitor,provider,consumer），则使用该节点的类型为id值 } id = generatedBeanName; int counter = 2; while(parserContext.getRegistry().containsBeanDefinition(id)) { //生成不重复的id id = generatedBeanName + (counter ++); } } if (id != null &amp;&amp; id.length() &gt; 0) { //目前这个判断不知道啥意义，目测必定会返回true if (parserContext.getRegistry().containsBeanDefinition(id)) { //这个判断应该用于防止并发 throw new IllegalStateException(&quot;Duplicate spring bean id &quot; + id); } //注册beanDefinition，BeanDefinitionRegistry相当于一张注册表 parserContext.getRegistry().registerBeanDefinition(id, beanDefinition); beanDefinition.getPropertyValues().addPropertyValue(&quot;id&quot;, id); } //下面这几个if-else分别针对不同类型做特殊处理 if (ProtocolConfig.class.equals(beanClass)) { //这段代码的逻辑是用来适配：当&lt;dubbo:protocol&gt;声明出现在配置文件中使用该协议的bean声明的后面时，解决它们之间的依赖关系的。 for (String name : parserContext.getRegistry().getBeanDefinitionNames()) { BeanDefinition definition = parserContext.getRegistry().getBeanDefinition(name); PropertyValue property = definition.getPropertyValues().getPropertyValue(&quot;protocol&quot;); if (property != null) { Object value = property.getValue(); //如果被检查的bean确实使用当前协议，则建立它们之间的依赖关系 if (value instanceof ProtocolConfig &amp;&amp; id.equals(((ProtocolConfig) value).getName())) { definition.getPropertyValues().addPropertyValue(&quot;protocol&quot;, new RuntimeBeanReference(id)); } } } } else if (ServiceBean.class.equals(beanClass)) { String className = element.getAttribute(&quot;class&quot;); //虽然文档上没有标注该配置支持class参数，但是在dubbo.xsd上却能看到这个属性的定义，类似这样的情况还有很多。 if(className != null &amp;&amp; className.length() &gt; 0) { //下面的处理方式应该算是语法糖吧，它支持直接把定义bean和创建serviceConfig压缩成一行 RootBeanDefinition classDefinition = new RootBeanDefinition(); classDefinition.setBeanClass(ReflectUtils.forName(className)); classDefinition.setLazyInit(false); parseProperties(element.getChildNodes(), classDefinition); //完成bean的初始化工作（注入等） beanDefinition.getPropertyValues().addPropertyValue(&quot;ref&quot;, new BeanDefinitionHolder(classDefinition, id + &quot;Impl&quot;)); //关联bean和serviceConfig } } else if (ProviderConfig.class.equals(beanClass)) { //按照providerConfig的定义解析并关联其影响的相关serviceConfig parseNested(element, parserContext, ServiceBean.class, true, &quot;service&quot;, &quot;provider&quot;, id, beanDefinition); } else if (ConsumerConfig.class.equals(beanClass)) { //按照consumerConfig的定义解析并关联其影响的相关referenceConfig parseNested(element, parserContext, ReferenceBean.class, false, &quot;reference&quot;, &quot;consumer&quot;, id, beanDefinition); } Set&lt;String&gt; props = new HashSet&lt;String&gt;(); ManagedMap parameters = null; for (Method setter : beanClass.getMethods()) { //利用反射拿到指定类型的所有用于注入的方法 String name = setter.getName(); if (name.length() &gt; 3 &amp;&amp; name.startsWith(&quot;set&quot;) &amp;&amp; Modifier.isPublic(setter.getModifiers()) &amp;&amp; setter.getParameterTypes().length == 1) { //注入方法的特征是：以set字母开头，是公共方法，且参数个数为1 Class&lt;?&gt; type = setter.getParameterTypes()[0]; String property = StringUtils.camelToSplitName(name.substring(3, 4).toLowerCase() + name.substring(4), &quot;-&quot;); //把方法名字的驼峰格式改成-分割格式 props.add(property); Method getter = null; try { getter = beanClass.getMethod(&quot;get&quot; + name.substring(3), new Class&lt;?&gt;[0]); } catch (NoSuchMethodException e) { try { getter = beanClass.getMethod(&quot;is&quot; + name.substring(3), new Class&lt;?&gt;[0]); } catch (NoSuchMethodException e2) { } } if (getter == null || ! Modifier.isPublic(getter.getModifiers()) || ! type.equals(getter.getReturnType())) { //如果没有满足条件的对应getter方法存在，则直接跳过该setter方法 continue; } if (&quot;parameters&quot;.equals(property)) { //从配置文件中解析出parameter配置，用于配置自定义参数，该配置项将作为扩展点设置自定义参数使用，parameter的值当string类型解析 parameters = parseParameters(element.getChildNodes(), beanDefinition); } else if (&quot;methods&quot;.equals(property)) { //注入对应的method配置 parseMethods(id, element.getChildNodes(), beanDefinition, parserContext); } else if (&quot;arguments&quot;.equals(property)) { //为method注入对应的argument配置 parseArguments(id, element.getChildNodes(), beanDefinition, parserContext); } else { String value = element.getAttribute(property); //检查该setter方法所适配要注入的属性是否在配置中明确定义 if (value != null) { //若存在定义，则完成其注入解析 value = value.trim(); if (value.length() &gt; 0) { if (&quot;registry&quot;.equals(property) &amp;&amp; RegistryConfig.NO_AVAILABLE.equalsIgnoreCase(value)) { //处理无注册中心的情况 RegistryConfig registryConfig = new RegistryConfig(); registryConfig.setAddress(RegistryConfig.NO_AVAILABLE); beanDefinition.getPropertyValues().addPropertyValue(property, registryConfig); } else if (&quot;registry&quot;.equals(property) &amp;&amp; value.indexOf(&apos;,&apos;) != -1) { //处理多注册中心的情况 parseMultiRef(&quot;registries&quot;, value, beanDefinition, parserContext); } else if (&quot;provider&quot;.equals(property) &amp;&amp; value.indexOf(&apos;,&apos;) != -1) { //处理继承多个provider的情况，缺使用第一个provider配置 parseMultiRef(&quot;providers&quot;, value, beanDefinition, parserContext); } else if (&quot;protocol&quot;.equals(property) &amp;&amp; value.indexOf(&apos;,&apos;) != -1) { //处理多协议暴露 parseMultiRef(&quot;protocols&quot;, value, beanDefinition, parserContext); } else { Object reference; if (isPrimitive(type)) { //如果setter的参数类型为jdk原始类型，直接当string注入到对应属性中去 if (&quot;async&quot;.equals(property) &amp;&amp; &quot;false&quot;.equals(value) || &quot;timeout&quot;.equals(property) &amp;&amp; &quot;0&quot;.equals(value) || &quot;delay&quot;.equals(property) &amp;&amp; &quot;0&quot;.equals(value) || &quot;version&quot;.equals(property) &amp;&amp; &quot;0.0.0&quot;.equals(value) || &quot;stat&quot;.equals(property) &amp;&amp; &quot;-1&quot;.equals(value) || &quot;reliable&quot;.equals(property) &amp;&amp; &quot;false&quot;.equals(value)) { // 兼容旧版本xsd中的default值 value = null; } reference = value; } else if (&quot;protocol&quot;.equals(property) &amp;&amp; ExtensionLoader.getExtensionLoader(Protocol.class).hasExtension(value) //是否存在指定的扩展点定义 &amp;&amp; (! parserContext.getRegistry().containsBeanDefinition(value) //检查当前解析出的要使用协议对应的protocolConfig是否已经被初始化 || ! ProtocolConfig.class.getName().equals(parserContext.getRegistry().getBeanDefinition(value).getBeanClassName()))) { if (&quot;dubbo:provider&quot;.equals(element.getTagName())) { logger.warn(&quot;Recommended replace &lt;dubbo:provider protocol=\\&quot;&quot; + value + &quot;\\&quot; ... /&gt; to &lt;dubbo:protocol name=\\&quot;&quot; + value + &quot;\\&quot; ... /&gt;&quot;); } // 兼容旧版本配置 ProtocolConfig protocol = new ProtocolConfig(); protocol.setName(value); reference = protocol; } else if (&quot;monitor&quot;.equals(property) &amp;&amp; (! parserContext.getRegistry().containsBeanDefinition(value) || ! MonitorConfig.class.getName().equals(parserContext.getRegistry().getBeanDefinition(value).getBeanClassName()))) { // 兼容旧版本配置 reference = convertMonitor(value); } else if (&quot;onreturn&quot;.equals(property)) { //对应methodConfig中的返回拦截 int index = value.lastIndexOf(&quot;.&quot;); String returnRef = value.substring(0, index); String returnMethod = value.substring(index + 1); reference = new RuntimeBeanReference(returnRef); beanDefinition.getPropertyValues().addPropertyValue(&quot;onreturnMethod&quot;, returnMethod); } else if (&quot;onthrow&quot;.equals(property)) { //对应methodConfig中的异常拦截 int index = value.lastIndexOf(&quot;.&quot;); String throwRef = value.substring(0, index); String throwMethod = value.substring(index + 1); reference = new RuntimeBeanReference(throwRef); beanDefinition.getPropertyValues().addPropertyValue(&quot;onthrowMethod&quot;, throwMethod); } else { if (&quot;ref&quot;.equals(property) &amp;&amp; parserContext.getRegistry().containsBeanDefinition(value)) { BeanDefinition refBean = parserContext.getRegistry().getBeanDefinition(value); if (! refBean.isSingleton()) { throw new IllegalStateException(&quot;The exported service ref &quot; + value + &quot; must be singleton! Please set the &quot; + value + &quot; bean scope to singleton, eg: &lt;bean id=\\&quot;&quot; + value+ &quot;\\&quot; scope=\\&quot;singleton\\&quot; ...&gt;&quot;); } } reference = new RuntimeBeanReference(value); } beanDefinition.getPropertyValues().addPropertyValue(property, reference); } } } } } } NamedNodeMap attributes = element.getAttributes(); int len = attributes.getLength(); for (int i = 0; i &lt; len; i++) { Node node = attributes.item(i); String name = node.getLocalName(); if (! props.contains(name)) { //处理配置中声明的没有满足注入条件的剩余属性 if (parameters == null) { parameters = new ManagedMap(); } String value = node.getNodeValue(); parameters.put(name, new TypedStringValue(value, String.class)); } } if (parameters != null) { beanDefinition.getPropertyValues().addPropertyValue(&quot;parameters&quot;, parameters); } return beanDefinition; } private static final Pattern GROUP_AND_VERION = Pattern.compile(&quot;^[\\\\-.0-9_a-zA-Z]+(\\\\:[\\\\-.0-9_a-zA-Z]+)?$&quot;); protected static MonitorConfig convertMonitor(String monitor) { if (monitor == null || monitor.length() == 0) { return null; } if (GROUP_AND_VERION.matcher(monitor).matches()) { String group; String version; int i = monitor.indexOf(&apos;:&apos;); if (i &gt; 0) { group = monitor.substring(0, i); version = monitor.substring(i + 1); } else { group = monitor; version = null; } MonitorConfig monitorConfig = new MonitorConfig(); monitorConfig.setGroup(group); monitorConfig.setVersion(version); return monitorConfig; } return null; } private static boolean isPrimitive(Class&lt;?&gt; cls) { return cls.isPrimitive() || cls == Boolean.class || cls == Byte.class || cls == Character.class || cls == Short.class || cls == Integer.class || cls == Long.class || cls == Float.class || cls == Double.class || cls == String.class || cls == Date.class || cls == Class.class; } @SuppressWarnings(&quot;unchecked&quot;) private static void parseMultiRef(String property, String value, RootBeanDefinition beanDefinition, ParserContext parserContext) { String[] values = value.split(&quot;\\\\s*[,]+\\\\s*&quot;); ManagedList list = null; for (int i = 0; i &lt; values.length; i++) { String v = values[i]; if (v != null &amp;&amp; v.length() &gt; 0) { if (list == null) { list = new ManagedList(); } list.add(new RuntimeBeanReference(v)); } } beanDefinition.getPropertyValues().addPropertyValue(property, list); } private static void parseNested(Element element, ParserContext parserContext, Class&lt;?&gt; beanClass, boolean required, String tag, String property, String ref, BeanDefinition beanDefinition) { NodeList nodeList = element.getChildNodes(); if (nodeList != null &amp;&amp; nodeList.getLength() &gt; 0) { boolean first = true; for (int i = 0; i &lt; nodeList.getLength(); i++) { Node node = nodeList.item(i); if (node instanceof Element) { if (tag.equals(node.getNodeName()) || tag.equals(node.getLocalName())) { if (first) { //如果该providerBean没有设置default开关，且子节点中定义了serviceBean，则明确赋值该参数为false，也就是说该providerBean只作为其子serviceBean节点的默认协议 //这样就不会让该providerBean的作用范围盲目扩大（成为所有serviceBean的默认协议） first = false; String isDefault = element.getAttribute(&quot;default&quot;); if (isDefault == null || isDefault.length() == 0) { beanDefinition.getPropertyValues().addPropertyValue(&quot;default&quot;, &quot;false&quot;); } } //所有子serviceBean定义节点全部解析并引用该providerBean作为默认值配置 BeanDefinition subDefinition = parse((Element) node, parserContext, beanClass, required); if (subDefinition != null &amp;&amp; ref != null &amp;&amp; ref.length() &gt; 0) { subDefinition.getPropertyValues().addPropertyValue(property, new RuntimeBeanReference(ref)); } } } } } } private static void parseProperties(NodeList nodeList, RootBeanDefinition beanDefinition) { if (nodeList != null &amp;&amp; nodeList.getLength() &gt; 0) { for (int i = 0; i &lt; nodeList.getLength(); i++) { Node node = nodeList.item(i); if (node instanceof Element) { if (&quot;property&quot;.equals(node.getNodeName()) || &quot;property&quot;.equals(node.getLocalName())) { String name = ((Element) node).getAttribute(&quot;name&quot;); if (name != null &amp;&amp; name.length() &gt; 0) { String value = ((Element) node).getAttribute(&quot;value&quot;); //java基础类型 String ref = ((Element) node).getAttribute(&quot;ref&quot;); //引用其他bean if (value != null &amp;&amp; value.length() &gt; 0) { beanDefinition.getPropertyValues().addPropertyValue(name, value); } else if (ref != null &amp;&amp; ref.length() &gt; 0) { beanDefinition.getPropertyValues().addPropertyValue(name, new RuntimeBeanReference(ref)); } else { throw new UnsupportedOperationException(&quot;Unsupported &lt;property name=\\&quot;&quot; + name + &quot;\\&quot;&gt; sub tag, Only supported &lt;property name=\\&quot;&quot; + name + &quot;\\&quot; ref=\\&quot;...\\&quot; /&gt; or &lt;property name=\\&quot;&quot; + name + &quot;\\&quot; value=\\&quot;...\\&quot; /&gt;&quot;); } } } } } } } @SuppressWarnings(&quot;unchecked&quot;) private static ManagedMap parseParameters(NodeList nodeList, RootBeanDefinition beanDefinition) { //解析参数配置，用于配置自定义参数，该配置项将作为扩展点设置自定义参数使用。 if (nodeList != null &amp;&amp; nodeList.getLength() &gt; 0) { ManagedMap parameters = null; for (int i = 0; i &lt; nodeList.getLength(); i++) { Node node = nodeList.item(i); if (node instanceof Element) { if (&quot;parameter&quot;.equals(node.getNodeName()) || &quot;parameter&quot;.equals(node.getLocalName())) { if (parameters == null) { parameters = new ManagedMap(); } String key = ((Element) node).getAttribute(&quot;key&quot;); String value = ((Element) node).getAttribute(&quot;value&quot;); boolean hide = &quot;true&quot;.equals(((Element) node).getAttribute(&quot;hide&quot;)); if (hide) { key = Constants.HIDE_KEY_PREFIX + key; } parameters.put(key, new TypedStringValue(value, String.class)); //注意parameter的值都是string类型 } } } return parameters; } return null; } @SuppressWarnings(&quot;unchecked&quot;) private static void parseMethods(String id, NodeList nodeList, RootBeanDefinition beanDefinition, ParserContext parserContext) { if (nodeList != null &amp;&amp; nodeList.getLength() &gt; 0) { ManagedList methods = null; for (int i = 0; i &lt; nodeList.getLength(); i++) { Node node = nodeList.item(i); if (node instanceof Element) { Element element = (Element) node; if (&quot;method&quot;.equals(node.getNodeName()) || &quot;method&quot;.equals(node.getLocalName())) { String methodName = element.getAttribute(&quot;name&quot;); if (methodName == null || methodName.length() == 0) { //name为必填项，这一点在文档里也表明 throw new IllegalStateException(&quot;&lt;dubbo:method&gt; name attribute == null&quot;); } if (methods == null) { methods = new ManagedList(); } BeanDefinition methodBeanDefinition = parse(((Element) node), parserContext, MethodConfig.class, false); //解析methodConfig String name = id + &quot;.&quot; + methodName; //注意这里，方法的名称前会加上bean的id BeanDefinitionHolder methodBeanDefinitionHolder = new BeanDefinitionHolder( methodBeanDefinition, name); methods.add(methodBeanDefinitionHolder); } } } if (methods != null) { beanDefinition.getPropertyValues().addPropertyValue(&quot;methods&quot;, methods); //关联bean和其method } } } @SuppressWarnings(&quot;unchecked&quot;) private static void parseArguments(String id, NodeList nodeList, RootBeanDefinition beanDefinition, ParserContext parserContext) { if (nodeList != null &amp;&amp; nodeList.getLength() &gt; 0) { ManagedList arguments = null; for (int i = 0; i &lt; nodeList.getLength(); i++) { Node node = nodeList.item(i); if (node instanceof Element) { Element element = (Element) node; if (&quot;argument&quot;.equals(node.getNodeName()) || &quot;argument&quot;.equals(node.getLocalName())) { String argumentIndex = element.getAttribute(&quot;index&quot;); //不清楚这里为何没有必填校验 if (arguments == null) { arguments = new ManagedList(); } BeanDefinition argumentBeanDefinition = parse(((Element) node), parserContext, ArgumentConfig.class, false); String name = id + &quot;.&quot; + argumentIndex; BeanDefinitionHolder argumentBeanDefinitionHolder = new BeanDefinitionHolder( argumentBeanDefinition, name); arguments.add(argumentBeanDefinitionHolder); } } } if (arguments != null) { beanDefinition.getPropertyValues().addPropertyValue(&quot;arguments&quot;, arguments); //关联arguments和其method } } } } 现在，我们的dubbo就已经把配置文件中定义的bean全部解析成对应的beanDefinition，为spring的getBean做好准备工作。 beanDefinition -&gt; bean其实也就是从beanDefinition转换成bean的过程，我在网上找了一幅图，可以辅助我们了解spring内部是如何初始化bean的： 这里也有一篇不错的文章，从代码的角度描述了spring内部是如何使用BeanDefinition生成Bean的，dubbo就是委托给spring来管理bean的生命周期的。 那么dubbo自定义schemas所产生的beanDefinition，spring是如何将其转换成dubbo需要的bean呢？毕竟我们从上面的解析中看到，解析生成的beanDefinition中包含太多dubbo特殊的配置方式。这里我有两个猜测： dubbo在spring提供的相关扩展点上实现了自己的getBean逻辑，可我却在dubbo的源码中找不到对应实现； spring解析生成的beanDefinition并没有dubbo特殊性，交给默认的BeanFactory没啥问题（这都怪我spring太差啊~） 带着疑问我进行了严酷的单步调试，事实证明是第二种路数。到目前为止，dubbo已经按照我们提供的配置文件把所有需要的bean初始化完成，这部分基本上都是交给spring来搞定的。 这还不够，我们还不知道这些bean是怎么服务于业务！ bean -&gt; service那么到底是哪些bean最终会被dubbo直接拿来使用呢？其实DubboNamespaceHandler.init中的那些就是答案。我们先看一下这些类的关系： 其实这么复杂的关系最终都会被转换成字符串以URL的形式交给dubbo的底层最终暴露成服务。我们先来重点看一下ServiceBean的实现，了解一下服务提供方的细节。 按照上面uml图来看，这些xxxConfig类的关系已经很清楚了（谁继承谁，谁聚合谁，谁依赖谁）。不过图里并没有出现我们的目标：ServiceBean，补充下图： 除了继承父类外，我们也要注意ServiceBean实现的相关接口： public class ServiceBean&lt;T&gt; extends ServiceConfig&lt;T&gt; implements InitializingBean, DisposableBean, ApplicationContextAware, ApplicationListener, BeanNameAware 这里我们着重看InitializingBean接口，该接口为spring留给开发者的一个hook，用来执行初始化bean的个性化逻辑的回调，详情可以看这篇文章。 既然我们的ServiceBean实现了这个接口，意味着当spring进行容器初始化任务过程中，会执行我们在ServiceBean.afterPropertiesSet方法中安排的逻辑，这也是bean导出为服务的关键入口，先把本尊注释过的代码贴出来： public class ServiceBean&lt;T&gt; extends ServiceConfig&lt;T&gt; implements InitializingBean, DisposableBean, ApplicationContextAware, ApplicationListener, BeanNameAware { private static final long serialVersionUID = 213195494150089726L; private static transient ApplicationContext SPRING_CONTEXT; private transient ApplicationContext applicationContext; private transient String beanName; private transient boolean supportedApplicationListener; public ServiceBean() { super(); } public ServiceBean(Service service) { super(service); } public static ApplicationContext getSpringContext() { return SPRING_CONTEXT; } public void setApplicationContext(ApplicationContext applicationContext) { this.applicationContext = applicationContext; //把该应用上下文存储在SpringExtensionFactory（dubbo的SPI扩展点机制）中 //把原先spring通过ApplicationContext获取bean的方式封装了一下，以dubbo统一的SPI扩展点机制风格接口暴露给业务使用 SpringExtensionFactory.addApplicationContext(applicationContext); if (applicationContext != null) { SPRING_CONTEXT = applicationContext; try { //把所有serviceBean都加入到ApplicationContext的事件通知中 Method method = applicationContext.getClass().getMethod(&quot;addApplicationListener&quot;, new Class&lt;?&gt;[]{ApplicationListener.class}); // 兼容Spring2.0.1 method.invoke(applicationContext, new Object[] {this}); supportedApplicationListener = true; } catch (Throwable t) { if (applicationContext instanceof AbstractApplicationContext) { try { Method method = AbstractApplicationContext.class.getDeclaredMethod(&quot;addListener&quot;, new Class&lt;?&gt;[]{ApplicationListener.class}); // 兼容Spring2.0.1 if (! method.isAccessible()) { method.setAccessible(true); } method.invoke(applicationContext, new Object[] {this}); supportedApplicationListener = true; } catch (Throwable t2) { } } } } } public void setBeanName(String name) { this.beanName = name; } //如果配置serviceBean时声明了延迟暴露（例如：&lt;dubbo:service delay=&quot;-1&quot; /&gt;），则会依赖监听spring提供的相关事件来触发export public void onApplicationEvent(ApplicationEvent event) { if (ContextRefreshedEvent.class.getName().equals(event.getClass().getName())) { //监听ContextRefreshedEvent事件（容器发生初始化或更新时触发） if (isDelay() &amp;&amp; ! isExported() &amp;&amp; ! isUnexported()) { //如果已导出过或者已手工放弃导出则不会执行export逻辑 if (logger.isInfoEnabled()) { logger.info(&quot;The service ready on spring started. service: &quot; + getInterface()); } export(); } } } private boolean isDelay() { Integer delay = getDelay(); ProviderConfig provider = getProvider(); if (delay == null &amp;&amp; provider != null) { //若没有明确指定延迟，则尝试继承provider配置 delay = provider.getDelay(); } return supportedApplicationListener &amp;&amp; (delay == null || delay.intValue() == -1); //注意这里对delay值的条件很奇怪，如果我设置delay为5000毫秒时，难道不算是延迟么？请参考\\com\\alibaba\\dubbo\\config\\ServiceConfig.java的132行 } @SuppressWarnings({ &quot;unchecked&quot;, &quot;deprecation&quot; }) public void afterPropertiesSet() throws Exception { if (getProvider() == null) { //如果当前serviceBean并没有指定provider，则下面的逻辑为其指定默认的providerConfig（如果存在的话） Map&lt;String, ProviderConfig&gt; providerConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ProviderConfig.class, false, false); if (providerConfigMap != null &amp;&amp; providerConfigMap.size() &gt; 0) { Map&lt;String, ProtocolConfig&gt; protocolConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ProtocolConfig.class, false, false); if ((protocolConfigMap == null || protocolConfigMap.size() == 0) &amp;&amp; providerConfigMap.size() &gt; 1) { // 兼容旧版本 List&lt;ProviderConfig&gt; providerConfigs = new ArrayList&lt;ProviderConfig&gt;(); for (ProviderConfig config : providerConfigMap.values()) { if (config.isDefault() != null &amp;&amp; config.isDefault().booleanValue()) { //把所有指定为默认范围的providerConfig拿到，跳转到下面 providerConfigs.add(config); } } if (providerConfigs.size() &gt; 0) { setProviders(providerConfigs); //接着上面，把所有指定为默认范围的providerConfig中与protocol相关的配置封装成protocolConfig并存入serviceConfig对应属性中 } } else { ProviderConfig providerConfig = null; for (ProviderConfig config : providerConfigMap.values()) { //如果某个provider配置包含子node（ServiceBean），且没有明确指定default，也会被当成默认配置么？这个疑问请参看：com\\alibaba\\dubbo\\config\\spring\\schema\\DubboBeanDefinitionParser.java中330行注解 if (config.isDefault() == null || config.isDefault().booleanValue()) { if (providerConfig != null) { //只能有一个provider设置为默认 throw new IllegalStateException(&quot;Duplicate provider configs: &quot; + providerConfig + &quot; and &quot; + config); } providerConfig = config; } } if (providerConfig != null) { setProvider(providerConfig); //为serviceBean绑定继承的providerConfig } } } } //如果当前serviceBean并没有指定Application且其继承的provider也没有指定Application，则下面的逻辑为其指定默认的applicationConfig（如果存在的话） if (getApplication() == null &amp;&amp; (getProvider() == null || getProvider().getApplication() == null)) { Map&lt;String, ApplicationConfig&gt; applicationConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ApplicationConfig.class, false, false); if (applicationConfigMap != null &amp;&amp; applicationConfigMap.size() &gt; 0) { ApplicationConfig applicationConfig = null; for (ApplicationConfig config : applicationConfigMap.values()) { if (config.isDefault() == null || config.isDefault().booleanValue()) { if (applicationConfig != null) { //只能有一个Application设置为默认 throw new IllegalStateException(&quot;Duplicate application configs: &quot; + applicationConfig + &quot; and &quot; + config); } applicationConfig = config; } } if (applicationConfig != null) { setApplication(applicationConfig); //为serviceBean绑定applicationConfig } } } //如果当前serviceBean并没有指定Module且其继承的provider也没有指定Module，则下面的逻辑为其指定默认的moduleConfig（如果存在的话） if (getModule() == null &amp;&amp; (getProvider() == null || getProvider().getModule() == null)) { Map&lt;String, ModuleConfig&gt; moduleConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ModuleConfig.class, false, false); if (moduleConfigMap != null &amp;&amp; moduleConfigMap.size() &gt; 0) { ModuleConfig moduleConfig = null; for (ModuleConfig config : moduleConfigMap.values()) { if (config.isDefault() == null || config.isDefault().booleanValue()) { if (moduleConfig != null) { //只能有一个Module设置为默认 throw new IllegalStateException(&quot;Duplicate module configs: &quot; + moduleConfig + &quot; and &quot; + config); } moduleConfig = config; } } if (moduleConfig != null) { setModule(moduleConfig); //为serviceBean绑定moduleConfig } } } //如果当前serviceBean并没有指定Registry且其继承的provider,application也没有指定Registry，则下面的逻辑为其指定默认的registryConfig（如果存在的话） if ((getRegistries() == null || getRegistries().size() == 0) &amp;&amp; (getProvider() == null || getProvider().getRegistries() == null || getProvider().getRegistries().size() == 0) &amp;&amp; (getApplication() == null || getApplication().getRegistries() == null || getApplication().getRegistries().size() == 0)) { Map&lt;String, RegistryConfig&gt; registryConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, RegistryConfig.class, false, false); if (registryConfigMap != null &amp;&amp; registryConfigMap.size() &gt; 0) { List&lt;RegistryConfig&gt; registryConfigs = new ArrayList&lt;RegistryConfig&gt;(); for (RegistryConfig config : registryConfigMap.values()) { if (config.isDefault() == null || config.isDefault().booleanValue()) { //允许为serviceBean指定多个Registry registryConfigs.add(config); } } if (registryConfigs != null &amp;&amp; registryConfigs.size() &gt; 0) { super.setRegistries(registryConfigs); } } } //如果当前serviceBean并没有指定Monitor且其继承的provider,application也没有指定Monitor，则下面的逻辑为其指定默认的monitorConfig（如果存在的话） if (getMonitor() == null &amp;&amp; (getProvider() == null || getProvider().getMonitor() == null) &amp;&amp; (getApplication() == null || getApplication().getMonitor() == null)) { Map&lt;String, MonitorConfig&gt; monitorConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, MonitorConfig.class, false, false); if (monitorConfigMap != null &amp;&amp; monitorConfigMap.size() &gt; 0) { MonitorConfig monitorConfig = null; for (MonitorConfig config : monitorConfigMap.values()) { if (config.isDefault() == null || config.isDefault().booleanValue()) { if (monitorConfig != null) { //只能有一个Monitor设置为默认 throw new IllegalStateException(&quot;Duplicate monitor configs: &quot; + monitorConfig + &quot; and &quot; + config); } monitorConfig = config; } } if (monitorConfig != null) { setMonitor(monitorConfig); //为serviceBean绑定monitorConfig } } } //如果当前serviceBean并没有指定Protocol且其继承的provider也没有指定Protocol，则下面的逻辑为其指定默认的protocolConfig（如果存在的话） if ((getProtocols() == null || getProtocols().size() == 0) &amp;&amp; (getProvider() == null || getProvider().getProtocols() == null || getProvider().getProtocols().size() == 0)) { Map&lt;String, ProtocolConfig&gt; protocolConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ProtocolConfig.class, false, false); if (protocolConfigMap != null &amp;&amp; protocolConfigMap.size() &gt; 0) { List&lt;ProtocolConfig&gt; protocolConfigs = new ArrayList&lt;ProtocolConfig&gt;(); for (ProtocolConfig config : protocolConfigMap.values()) { if (config.isDefault() == null || config.isDefault().booleanValue()) { protocolConfigs.add(config); //允许为serviceBean指定多个Protocol } } if (protocolConfigs != null &amp;&amp; protocolConfigs.size() &gt; 0) { super.setProtocols(protocolConfigs); } } } //设置服务路径，默认使用的是该bean在spring容器中注册的beanName，这也是该类继承BeanNameAware的原因 if (getPath() == null || getPath().length() == 0) { if (beanName != null &amp;&amp; beanName.length() &gt; 0 &amp;&amp; getInterface() != null &amp;&amp; getInterface().length() &gt; 0 &amp;&amp; beanName.startsWith(getInterface())) { setPath(beanName); } } //若不是延迟加载，就上演好戏 if (! isDelay()) { export(); } } public void destroy() throws Exception { unexport(); } } 这里就明白为何ServiceBean和其父类ServiceConfig不在同一个包内，因为前者是为了适配spring而提供的适配器。ServiceBean依赖spring提供的相关hook接口完成了bean的初始化，最终export逻辑交给ServiceConfig来完成，这才是dubbo的核心服务配置类，这也解释了为何上面UML图中没有画ServiceBean的原因。 我们继续跟着线索来看一下ServiceConfig.export： public synchronized void export() { //从provider中继承一些必要但没有明确设置的参数 if (provider != null) { if (export == null) { export = provider.getExport(); } if (delay == null) { delay = provider.getDelay(); } } if (export != null &amp;&amp; ! export.booleanValue()) { //如果不需要暴露该服务，则就此结束 return; } if (delay != null &amp;&amp; delay &gt; 0) { //如果明确指定了想要延迟的时间差，则依赖线程休眠来完成延迟暴露，delay的值只有为-1或null才依赖spring的事件机制完成延迟暴露 Thread thread = new Thread(new Runnable() { public void run() { try { Thread.sleep(delay); } catch (Throwable e) { } doExport(); } }); thread.setDaemon(true); thread.setName(&quot;DelayExportServiceThread&quot;); thread.start(); } else { doExport(); } } 一目了然，这个方法主要就是解决了到底暴露不暴露的问题，并且到底是不是延迟暴露的问题。接下来看看doExport方法： protected synchronized void doExport() { if (unexported) { throw new IllegalStateException(&quot;Already unexported!&quot;); } if (exported) { return; } exported = true; //修改暴露状态 if (interfaceName == null || interfaceName.length() == 0) { throw new IllegalStateException(&quot;&lt;dubbo:service interface=\\&quot;\\&quot; /&gt; interface not allow null!&quot;); } checkDefault(); //根据文档中提到的参数优先级，决定最终使用的配置值，在spring的xml解析阶段只是简单解析xml的配置值，在真正使用前，还需要看一下：-D和properties文件 //下面根据文档中的优先级创建对应的继承链 if (provider != null) { //todo 这里必然成立吧？ if (application == null) { application = provider.getApplication(); } if (module == null) { module = provider.getModule(); } if (registries == null) { registries = provider.getRegistries(); } if (monitor == null) { monitor = provider.getMonitor(); } if (protocols == null) { protocols = provider.getProtocols(); } } if (module != null) { if (registries == null) { registries = module.getRegistries(); } if (monitor == null) { monitor = module.getMonitor(); } } if (application != null) { if (registries == null) { registries = application.getRegistries(); } if (monitor == null) { monitor = application.getMonitor(); } } if (ref instanceof GenericService) { //泛接口实现方式主要用于服务器端没有API接口及模型类元的情况，参数及返回值中的所有POJO均用Map表示，通常用于框架集成，比如：实现一个通用的远程服务Mock框架，可通过实现GenericService接口处理所有服务请求。 interfaceClass = GenericService.class; if (StringUtils.isEmpty(generic)) { generic = Boolean.TRUE.toString(); } } else { try { interfaceClass = Class.forName(interfaceName, true, Thread.currentThread() .getContextClassLoader()); } catch (ClassNotFoundException e) { throw new IllegalStateException(e.getMessage(), e); } checkInterfaceAndMethods(interfaceClass, methods); //检查接口和方法的匹配情况 checkRef(); //检查接口和实现的匹配情况 generic = Boolean.FALSE.toString(); } if(local !=null){ //todo 文档中并没有与local相关的参数解释 if(local==&quot;true&quot;){ local=interfaceName+&quot;Local&quot;; } Class&lt;?&gt; localClass; try { localClass = ClassHelper.forNameWithThreadContextClassLoader(local); } catch (ClassNotFoundException e) { throw new IllegalStateException(e.getMessage(), e); } if(!interfaceClass.isAssignableFrom(localClass)){ throw new IllegalStateException(&quot;The local implemention class &quot; + localClass.getName() + &quot; not implement interface &quot; + interfaceName); } } //本地存根，http://alibaba.github.io/dubbo-doc-static/Stub+Proxy-zh.htm if(stub !=null){ if(stub==&quot;true&quot;){ stub=interfaceName+&quot;Stub&quot;; //todo 这里文档中的解释貌似有错误：http://alibaba.github.io/dubbo-doc-static/Service+Config-zh.htm } Class&lt;?&gt; stubClass; try { stubClass = ClassHelper.forNameWithThreadContextClassLoader(stub); } catch (ClassNotFoundException e) { throw new IllegalStateException(e.getMessage(), e); } if(!interfaceClass.isAssignableFrom(stubClass)){ throw new IllegalStateException(&quot;The stub implemention class &quot; + stubClass.getName() + &quot; not implement interface &quot; + interfaceName); } } //作用雷同于上面的checkDefault()，根据文档中提到的参数优先级来选择使用的配置参数 checkApplication(); checkRegistry(); checkProtocol(); appendProperties(this); checkStubAndMock(interfaceClass); //检查local，stub和mock的有效性 if (path == null || path.length() == 0) { //此时path如果还为空，这使用interfaceName path = interfaceName; } doExportUrls(); } 木牛错，doExport方法依然是在做预备工作，感觉越来越靠近真像了，目前为止，我们已经按照规定的优先级最终确定了要暴露成为服务的bean的”大部分”相关配置参数，并校验了相关参数的有效性（例如：ref，method，stub，mock，path等）。再来看一下doExportUrls方法： private void doExportUrls() { List&lt;URL&gt; registryURLs = loadRegistries(true); //获取所有的注册中心地址 for (ProtocolConfig protocolConfig : protocols) { doExportUrlsFor1Protocol(protocolConfig, registryURLs); } } 好，是该真刀真枪干一架的时候了，doExportUrlsFor1Protocol应该就是今次的Boss，解决掉它我们就回家。 private void doExportUrlsFor1Protocol(ProtocolConfig protocolConfig, List&lt;URL&gt; registryURLs) { String name = protocolConfig.getName(); if (name == null || name.length() == 0) { name = &quot;dubbo&quot;; //N多次的检查，N多次的赋值，这算是严谨呢？还是重复？ } String host = protocolConfig.getHost(); if (provider != null &amp;&amp; (host == null || host.length() == 0)) { host = provider.getHost(); } boolean anyhost = false; if (NetUtils.isInvalidLocalHost(host)) { //检查host是否为本地ip，或者无效的 anyhost = true; try { host = InetAddress.getLocalHost().getHostAddress(); } catch (UnknownHostException e) { logger.warn(e.getMessage(), e); } if (NetUtils.isInvalidLocalHost(host)) { //如果拿到的还是本地地址，就只能出杀手锏了 if (registryURLs != null &amp;&amp; registryURLs.size() &gt; 0) { for (URL registryURL : registryURLs) { try { Socket socket = new Socket(); try { //尝试连接注册中心，选用连接时使用的ip地址 SocketAddress addr = new InetSocketAddress(registryURL.getHost(), registryURL.getPort()); socket.connect(addr, 1000); host = socket.getLocalAddress().getHostAddress(); break; } finally { try { socket.close(); } catch (Throwable e) {} } } catch (Exception e) { logger.warn(e.getMessage(), e); } } } if (NetUtils.isInvalidLocalHost(host)) { host = NetUtils.getLocalHost(); //实在不行，就只能使用本机上第一个找到的合法ip了 } } } Integer port = protocolConfig.getPort(); if (provider != null &amp;&amp; (port == null || port == 0)) { port = provider.getPort(); } final int defaultPort = ExtensionLoader.getExtensionLoader(Protocol.class).getExtension(name).getDefaultPort(); if (port == null || port == 0) { port = defaultPort; } if (port == null || port &lt;= 0) { port = getRandomPort(name); if (port == null || port &lt; 0) { port = NetUtils.getAvailablePort(defaultPort); //到这里如果还没有拿到port，就直接随机拿个能用的端口 putRandomPort(name, port); //这一步很讲究，意味着相同协议使用相同的端口，要理解这个就需要先消化dubbo底层通信方式。 } logger.warn(&quot;Use random available port(&quot; + port + &quot;) for protocol &quot; + name); } Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); if (anyhost) { map.put(Constants.ANYHOST_KEY, &quot;true&quot;); } map.put(Constants.SIDE_KEY, Constants.PROVIDER_SIDE); map.put(Constants.DUBBO_VERSION_KEY, Version.getVersion()); map.put(Constants.TIMESTAMP_KEY, String.valueOf(System.currentTimeMillis())); if (ConfigUtils.getPid() &gt; 0) { map.put(Constants.PID_KEY, String.valueOf(ConfigUtils.getPid())); } //获取相关的配置参数用于后面的url生成，注意优先级顺序哟 appendParameters(map, application); appendParameters(map, module); appendParameters(map, provider, Constants.DEFAULT_KEY); appendParameters(map, protocolConfig); appendParameters(map, this); if (methods != null &amp;&amp; methods.size() &gt; 0) { for (MethodConfig method : methods) { appendParameters(map, method, method.getName()); //处理重试设置 String retryKey = method.getName() + &quot;.retry&quot;; if (map.containsKey(retryKey)) { String retryValue = map.remove(retryKey); if (&quot;false&quot;.equals(retryValue)) { map.put(method.getName() + &quot;.retries&quot;, &quot;0&quot;); } } List&lt;ArgumentConfig&gt; arguments = method.getArguments(); if (arguments != null &amp;&amp; arguments.size() &gt; 0) { for (ArgumentConfig argument : arguments) { //ArgumentConfig作用主要就是用来完成事件回调机制。 //类型自动转换. if(argument.getType() != null &amp;&amp; argument.getType().length() &gt;0){ Method[] methods = interfaceClass.getMethods(); //遍历所有方法 if(methods != null &amp;&amp; methods.length &gt; 0){ for (int i = 0; i &lt; methods.length; i++) { String methodName = methods[i].getName(); //匹配方法名称，获取方法签名. if(methodName.equals(method.getName())){ //注意方法重载情况 Class&lt;?&gt;[] argtypes = methods[i].getParameterTypes(); //一个方法中单个callback if (argument.getIndex() != -1 ){ //todo 这部分和文档写的有出入，不过这也不是第一次了。。 if (argtypes[argument.getIndex()].getName().equals(argument.getType())){ appendParameters(map, argument, method.getName() + &quot;.&quot; + argument.getIndex()); }else { throw new IllegalArgumentException(&quot;argument config error : the index attribute and type attirbute not match :index :&quot;+argument.getIndex() + &quot;, type:&quot; + argument.getType()); } } else { //一个方法中多个callback for (int j = 0 ;j&lt;argtypes.length ;j++) { //todo 这部分和文档写的有出入，不过这也不是第一次了。。 Class&lt;?&gt; argclazz = argtypes[j]; if (argclazz.getName().equals(argument.getType())){ appendParameters(map, argument, method.getName() + &quot;.&quot; + j); if (argument.getIndex() != -1 &amp;&amp; argument.getIndex() != j){ throw new IllegalArgumentException(&quot;argument config error : the index attribute and type attirbute not match :index :&quot;+argument.getIndex() + &quot;, type:&quot; + argument.getType()); } } } } } } } }else if(argument.getIndex() != -1){ appendParameters(map, argument, method.getName() + &quot;.&quot; + argument.getIndex()); }else { throw new IllegalArgumentException(&quot;argument config must set index or type attribute.eg: &lt;dubbo:argument index=&apos;0&apos; .../&gt; or &lt;dubbo:argument type=xxx .../&gt;&quot;); } } } } // end of methods for } if (ProtocolUtils.isGeneric(generic)) { //处理泛化 map.put(&quot;generic&quot;, generic); map.put(&quot;methods&quot;, Constants.ANY_VALUE); } else { String revision = Version.getVersion(interfaceClass, version); if (revision != null &amp;&amp; revision.length() &gt; 0) { map.put(&quot;revision&quot;, revision); //todo 为什么是revision? } String[] methods = Wrapper.getWrapper(interfaceClass).getMethodNames(); //todo 动态封装interfaceClass，目前不知道干啥用，猜测dubbo直接操作的都是这个封装后的wrapper if(methods.length == 0) { logger.warn(&quot;NO method found in service interface &quot; + interfaceClass.getName()); map.put(&quot;methods&quot;, Constants.ANY_VALUE); } else { map.put(&quot;methods&quot;, StringUtils.join(new HashSet&lt;String&gt;(Arrays.asList(methods)), &quot;,&quot;)); } } //令牌验证，为空表示不开启，如果为true，表示随机生成动态令牌，否则使用静态令牌，令牌的作用是防止消费者绕过注册中心直接访问，保证注册中心的授权功能有效，如果使用点对点调用，需关闭令牌功能 if (! ConfigUtils.isEmpty(token)) { if (ConfigUtils.isDefault(token)) { map.put(&quot;token&quot;, UUID.randomUUID().toString()); } else { map.put(&quot;token&quot;, token); } } //injvm表示不会跨进程，所以不需要注册中心 if (&quot;injvm&quot;.equals(protocolConfig.getName())) { protocolConfig.setRegister(false); map.put(&quot;notify&quot;, &quot;false&quot;); } // 导出服务 String contextPath = protocolConfig.getContextpath(); if ((contextPath == null || contextPath.length() == 0) &amp;&amp; provider != null) { contextPath = provider.getContextpath(); } URL url = new URL(name, host, port, (contextPath == null || contextPath.length() == 0 ? &quot;&quot; : contextPath + &quot;/&quot;) + path, map); //拿到服务的url if (ExtensionLoader.getExtensionLoader(ConfiguratorFactory.class) .hasExtension(url.getProtocol())) { url = ExtensionLoader.getExtensionLoader(ConfiguratorFactory.class) .getExtension(url.getProtocol()).getConfigurator(url).configure(url); } String scope = url.getParameter(Constants.SCOPE_KEY); //配置为none不暴露 if (! Constants.SCOPE_NONE.toString().equalsIgnoreCase(scope)) { //配置不是remote的情况下做本地暴露 (配置为remote，则表示只暴露远程服务) if (!Constants.SCOPE_REMOTE.toString().equalsIgnoreCase(scope)) { exportLocal(url); } //如果配置不是local则暴露为远程服务.(配置为local，则表示只暴露远程服务) if (! Constants.SCOPE_LOCAL.toString().equalsIgnoreCase(scope) ){ if (logger.isInfoEnabled()) { logger.info(&quot;Export dubbo service &quot; + interfaceClass.getName() + &quot; to url &quot; + url); } if (registryURLs != null &amp;&amp; registryURLs.size() &gt; 0 &amp;&amp; url.getParameter(&quot;register&quot;, true)) { for (URL registryURL : registryURLs) { url = url.addParameterIfAbsent(&quot;dynamic&quot;, registryURL.getParameter(&quot;dynamic&quot;)); URL monitorUrl = loadMonitor(registryURL); if (monitorUrl != null) { url = url.addParameterAndEncoded(Constants.MONITOR_KEY, monitorUrl.toFullString()); } if (logger.isInfoEnabled()) { logger.info(&quot;Register dubbo service &quot; + interfaceClass.getName() + &quot; url &quot; + url + &quot; to registry &quot; + registryURL); } //todo 暴露为何要封装一层代理呢？ Invoker&lt;?&gt; invoker = proxyFactory.getInvoker(ref, (Class) interfaceClass, registryURL.addParameterAndEncoded(Constants.EXPORT_KEY, url.toFullString())); Exporter&lt;?&gt; exporter = protocol.export(invoker); exporters.add(exporter); } } else { //todo 暴露为何要封装一层代理呢？ Invoker&lt;?&gt; invoker = proxyFactory.getInvoker(ref, (Class) interfaceClass, url); Exporter&lt;?&gt; exporter = protocol.export(invoker); exporters.add(exporter); } } } this.urls.add(url); } 一路走来可以发现，dubbo会为每个有效协议暴露一份服务，并且会注册到所有有效的注册中心里。而bean转变为service中最重要的就是映射出来的URL，也就是说我们在配置文件中进行的相关配置都会映射成对应url中的相关部分，举个例子： &lt;dubbo:application name=&quot;demo-provider&quot; owner=&quot;programmer&quot; organization=&quot;dubbox&quot;/&gt; &lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot;/&gt; &lt;dubbo:protocol name=&quot;dubbo&quot; serialization=&quot;kryo&quot; optimizer=&quot;com.alibaba.dubbo.demo.SerializationOptimizerImpl&quot;/&gt; &lt;bean id=&quot;bidService&quot; class=&quot;com.alibaba.dubbo.demo.bid.BidServiceImpl&quot; /&gt; &lt;dubbo:service interface=&quot;com.alibaba.dubbo.demo.bid.BidService&quot; ref=&quot;bidService&quot; protocol=&quot;dubbo&quot; /&gt; 我们通过debug看一下最终它映射出来的url是什么： //exportLocal injvm://127.0.0.1/com.alibaba.dubbo.demo.bid.BidService?anyhost=true&amp;application=demo-provider&amp;dubbo=2.0.0&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.bid.BidService&amp;methods=throwNPE,bid&amp;optimizer=com.alibaba.dubbo.demo.SerializationOptimizerImpl&amp;organization=dubbox&amp;owner=programmer&amp;pid=3872&amp;serialization=kryo&amp;side=provider&amp;timestamp=1422241023451 //exportRemote registry://127.0.0.1:2181/com.alibaba.dubbo.registry.RegistryService?application=demo-provider&amp;dubbo=2.0.0&amp;export=dubbo%3A%2F%2F192.168.153.1%3A20880%2Fcom.alibaba.dubbo.demo.bid.BidService%3Fanyhost%3Dtrue%26application%3Ddemo-provider%26dubbo%3D2.0.0%26generic%3Dfalse%26interface%3Dcom.alibaba.dubbo.demo.bid.BidService%26methods%3DthrowNPE%2Cbid%26optimizer%3Dcom.alibaba.dubbo.demo.SerializationOptimizerImpl%26organization%3Ddubbox%26owner%3Dprogrammer%26pid%3D3872%26serialization%3Dkryo%26side%3Dprovider%26timestamp%3D1422241023451&amp;organization=dubbox&amp;owner=programmer&amp;pid=3872&amp;registry=zookeeper&amp;timestamp=1422240274186 那么这个url的作用是什么呢？官方给出的解释很明确，这个url作为解耦的通信数据（跨层调用的参数），有了它dubbo就可以更容易做到业务逻辑实现的替换。除此之外可以看到url中还包含了大量的辅助参数（例如：timeout，version，organization等）供服务治理使用，这些都是根据真实需求一步一步补充完善的，可见，好的架构是演化而来的。 可以看到贴出来的代码中包含很多todo项，其中一些问题我们从代码层面是很难找到答案的，我们需要上升到业务，运维甚至架构师高度才能消化得了，小弟将在后续的分析中慢慢的尝试解开谜团。","tags":[{"name":"spring","slug":"spring","permalink":"https://blog.kazaff.me/tags/spring/"},{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"dubbox","slug":"dubbox","permalink":"https://blog.kazaff.me/tags/dubbox/"}]},{"title":"Rsync和inotify配置","date":"2015-01-26T16:53:12.000Z","path":"2015/01/26/rsync和inotify配置/","text":"两台服务器之间需要进行文件的主从同步，常用的方案是rsync+inotify，网上可以看到一大把相关的资料。可我在按照前辈说的方法配置时却遇到了一些问题，首先是因为自己的懒惰，直接从网页上把shell脚本拷贝到win系统下的编辑器中，然后再放入cecntos中，结果发现执行报错，是那种很诡异的提示： syntax error: unexpected end of file 只需要在centos中用vi手动key一遍脚本即可，这个错误应该是有win-&gt;linux时编码不同导致的。 接下来脚本执行成功了，可每次同步时都会报错，提示无法连接从机地址，这是由于防火墙没有放行对应端口导致的，简单的方法就是关闭防火墙： service iptables stop 最后我又发现，除了mv命令以外其他命令都可以触发同步，唯独mv不行，查了一下发现是上面前辈提供的脚本并没有监控mv对应的事件导致的，详情可见这篇博文。 知道原因了就很简答了，把最终脚本改成： #!/bin/bash host=192.168.76.135 src=/www/ des=web user=kazaff /usr/local/inotify/bin/inotifywait -mrq --timefmt &apos;%d/%m/%y %H:%M&apos; --format &apos;%T %w%f%e&apos; -e modify,delete,create,attrib,moved_to $src | while read files do /usr/local/rsync/bin/rsync -vzrtopg --delete --progress --password-file=/usr/local/rsync/rsync.passwd $src $user@$host::$des &gt;&gt;dev/null 2&gt;&gt;/tmp/rsync.log done","tags":[{"name":"rsync","slug":"rsync","permalink":"https://blog.kazaff.me/tags/rsync/"},{"name":"inotify","slug":"inotify","permalink":"https://blog.kazaff.me/tags/inotify/"},{"name":"文件同步","slug":"文件同步","permalink":"https://blog.kazaff.me/tags/文件同步/"}]},{"title":"Twemproxy安装问题与不支持的操作明细","date":"2015-01-26T14:53:12.000Z","path":"2015/01/26/twemproxy安装问题与不支持的操作明细/","text":"redis已经成为我们项目中不可或缺的一部分，充当了各种业务数据的持久化：用户会话，部分数据关系，缓存等。那么，保证redis的高可用高扩展性自然成了一个不可忽视的目标。值得庆幸的是业界已经有大量的成功案例用来驾驭redis，我们只需要拿来主义即可。今天要聊的就是一款普世的redis中间代理：twemproxy。相关内容可以在网上找到很多，这里肯定没必要重复了，我就先来说一下本人安装twemproxy时遇到的问题吧。 先安装git： yum install git 这个没啥好说的，接下来我们来下载twemproxy源文件： cd /usr/local git clone https://github.com/twitter/twemproxy.git cd twemproxy 第一次编译： autoreconf -fvi 结果提示autoconf版本太低，我们只能更新autoconf。 第二次编译，依然不顺利，报错如下： autoreconf: Entering directory `.&apos; autoreconf: configure.ac: not using Gettext autoreconf: running: aclocal --force -I m4 Can&apos;t exec xxxxxxx //忘记了，不过不重要 查了一下，原因是由于我们手动安装的autoconf，导致其他两个必要的依赖包缺失，依照顺序安装下面两个包： automake-1.12.tar.gz： ftp://ftp.gnu.org/gnu/automake/automake-1.12.tar.xz libtool-2.2.4.tar.gz 一切就绪了，可以顺利安装了。 最后需要提醒你看一下twemproxy不支持的命令集合，把自己项目中与redis相关的逻辑都过滤一遍，避免执行失败。","tags":[{"name":"redis","slug":"redis","permalink":"https://blog.kazaff.me/tags/redis/"},{"name":"twemproxy","slug":"twemproxy","permalink":"https://blog.kazaff.me/tags/twemproxy/"},{"name":"可伸缩","slug":"可伸缩","permalink":"https://blog.kazaff.me/tags/可伸缩/"}]},{"title":"dubbo中SPI的基础--Cooma微容器","date":"2015-01-15T15:54:30.000Z","path":"2015/01/15/dubbo中SPI的基础--Cooma微容器/","text":"断断续续研究dubbo其实有一段时间了，总是搁浅的原因和工作安排有很大关系，不过我始终保持着学习它的兴趣。随着dubbox项目的更新，我决定再一次尝试学习这个项目。之前把玩的时候自己甚至连一本完整的javaEE资料都没有看完过，虽然现在也还是入门级新手，不过在其他项目中的工作也让我有了比之前更多的积累，应该可以看得更透一些。 从哪里开始呢？我个人认为应该从dubbo的SPI理念开始，由于dubbo的项目Leader一开始就把其定义成一个方便扩展的服务框架，所以在dubbo的架构设计中始终保持了良好的依赖扩展机制：微内核+插件。简而言之就是让扩展者可以和项目开发者拥有一样的灵活度，这也是dubbo得以迅速流行的一个必要条件。 要想实现这种自由度，除了在架构分层组件上要保持高内聚低耦合外，底层也需要一套强大的类管理工具。在javaEE世界里，把这份工作做到极致的也已经有成熟的标准规范：OSGi。不过OSGi并不完全适配dubbo的需求，而且这玩意儿也有些过于重了，所以在此基础上，dubbo结合JDK标准的SPI机制设计出来一个轻量级的实现：Cooma。 这篇文章，我就打算从Cooma说起，官方介绍的已经非常详细了，不过它在从dubbo独立出来发布之前是做过修改优化的，在dubbo项目中使用时可能会存在些许的不同，我们就从dubbo内部来研读这部分实现的代码，并结合dubbo中的上下文来了解一下dubbo是如何使用SPI的。 我们把目标定位在dubbo的这个包上： com.alibaba.dubbo.common.extension 看一下这个包的目录结构： com.alibaba.dubbo.common.extension | |--factory | |--AdaptiveExtensionFactory #稍后解释 | |--SpiExtensionFactory #稍后解释 | |--support | |--ActivateComparator | |--Activate #自动激活加载扩展的注解 |--Adaptive #自适应扩展点的注解 |--ExtensionFactory #扩展点对象生成工厂接口 |--ExtensionLoader #扩展点加载器，扩展点的查找，校验，加载等核心逻辑的实现类 |--SPI #扩展点注解 我们通过对照dubbo如何使用扩展点机制来完成扩展点工厂实例的选择与加载来了解一下扩展点实现的细节，这句话很拗口，有点递归的味道，我们不妨直接从代码中来理解： public class ExtensionLoader&lt;T&gt; { ... private static final ConcurrentMap&lt;Class&lt;?&gt;, ExtensionLoader&lt;?&gt;&gt; EXTENSION_LOADERS = new ConcurrentHashMap&lt;Class&lt;?&gt;, ExtensionLoader&lt;?&gt;&gt;(); private final Class&lt;?&gt; type; ... @SuppressWarnings(&quot;unchecked&quot;) public static &lt;T&gt; ExtensionLoader&lt;T&gt; getExtensionLoader(Class&lt;T&gt; type) { if (type == null) throw new IllegalArgumentException(&quot;Extension type == null&quot;); if(!type.isInterface()) { throw new IllegalArgumentException(&quot;Extension type(&quot; + type + &quot;) is not interface!&quot;); } if(!withExtensionAnnotation(type)) { throw new IllegalArgumentException(&quot;Extension type(&quot; + type + &quot;) is not extension, because WITHOUT @&quot; + SPI.class.getSimpleName() + &quot; Annotation!&quot;); } ExtensionLoader&lt;T&gt; loader = (ExtensionLoader&lt;T&gt;) EXTENSION_LOADERS.get(type); if (loader == null) { EXTENSION_LOADERS.putIfAbsent(type, new ExtensionLoader&lt;T&gt;(type)); loader = (ExtensionLoader&lt;T&gt;) EXTENSION_LOADERS.get(type); } return loader; } private ExtensionLoader(Class&lt;?&gt; type) { this.type = type; objectFactory = (type == ExtensionFactory.class ? null : ExtensionLoader.getExtensionLoader(ExtensionFactory.class).getAdaptiveExtension()); } ... } 我们主要看ExtensionLoader构造方法，其中它初始化了type和objectFactory，前者为要作为扩展点的接口类型，后者表示要如何获取指定名称的扩展点实例（工厂类），目前dubbo提供了2个实现类，上面在包结构图上已经标注过了。 @SuppressWarnings(&quot;unchecked&quot;) public T getAdaptiveExtension() { Object instance = cachedAdaptiveInstance.get(); if (instance == null) { if(createAdaptiveInstanceError == null) { synchronized (cachedAdaptiveInstance) { instance = cachedAdaptiveInstance.get(); if (instance == null) { try { instance = createAdaptiveExtension(); cachedAdaptiveInstance.set(instance); } catch (Throwable t) { createAdaptiveInstanceError = t; throw new IllegalStateException(&quot;fail to create adaptive instance: &quot; + t.toString(), t); } } } } else { throw new IllegalStateException(&quot;fail to create adaptive instance: &quot; + createAdaptiveInstanceError.toString(), createAdaptiveInstanceError); } } return (T) instance; } 上面这个方法是用来获取自适应扩展类实例的，但其实它只是封装了一层缓存而已，真正完成创建实例的是createAdaptiveExtension方法。由于调用关系太深，请看下面的图： 上图中给出的路径，缺少了查找扩展点实现的细节，也就是并没有展开getExtensionClasses方法，该方法会根据指定位置的配置文件扫描并解析拿到所有可用的扩展点实现，代码如下： private Map&lt;String, Class&lt;?&gt;&gt; getExtensionClasses() { Map&lt;String, Class&lt;?&gt;&gt; classes = cachedClasses.get(); if (classes == null) { synchronized (cachedClasses) { classes = cachedClasses.get(); if (classes == null) { classes = loadExtensionClasses(); cachedClasses.set(classes); } } } return classes; } 可见它也只是封装了一层缓存而已，我们继续深挖loadExtensionClasses方法： // 此方法已经getExtensionClasses方法同步过。 private Map&lt;String, Class&lt;?&gt;&gt; loadExtensionClasses() { //检查并获取该接口类型声明的默认扩展点实现 final SPI defaultAnnotation = type.getAnnotation(SPI.class); if(defaultAnnotation != null) { String value = defaultAnnotation.value(); if(value != null &amp;&amp; (value = value.trim()).length() &gt; 0) { String[] names = NAME_SEPARATOR.split(value); if(names.length &gt; 1) { throw new IllegalStateException(&quot;more than 1 default extension name on extension &quot; + type.getName() + &quot;: &quot; + Arrays.toString(names)); } if(names.length == 1) cachedDefaultName = names[0]; } } //去三个指定的位置查找配置文件并解析拿到扩展点键值映射关系 Map&lt;String, Class&lt;?&gt;&gt; extensionClasses = new HashMap&lt;String, Class&lt;?&gt;&gt;(); loadFile(extensionClasses, DUBBO_INTERNAL_DIRECTORY); loadFile(extensionClasses, DUBBO_DIRECTORY); loadFile(extensionClasses, SERVICES_DIRECTORY); return extensionClasses; } 我们先来看一下配置文件的格式： adaptive=com.alibaba.dubbo.common.extension.factory.AdaptiveExtensionFactory spi=com.alibaba.dubbo.common.extension.factory.SpiExtensionFactory loadFile方法会从指定位置（META-INF/dubbo/internal/）根据指定接口类型（type）为文件名称查找目标配置文件，然后解析并校验，最终拿到匹配的扩展点类的所有Class实例。对应上面给出的配置文件，也就是AdaptiveExtensionFactory和SpiExtensionFactory，它们已经在包结构图上提到过了。 现在我们来着重看一下这两个类，它们到底是做什么用的呢？首先，AdaptiveExtensionFactory定义上有@Adaptive注解标识，很明显，它就是自适应扩展点的实现，从loadFile方法中可以留意到：同一个接口类型只能存在一个自适应扩展点实现： @Adaptive public class AdaptiveExtensionFactory implements ExtensionFactory { private final List&lt;ExtensionFactory&gt; factories; public AdaptiveExtensionFactory() { ExtensionLoader&lt;ExtensionFactory&gt; loader = ExtensionLoader.getExtensionLoader(ExtensionFactory.class); List&lt;ExtensionFactory&gt; list = new ArrayList&lt;ExtensionFactory&gt;(); for (String name : loader.getSupportedExtensions()) { list.add(loader.getExtension(name)); } factories = Collections.unmodifiableList(list); } //从这个方法定义来看，这个自适应扩展点实现类并没有做任何事儿，唯一的工作就是把真正获取扩展点实例的逻辑依次交给 //框架中声明的所有ExtensionFactory扩展点实例，默认也就是SpiExtensionFactory public &lt;T&gt; T getExtension(Class&lt;T&gt; type, String name) { for (ExtensionFactory factory : factories) { T extension = factory.getExtension(type, name); if (extension != null) { return extension; } } return null; } } 可以看到，AdaptiveExtensionFactory把逻辑委托给SpiExtensionFactory来做，而后者又是怎么做的呢： public class SpiExtensionFactory implements ExtensionFactory { public &lt;T&gt; T getExtension(Class&lt;T&gt; type, String name) { if (type.isInterface() &amp;&amp; type.isAnnotationPresent(SPI.class)) { ExtensionLoader&lt;T&gt; loader = ExtensionLoader.getExtensionLoader(type); if (loader.getSupportedExtensions().size() &gt; 0) { //获取自适应扩展点实例，这是dubbo默认的行为， //也可以自己写一个ExtensionFactory来按照要求加载扩展点 return loader.getAdaptiveExtension(); } } return null; } } 而objectFactory（真实工作的也就是SpiExtensionFactory.getExtension）只是用在ExtendLoader的注入方法（injectExtension）中，该方法用于为选定的扩展点实现注入相关的其他扩展点实例。 目前为止，我们已经大概了解在dubbo内部，是以什么样的规则来使用扩展点机制，也为以后学习dubbo的其它方面提供了基础。","tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"dubbox","slug":"dubbox","permalink":"https://blog.kazaff.me/tags/dubbox/"},{"name":"cooma","slug":"cooma","permalink":"https://blog.kazaff.me/tags/cooma/"},{"name":"微内核","slug":"微内核","permalink":"https://blog.kazaff.me/tags/微内核/"}]},{"title":"Jedis异常Could Not Get a Resource From the Pool","date":"2015-01-04T16:42:58.000Z","path":"2015/01/04/Jedis异常Could not get a resource from the pool/","text":"今天用jmeter对项目中的相关接口进行压力测试，发现一个问题： 严重: Servlet.service() for servlet [mvc-dispatcher] in context with path [] threw exceptionjava.io.IOException: org.springframework.web.util.NestedServletException: Request processing failed; nested exception is org.springframework.data.redis.RedisConnectionFailureException: Cannot get Jedis connection; nested exception is redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool 字面意思来看，应该是redis连接池没有可申请的连接了。当然，在低并发测试时是不会看到这个报错的，大概在我把并发数调到200左右时会狂报这个错误！ 根据网上前辈们的总结，无非就是调整Jedis连接池的配置，redis本身的连接数的配置等等。这里我就不再复述了，我只想说，按照这些指示做了一边后问题并没任何好转。 我用的是工作机，操作系统是WIN7 64位，redis用的是2.8.3 win64版，jedis的版本是2.6.1（网上说2.1.0之前的版本要手动归还连接），应用中操作redis的api是依赖spring-data封装的，配置如下： &lt;!-- 会话持久层redis配置 --&gt; &lt;bean id=&quot;poolConfig&quot; class=&quot;redis.clients.jedis.JedisPoolConfig&quot;&gt; &lt;property name=&quot;maxIdle&quot; value=&quot;${redis.maxIdle}&quot; /&gt; &lt;property name=&quot;maxTotal&quot; value=&quot;${redis.maxTotal}&quot; /&gt; &lt;property name=&quot;maxWaitMillis&quot; value=&quot;${redis.maxWaitMillis}&quot; /&gt; &lt;property name=&quot;testOnBorrow&quot; value=&quot;${redis.testOnBorrow}&quot; /&gt; &lt;property name=&quot;testOnReturn&quot; value=&quot;${redis.testOnReturn}&quot; /&gt; &lt;/bean&gt; &lt;bean id=&quot;jedisConnFactory&quot; class=&quot;org.springframework.data.redis.connection.jedis.JedisConnectionFactory&quot;&gt; &lt;property name=&quot;hostName&quot; value=&quot;${redis.host}&quot; /&gt; &lt;property name=&quot;port&quot; value=&quot;${redis.port}&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;${redis.password}&quot; /&gt; &lt;property name=&quot;usePool&quot; value=&quot;true&quot; /&gt; &lt;property name=&quot;poolConfig&quot; ref=&quot;poolConfig&quot; /&gt; &lt;property name=&quot;database&quot; value=&quot;${redis.db}&quot; /&gt; &lt;/bean&gt; &lt;bean id=&quot;redisTemplate&quot; class=&quot;org.springframework.data.redis.core.RedisTemplate&quot;&gt; &lt;property name=&quot;connectionFactory&quot; ref=&quot;jedisConnFactory&quot; /&gt; &lt;property name=&quot;keySerializer&quot;&gt; &lt;bean class=&quot;org.springframework.data.redis.serializer.StringRedisSerializer&quot; /&gt; &lt;/property&gt; &lt;property name=&quot;valueSerializer&quot;&gt; &lt;bean class=&quot;org.springframework.data.redis.serializer.JdkSerializationRedisSerializer&quot; /&gt; &lt;/property&gt; &lt;/bean&gt; redis连接池相关的参数如下： #redis配置 redis.maxIdle=100 redis.maxTotal=1000 redis.maxWaitMillis=2000 redis.testOnBorrow=true redis.testOnReturn=true 中规中矩，而且按照我配置的相关参数，设置的并发500绝逼不应该报这个错误的！其中一个细节是，当应用狂报错时，redis-cli命令也无法连接到redis服务了，也就是说redis本身的配置根本就不匹配我在应用中连接池的配置，这应该是导致问题的根本原因，简单地说就是： redis本身的连接数上限必须大于应用连接池中的最大连接数 可本尊看了一下redis.conf文件，其中maxClients的值为0，也就是不限制。这尼玛如何是好？ 突然想到多年之前，小弟我刚接触redis的时候，就看到过一个大牛曾经说过：redis的win版本基本上就是个玩具。 抱着试一试的心态（我也购买了两盒…），我试着让应用直连centos中的redis服务，尼玛直接就解决了，或者说出现的情况至少和配置相吻合，而且在linux下redis-server在启动之初就会提醒你关于连接数的配置，很贴心。 好吧，最好再延伸一个老知识点，如果你想把redis的连接数设置的很大，很可能会超过目前配置的系统可使用文件句柄数上限，这时你的配置就会失效，那如何突破这个瓶颈呢，你需要了解一下这个命令： ulimit。 PS：redis在linux上跑起来，就像吃了枪药，打了鸡血，磕了军粮丸一样，根本停不下来啊！","tags":[{"name":"jedis","slug":"jedis","permalink":"https://blog.kazaff.me/tags/jedis/"},{"name":"redis","slug":"redis","permalink":"https://blog.kazaff.me/tags/redis/"},{"name":"连接池","slug":"连接池","permalink":"https://blog.kazaff.me/tags/连接池/"},{"name":"jmeter","slug":"jmeter","permalink":"https://blog.kazaff.me/tags/jmeter/"},{"name":"ulimit","slug":"ulimit","permalink":"https://blog.kazaff.me/tags/ulimit/"}]},{"title":"在无图形界面的Centos6.5下使用OpenOffice4.1","date":"2014-12-24T11:37:12.000Z","path":"2014/12/24/在无图形界面的Centos6.5下使用OpenOfiice4.1/","text":"这两天发生了一件尴尬的事儿，倾听我娓娓道来：我在开发机上（Win7）上写了一段java代码，使用jodconverter类调用OpenOffice来将office格式的文件转换成pdf。写完以后第一件事儿就是去虚拟机中的CentOS中测试兼容性，一切比想象中的还要顺利！事儿如果就这么完了，那就没有这篇日志了，昨天交给同事往线上服务器上部署，可他说有异常，提示无法连接openoffice服务。我就说嘛，事儿不可能这么顺利的！ 网上查了一下原因，苦逼了：OpenOffice安装声明。 X-Server with 1024 x 768 pixel or higher resolution with at least 256 colors (16.7 million colors recommended) 图形界面沃特华克，还需要X-Server啊，线上服务器上是没有装图形界面的，难道需要在为系统安装一套图形界面么？这勾起了我大学时代的残酷回忆，曾经给Ubuntu安装图形界面搞崩系统N多次！我怎么可能有胆子直接给线上服务器上搞这个动作呢？ 可是不搞又不行，领导在后面催得要死要活，只能另想办法了！最坏的打算就是给服务器重装系统，所以拜托运维同学先把服务器上的系统进行了备份，然后找机房负责人协商装系统的事儿，不过在此期间，我还是在本地虚拟机装了一套测试环境来尝试解决这个问题。 安装jdk和openoffice的流程我就不写了，我们直接从安装Xvfb开始。 yum install xorg-x11-server-Xvfb 挺顺利的，一次性搞定，不过还是建议先执行yum update一下。这个东西我理解的就是相当于给系统装了一套虚拟的图形界面，这样就满足了openoffice的要求。 安装完后，执行： Xvfb :1 -screen 0 800x600x24 &amp; soffice -headless -accept=&quot;socket,host=127.0.0.1,port=8100;urp;&quot; -nofirstartwizard -display :1 &amp; 执行完上面第一条命令后，系统提示了一个报错： ...... Initializing built-in extension GLX The XKEYBOARD keymap compiler (xkbcomp) reports: &gt; Internal error: Could not resolve keysym XF86AudioMicMute Errors from xkbcomp are not fatal to the X server 不过提示的好像不是致命错误，所以暂时没有理会它。需要注意的是，虽然OpenOffice提示需要的是1027x768x256的图形分辨率，但是由于我们的服务器配置问题达不到这个标准，但我实际测试填写800x600x24也是完全没问题的！ 其实上面第二条命令也可以忽略，如果你用的是jodconverter3.0+的话！ 现在我们已经可以成功的启动openoffice服务了，我写的那个服务也成功的进行了pdf的生成。 中文字体可别高兴太早，打开生成的pdf一看，妈蛋，无字经书么？你拿我当唐僧么？这个问题其实也很好理解，系统里没有中文嘛，好说好说，把C:\\Windows\\Fonts下所有的字体文件都打包传到服务器上，放在你安装的openoffice的文件夹中，我的是这里： /opt/openoffice4/share/fonts/truetype/ 不过需要注意的是，放字体的时候最好是先让刚才开启的openoffice服务停止，如果忘记进程id了，可以执行： netstat -lnp | grep 8100 netstat -lnp | grep 2002 //这个是jodconverter3.0+默认使用的端口号 直接kill掉就行了。 ok，完美了，终于可以不用重装系统就搞定问题了！！！","tags":[{"name":"pdf","slug":"pdf","permalink":"https://blog.kazaff.me/tags/pdf/"},{"name":"centOS","slug":"centOS","permalink":"https://blog.kazaff.me/tags/centOS/"},{"name":"openoffice","slug":"openoffice","permalink":"https://blog.kazaff.me/tags/openoffice/"},{"name":"jodconverter","slug":"jodconverter","permalink":"https://blog.kazaff.me/tags/jodconverter/"},{"name":"中文","slug":"中文","permalink":"https://blog.kazaff.me/tags/中文/"},{"name":"图形界面","slug":"图形界面","permalink":"https://blog.kazaff.me/tags/图形界面/"}]},{"title":"如何把Java项目打包为可执行的jar","date":"2014-12-15T09:37:12.000Z","path":"2014/12/15/如何把java项目打包为可执行的jar/","text":"这几天用java写了一个后台服务，就是把office文件转换成pdf。实现挺简单的，直接调用openoffice服务即可，大致做法网上有很多教程，这里就不多扯了，今天主要是说一下项目打包的事儿。 用Idea打包前段时间是直接用IDE提供的支持把java Web项目打包成war。一切都是那么的顺利，可现在想要的是直接把项目打包成jar，而且需要的是直接在终端中输入java -jar xxxx.jar即可运行！ 一开始也打算尝试使用IDE提供的打包方式，可总是失败，jar是生成了，不过运行时总提示类缺失，明明已经把所有依赖的第三方jar包都copy了一份，可死活就是找不到，十分怀疑是MANIFEST.MF文件的配置问题。 总之使用了几种网上找的方式都出现各种问题，所以暂时放弃了Idea打包jar的想法，有经验的童鞋可以来这里捧场，一经测试，立马送分哟~。 用Maven打包要说吧，这算是大家最认可的方式，不过maven打包也有好多种插件，这让我这种新手很是头疼。 方案一： &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;me.kazaff.Main&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 这会将所有依赖的包解压后重新和自己的项目包合并打入jar包中，至于这么做的优劣可以留给你自己去体会，这么做的理由我猜应该和jar包查找classpatch有关系吧，否则为什么要这么奇葩呢？ 方案二： &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;mainClass&gt;me.kazaff.Main&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;./target/lib&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 这种方式相对方案一，似乎好了那么一些，它会把你的项目打成一个独立的jar包，并在jar包所在的目录中建立一个lib文件夹，并把所有三方依赖jar包拷贝进去，这还不算完，生成的那个jar包中的MANIFEST.MF会配置好Class-Path属性。 需要注意的是，如果你想直接在终端中运行该jar包，必须保证终端进入到项目jar包所在的目录下，否则相对路径会导致依赖的三方jar包找不到哟~~ 好吧，如果你像我一样记性不好，又需要将打包的jar文件拷贝到其他机器上部署的话，很容易忘记lib文件夹！这里有一个进阶的打包方法：传送门。 方案三： 那有没有一种方法，可以让我们把项目真正的打包成单一可执行的jar包呢？而且这个jar包中的结构不会像方案一那样混乱，而是把所有第三方依赖包以jar包的方式存储。这难道成了奢望么？ 油腻的湿姐并非我一个人在寻找：队友。不过，按照他的最后一个办法打包出来的jar是无法直接执行的，因为其中MANIFEST.MF缺少Main-Class和Class-Path属性，他的这种方式如果是打包成一个非执行jar包的话应该还是不错的。 所以，如你所见，方案三并不存在，至少我不知道应该怎么做！有办法的朋友可以在文章下面留言，不胜感激！ java读取jar包中的资源文件打包后发现原先在代码中直接通过路径读取的配置文件再也找不到了，可是打开jar包明明看得到啊！ 查了一下资料，才知道到底发生了什么，看这里：传送门，传送门2。","tags":[{"name":"打包","slug":"打包","permalink":"https://blog.kazaff.me/tags/打包/"},{"name":"jar","slug":"jar","permalink":"https://blog.kazaff.me/tags/jar/"},{"name":"idea","slug":"idea","permalink":"https://blog.kazaff.me/tags/idea/"},{"name":"maven","slug":"maven","permalink":"https://blog.kazaff.me/tags/maven/"},{"name":"MANIFEST.MF","slug":"MANIFEST-MF","permalink":"https://blog.kazaff.me/tags/MANIFEST-MF/"},{"name":"资源","slug":"资源","permalink":"https://blog.kazaff.me/tags/资源/"}]},{"title":"No WebApplicationContext Found","date":"2014-12-09T11:13:12.000Z","path":"2014/12/09/No WebApplicationContext found no ContextLoaderListener registered/","text":"No WebApplicationContext found: no ContextLoaderListener registered? 上面个异常信息可把我给坑苦了，整整一下午啊，外加一晚上。其实吧，问题不在于人家这个报错，你瞅瞅，多体贴，甚至都加上了人性化的提醒，相对其他异常信息来说，这已经很友好了不是么？先来简单说一下问题场景吧：我的目的很直白，就是想在Servlet的Filter中使用Spring的Ioc注入指定的Bean。 怎么样，直白的都要哭了！ 对于我这种新手，确实一开始是不知道Filter中是无法使用spring的依赖注入的，原因很简单，因为Filter并不受spring管理，自然无法使用Ioc了！不过，我又不能直接“new”想用的bean，因为最终我的bean会使用依赖注入相关的资源！ 要说这种需求一点儿都不偏门，gg上一搜一大把：苦逼版，二逼版。 苦逼版其实也挺好的，简单明了，不过对我这种强迫症患者来说，总感觉难受，可能是由于手动调用getBean()的缘故吧～我也不清楚，反正难受！ 着重来说说这个二逼版，DelegatingFilterProxy这个类就是spring专门针对我们的问题场景量身定做的一个现成的实现版本，其实TA内部会帮我们调用苦逼版中的相关代码，有兴趣的同学强烈推荐看看源码。 我就是由于白天浮躁的环境让我一不小心上了鬼子的当！我其实是一个温文尔雅的人，真真儿的。但我现在杀人的心都有！如果你点开了我上面提供的二逼版链接，那你留意一下TA的这句话： (1) contextAttribute，使用委派Bean的范围，其值必须从org.springframework.context.ApplicationContext.WebApplicationContext中取得，默认值是session； 卧槽，写的真尼玛专业啊，我都信了！原因是因为我的场景恰恰是需要设置注入进来的Bean的生命周期为request的，我就理所应当的认为TA说的木牛错，我还傻不拉唧的花了一下午的时间把”singleton”、“session”、“request”等使了个遍！ 片头那个错误信息折磨了我几个小时，作者你造么？你能别吓唬乱写么？ 哥这性子就是倔，回到家游戏都没玩，电影也没看，美剧缓冲着也不管了，心想劳资这么正儿八经的一个需求，怎么可能Spring就不能满足呢？ 我又写了个简单的测试专门来排查问题，然后又去看源码！可算让我知道原因了！就是上面的那句扯犊子的话给我坑的！ contextAttribute属性是在调用WebApplicationContextUtils.getWebApplicationContext(getServletContext(), attrName)的时候当第二个参数用的，追到WebApplicationContextUtils类去查源码，你就知道世界满满的恶意了！ 哥为了真理，网吧包宿的钱都准备好了，作者你造么？ 那么这个属性到底做甚的？看这里： attrName the name of the ServletContext attribute to look for 就这吧，我还能说点啥，妈蛋，哥再也不相信你(http://my.csdn.net/geloin)了。","tags":[{"name":"spring","slug":"spring","permalink":"https://blog.kazaff.me/tags/spring/"},{"name":"ioc","slug":"ioc","permalink":"https://blog.kazaff.me/tags/ioc/"},{"name":"filter","slug":"filter","permalink":"https://blog.kazaff.me/tags/filter/"},{"name":"servlet","slug":"servlet","permalink":"https://blog.kazaff.me/tags/servlet/"},{"name":"DelegatingFilterProxy","slug":"DelegatingFilterProxy","permalink":"https://blog.kazaff.me/tags/DelegatingFilterProxy/"}]},{"title":"注解Scope(prototype)的正确用法","date":"2014-12-05T10:37:12.000Z","path":"2014/12/05/@Scope(prototype)的正确用法/","text":"昨天加班在搞一个关于session的问题，有兴趣的童鞋可以去这里观望一下。 总之除了吃惊外就是感觉手脚束缚！现在只能彻底放弃使用默认的session机制了，不知道这算是好事儿还是坏事儿。现在的思路是这样的： 在Servlet过滤器中把ServletRequest对象替换为自己的Wrapper实例，在其中实现getSession()方法，返回我们自定义的HttpSession对象 我们实现一个继承了HttpSession接口的自定义会话类，用于第一步的返回，该自定义会话类提供接受sessionId参数获取会话数据的方法 通过拿在request对象中获取的sessionId去redis中取得对应的登录用户的信息：sessionUser对象； 在每次请求处理完后（过滤器最后一行代码）让自定义会话类把会话数据回写入redis，由于要持久化对象到redis中，所以要选择一个高效能的序列化与反序列化实现。 当然也可以再进一步优化，例如说最后一步，检查如果MySession对象没有被修改则不需要回写等。 说到现在，貌似和这篇文章的主题没有半毛钱关系的说~~表着急嘛，这不是要开始说了嘛！ 上面的第三步提到，我们自定义的这个会话对象的生命周期应该为：每次请求。如果采用spring默认提供的singleton的话就乱套了，你懂的！ 在和同事讨论上面的自定义会话机制的实现时，他叮嘱我说： @Scope(&quot;request&quot;)作用域使用时要加上额外的参数（proxyMode） 不过并没有给我说清楚到底为啥。本着打破砂锅问到底的神经，我在GG和百度上搜索了一下，不知道是不是因为关键字写的不合理，总之并没有找到有用的中文资料，大多都是相互转载，而且讲的都是理论，并且用的也配置文件方式而非注解。 无奈只得翻墙查了一下，找到了一篇很好的文章，而且早在2010年就已经总结出来了，为啥国内社区的就没有呢？真的是我搜的方式不对么？ 不扯淡了，文章中说： Method Injection is useful in scenarios where you need to inject a smaller scope bean in a larger scope bean. For example, you have to inject a prototype bean inside an singleton bean , on each method invocation of Singleton bean. Just defining your bean prototype, does not create new instance each time a singleton bean is called because container creates a singleton bean only once, and thus only sets a prototype bean once. 大概意思是，当你把声明为相对范围小的作用域（例如：prototy）对象注入到相对范围大的作用域（例如：singleton）对象时，由于外层对象只会初始化一次，所以会导致内部注入的对象也只会被初始化一次。 道理十分的简单，就是因为这个原因，所以你有意把作用域定义为prototy的类可能并不是按照你认为的方式运作。 解决这个问题的方法有多种，比方说你可以使用Method Injection，而不是直接注入在类属性中，不过这就要求程序员在使用时需要特别对待这些类。 现在就是*proxyMode发挥作用的时候： @Scope(proxyMode = ScopedProxyMode.TARGET_CLASS, value = &quot;prototype&quot;) 这么写，就可以保证该类会按照你大脑中的方式使用了：每次使用都会重新初始化一个对象。 源码就不去深究了。8~","tags":[{"name":"会话","slug":"会话","permalink":"https://blog.kazaff.me/tags/会话/"},{"name":"session","slug":"session","permalink":"https://blog.kazaff.me/tags/session/"},{"name":"spring","slug":"spring","permalink":"https://blog.kazaff.me/tags/spring/"},{"name":"依赖注入","slug":"依赖注入","permalink":"https://blog.kazaff.me/tags/依赖注入/"},{"name":"作用域","slug":"作用域","permalink":"https://blog.kazaff.me/tags/作用域/"},{"name":"生命周期","slug":"生命周期","permalink":"https://blog.kazaff.me/tags/生命周期/"},{"name":"scope","slug":"scope","permalink":"https://blog.kazaff.me/tags/scope/"}]},{"title":"解决springMVC4下使用@ResponseBody的中文乱码问题","date":"2014-12-01T09:37:12.000Z","path":"2014/12/01/解决springMVC4下使用@ResponseBody的中文乱码/","text":"由于现在的项目一般都追求前后端分离，依靠Ajax进行通信，这样有助于团队分工、项目维护和后期的平台移植，这就使得后端框架对视图层的功能要求越来越低~今天要说的是基于SpringMVC开发web后端时，为了简单而直接在控制器方法中返回json字符串时碰到的中文乱码问题。算是非常基础的问题，大牛请绕道~ 其实我自己一开始也没觉得能有多复杂，认为一搜索就能找到一大把解决方案，所以没有计划耗费多久时间，更没打算转成写一篇博文记录过程。可不曾想到，足足花了我2个半小时，今天看来又要加班了！其实确实在GG和百度中搜索到了大量的相关解决方案，晒晒捡捡也发现至少有不下七八种解决方案。可悲剧的是统统在我测试下无效。 对于JAVAEE，我真真儿的是新手，项目也没有给我太多时间来深究源码，只能快速的试错，总算把几个方案拼凑出来一个能用的了！下面我就简单的说一下我的解决方法吧。 首先，我们要知道，为毛@ResponseBody不支持中文：传送门，这是我找到的写的最细的一篇文章了，尽管它并没有解决我的问题。 知道了原因，再来选择解决方案，一开始满心欢喜的找到一个最简单的方案： @RequestMapping(value = &quot;/add&quot;, produces = {&quot;application/json;charset=UTF-8&quot;}) 可是不管用撒，原因不明~~ 好吧，那再试一个： response.setContentType(&quot;text/plain;charset=UTF-8&quot;); 也不行哇，这个其实只是设置了响应头中的字符集，但是@ResponseBody最终还是会把字符以“ISO-8859-1”的方式输出，可恶！ 简单的方法木牛了，只能选择手动设置字符转换类了： &lt;bean class=&quot;org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter&quot; &gt; &lt;property name=&quot;messageConverters&quot;&gt; &lt;list&gt; &lt;bean class = &quot;org.springframework.http.converter.StringHttpMessageConverter&quot;&gt; &lt;property name = &quot;supportedMediaTypes&quot;&gt; &lt;list&gt; &lt;value&gt;text/html;charset=UTF-8&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; 注意，需要把这段放在xxx-servlet.xml中&lt;context:component-scan base-package=&quot;xxxxx&quot;/&gt;前面哦~ 其实这样已经可以解决了，不过不完美，留一下这个时候的响应头，你会发现体积非常大（Accept-Charset会达到4K+），这是因为默认情况下StringHttpMessageConverter.writeInternal()会将所有可用字符集回写到响应头中，这会消耗非常大的带宽！浪费可耻！ 一筹莫展了吧~幸好发现StringHttpMessageConverter提供的参数：writeAcceptCharset，所以最终的写法如下： &lt;!-- 用于使用@ResponseBody后返回中文避免乱码 --&gt; &lt;bean class=&quot;org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter&quot; &gt; &lt;property name=&quot;messageConverters&quot;&gt; &lt;list&gt; &lt;bean id=&quot;stringHttpMessageConverter&quot; class=&quot;org.springframework.http.converter.StringHttpMessageConverter&quot;&gt; &lt;property name=&quot;writeAcceptCharset&quot; value=&quot;false&quot; /&gt;&lt;!-- 用于避免响应头过大 --&gt; &lt;property name=&quot;supportedMediaTypes&quot;&gt; &lt;list&gt; &lt;value&gt;text/html;charset=UTF-8&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; 哎，两个多小时就干了点儿这，感觉好尴尬啊！！","tags":[{"name":"springmvc","slug":"springmvc","permalink":"https://blog.kazaff.me/tags/springmvc/"},{"name":"中文乱码","slug":"中文乱码","permalink":"https://blog.kazaff.me/tags/中文乱码/"}]},{"title":"聊聊大文件上传","date":"2014-11-14T09:37:12.000Z","path":"2014/11/14/聊聊大文件上传/","text":"早先我们在做网站的大文件上传时一般都是借助flash控件的，那个时代html5没有开放文件操作接口给js，而直接用表单提交大文件的话，以那个时候的带宽来上传百兆文件要花非常久的时间，我们无法保证用户能耐心等到上传完（在此期间，用户不能刷新页面，只能等~）。一旦用户的网络出现抖动，就可能导致前功尽弃，这些是不可接受的！那么做到什么样的程度才能够保证大文件上传的可用性呢？我在这里简单的列出来一些点： 支持断点续传 支持分块 支持多线程(*) 支持秒传(*) 支持显示上传进度 支持图片预览 支持暂停上传 拖拽上传 上面列出的这几点是我认为都需要实现的，带星号的我觉得算是加分项吧，毕竟像迅雷客户端这样的体验真的是太劲爆了。 接下来我们就根据上面列出的几点来分析，基于一款百度开源的前端上传控件：WebUploader（以下简称：WU）。 上传图片预览这一点其实在html5开放了本地文件操作接口后，就变得非常简单了。WU已经把这个功能封装起来了，用起来非常的简单，这里就不多说了。 不仅如此，WU还提供了更强大的功能，它可以在生成缩略图的时候进行图片压缩处理，具体可从API文档中查看到。 上传进度如果让用户等待十几分钟，不做任何提示肯定是非常不合适的，天知道是正在上传，还是死机了啊！这一点也是依靠HTML5的新特性做到的。不过在WU封装后，它就成了一个可监听的事件而已，这里也不多说了。 拖拽上传你可能会觉得这个功能没啥大用，但我却把它视为革命性的飞跃。不过在WU里实现起来只需要靠一个配置两个参数即可：dnd和disableGlobalDnd。 分块，暂停上传，多线程，断点续传其实只要支持了分块功能，像什么断点续传啊多线程啊就都有了！而借助html5提供给我们的文件API，分块也灰常的简单。我们就直接拿WU来说吧，它的配置中有以下几个参数： chunked chunkSize chunkRetry threads prepareNextFile var uploader = WebUploader.create({ swf: &quot;Uploader.swf&quot; , server: &quot;fileUpload.php&quot; , pick: &quot;#picker&quot; , resize: false , dnd: &quot;#theList&quot; , paste: document.body , disableGlobalDnd: true , thumb: { width: 100 , height: 100 , quality: 70 , allowMagnify: true , crop: true } , compress: false , prepareNextFile: true , chunked: true , chunkSize: 5000 * 1024 , threads: true , formData: userInfo , fileNumLimit: 1 , fileSingleSizeLimit: 1000 * 1024 * 1024 , duplicate: true }); 我把每个分片设置为5M大小，同时开启了多线程。这样设置以后，你就会发现上传被切分成多次请求了，如下图： 注意，每个分片的上传请求中都包含三个重要的标识信息： size：表示文件的总大小 chunks：表示分片的总个数 chunk：表示当前是第几个分片（分片下标从0开始） WU已经把该做的做完了，而且做得还很不错，那么它都做了什么： 文件拆分成指定大小的块 分块多线程发起上传请求 提供暂停功能 分块上传失败后重试 接下来就是我们后端代码要负责的了，我简单列一下后端要解决的问题点： 如何识别不同的分块是否属于同一个目标文件 何时把多分片合并成一个完整的文件 如何知道所有的分块全部传完完毕 如何避免合并大文件造成的内存占用 长期未上传完成的文件如何清理 如何从指定分块开始上传（续传，暂停后继续） 第一个问题要如何解决呢？如果能拿到每个文件的唯一标识的话，我们就可以通过这个标识来甄别分块的归属，至于这个唯一标识要根据哪些数据生成，这就要看你的系统的具体情况了，我这里简单的利用上传文件的相关信息和当前用户的id信息做md5签名作为这个标识： uniqueFileName = md5(&apos;&apos;+userInfo.userId+file.name+file.type+file.lastModifiedDate+file.size); 其中userInfo来自于用户登录后返回给前端框架的用户信息，其中userId即为用户的id。file其实是WU提供的一个对象，用来表示特定的上传文件，我们是怎么拿到这个对象的呢？ 这里需要简单的介绍一下WU提供的Hook：before-send-file。 文档中指明，它用来在文件发送之前触发，接受file为参数，这个对象就是我们要的！当然，其实也可以WU提供的其它相关事件的回调中获得这个对象，不过放在这里是为了更好的和后面讲的秒传配合。代码片段为： var userInfo = {userId:&quot;kazaff&quot;}; //模拟用户信息 var uniqueFileName = null; //为了避免每次分片都重新计算唯一标识，放在全局变量中，但如果你的项目需要WU支持多文件上传，那么这里你需要的是一个K/V结构，key可以存file.id WebUploader.Uploader.register({ &quot;before-send-file&quot;: &quot;beforeSendFile&quot; ...... }, { beforeSendFile: function(file){ ..... //拿到上传文件的唯一名称，用于断点续传 uniqueFileName = md5(&apos;&apos;+userInfo.userId+file.id+file.name+file.type+file.lastModifiedDate+file.size); ..... } ...... }); 上面的代码片段我省略了很多不相干的代码，不过放心最后我会把这个例子的完整代码放到github上的。 这样我们就有了这个唯一标识文件的戳，不过这里我们是在前端生成的，前端的这个戳主要是用于断点续传的，因为每个分片上传前我们都会让WU先发送一个请求来确认该分片是否已经上传过，如果后端告诉我们已经已经传过了，那么WU就会直接跳过该分片，这就实现了我们的断点续传，其实暂停继续和断点续传没啥两样，我们后面再说。当然，判断一个分片是否已经上传，只通过这个文件戳肯定不够，看下图： 请求中还提供了：分片索引和该分片的大小，这两个数据我们是怎么获取到的呢？依然是利用WU提供的Hook：before-send。这个Hook可以在分片发送之前触发，提供给我们block对象参数就是分片的相关信息，其中包括了我们要的两个数据，代码如下： ....... , beforeSend: function(block){ //分片验证是否已传过，用于断点续传 var task = new $.Deferred(); $.ajax({ type: &quot;POST&quot; , url: &quot;fileUpload.php&quot; , data: { type: &quot;chunkCheck&quot; , file: uniqueFileName , chunkIndex: block.chunk , size: block.end - block.start } , cache: false , timeout: 1000 //todo 超时的话，只能认为该分片未上传过 , dataType: &quot;json&quot; }).then(function(data, textStatus, jqXHR){ if(data.ifExist){ //若存在，返回失败给WebUploader，表明该分块不需要上传 task.reject(); }else{ task.resolve(); } }, function(jqXHR, textStatus, errorThrown){ //任何形式的验证失败，都触发重新上传 task.resolve(); }); return $.when(task); } ....... 我在这个Hook中发起了一个ajax请求，用于让后端告诉我们该分片是否需要上传~~ 细心的童鞋应该能注意到一个细节，这么设计也就会导致每个分片的上传可能要发送2次请求，一次用于分片验证，一次用于分片上传。这就需要我们设置一个合理的分片大小，用来避免一个大文件会向后端发起大量的请求，我的选择是5M~~ 做到这里，可以说WU已经做好了暂停继续，断点续传，多线程的全部工作，它们都是基于分块的。再进入后端前，我们先来分析一下断点续传和暂停继续的差别： 断点续传表示用户可能刷新了页面，或者甚至干脆重启了电脑，反正总之浏览器丢失了正在上传的文件的所有相关数据：文件路径，正在上传的分片索引，上传进度等信息。 暂停继续表示用户仍然在当前页面，只是主动触发暂停上传的动作，并在一定时间间隔后再次触发继续上传行为，期间浏览器依然记录着关于上传文件的相关信息。 除了上面提到的差别外，就没有其它了。也就是说这些差别仅位于浏览器，也就是前端，也就是WU，跟后端关系不大，或者说对于后端处理来说，这两者并没有什么区别，难道不是么？后端只需要回答指定分片是否已存在的问题即可，才不会管这次验证请求来自于断点续传还是暂停继续！！ ok，现在来说后端吧，刚才我们在前端创建文件的唯一标识，后端要怎么应对呢？当前端的分片验证请求或分片上传请求过来后，后端需要确定这个请求所对应的文件是哪个，而且要把在整个上传过程中生成的分片关联起来，用于合并时查找使用。方案很多，我这里选择一个不依赖数据库的。 后端在接受到分片上传请求后，会根据我们在前端创建文件标识的方法在后端也创建一个一模一样的标识，用这个标识创建一个文件夹，并把分片上传至这个文件夹中，这样所有同一个文件的分片都会保存在相同的文件夹中，如下图所示： 上图来自于某个大文件上传的过程中，以2e2a311b96d816b695fdf5817bd030f8命名的文件夹下的分片。 到此为止，我们解决了第一个问题（也为后面的问题做了铺垫）。 第二个问题涉及到合并这些分片的时机和方式，这里要明确一点，WU分片上传时会以分片的先后顺序上传，但由于网络因素，不保证0号分片一定先于1号分片上传完成，所以后端不能根据分片下标来判断上传是否完毕。 既然如此，我们只能在每个分片上传完毕后检查一下当前文件夹下的文件数是否与总分片数一致，以此来确定是否上传完毕，如果觉得这样做性能不满意，也可以借助WU的Hook：after-send-file，这样在所有分片上传完毕后，让WU在该Hook中发送一个ajax请求告诉后端可以合并了，并拿到合并后的目标文件相关信息（路径）。这里我选择了前者。 以上段落需更正，见文章末尾的[更正1]。 至于合并的方式，我们只需要避免大文件合并时的内存溢出问题即可，这一点跟后端实现语言相关，我打算分别以PHP，NodeJS，JAVA来实现。避免文件读写造成的内存溢出，常用的手段是管道（Pipe），后端代码也会在github上提供，在此就不展示了。 另外我们还要解决一件事儿：如果某个大文件上传了一半，出于种种理由用户决定再也不会上传它的其他部分了，服务器端怎么办？这些过期分片永远存在服务器端的硬盘上显然无法让正常人接受。我们要怎么办呢？方案同样很多，我这里使用的是为每个上传的文件（不是分片）创建一个临时文件，名字使用上面创建的那个文件唯一标识，如：2e2a311b96d816b695fdf5817bd030f8.tmp。这个文件的修改时间会在每次有分片请求时被更新，然后再开启一个定时任务，比方说每夜凌晨检查一下上传文件夹路径下的所有tmp文件的修改时间，如果发现最后的修改时间距当前时间已经很久了，则认为其对应的分片数据需要被清理。如下图： 好吧，关于第二个问题我们也算搞定了，最后一个问题：如何从指定分块开始上传？这个算是断点续传和暂停继续的核心问题了，而这一点我们需要依赖WU提供的机制：WU分片上传大文件时，会依次上传被切分的分片数据，在这个过程中所需要的分片下标，上传进度等数据都是由WU负责管理的，后端并不需要关注。 每次分片上传前，都会发起一次校验请求，我们就是通过这样的方式来解决第三个问题的。对于暂停继续，WU保存着分片下标的信息，所以它只会从暂停时的那个分片开始询问，而断点续传由于丢失了分片下标信息，则会从0号分片开始询问。这些内容上面已经提及了~~ 秒传相信大家肯定使用过各种类型的网盘，或者是迅雷会员，都会非常中意“离线下载”这个功能。谁都希望添加到迅雷里的下载任务能够一秒钟下载完毕，同样，我们的用户也希望能够一秒钟就上传完一个1G的文件。但这是不可能的，至少不是真的通过上传的方式实现的！ 假设用户A要传的文件是在之前就被用户B上传过的（比方说一部岛国动作片儿，你懂的），这个时候我们不需要傻傻的把这个文件再上传一遍，只需要告诉用户A已经上传完毕了，并返回给他对应的链接，我相信用户A会非常满意的！当然，我们的系统还要处理“写时复制”问题（当用户B删除文件时，由于用户A还在引用，所以文件不应该删除，而应该切断用户B的链接，直到所有用户都不再引用该文件，此时才应该真实删除文件）！ 好吧，我们来看看具体如何实现秒传：应该找个方法来判断上传的文件是否服务器端已经存在！这似乎和我们上面提到的文件唯一标识有点类似，但注意我们上面的那个标识中引入了用户id，所以肯定不符合我们这里的情况。那么去掉用户id是否就ok了呢？ 恩，其实是可以的，不过这种方式生成的一致性验证其实并没有对内容本身进行验证，而是通过其文件的相关信息来做的判断，也就是说如果两个文件的文件名，大小等“凑巧”一模一样，但这两个文件内容不同的话，就会导致一致性校验错误。 电驴是怎么做的？它会用文件的二进制数据计算文件的md5签名，这样即使文件的名字大小都一样，内容不同也会导致它们的签名不同~这样基本上就是万无一失了，不过对一个大文件进行md5签名可能要话费非常多的时间，这就需要我们来取舍了。 我在自己的开发机测试了一下，用WU提供的md5File函数计算一个266M的文件的md5签名，耗时：15.573秒。 WU的md5File函数支持对文件的部分数据做md5签名，这样的话我们就可以对大文件中的一部分内容签名，避免太大的耗时。牛逼坏了！我们按照官方文档的提示使用Hook：before-send-file： ...... beforeSendFile: function(file){ //秒传验证 var task = new $.Deferred(); var start = new Date().getTime(); (new WebUploader.Uploader()).md5File(file, 0, 10*1024*1024).progress(function(percentage){ console.log(percentage); }).then(function(val){ console.log(&quot;总耗时: &quot;+((new Date().getTime()) - start)/1000); $.ajax({ type: &quot;POST&quot; , url: &quot;fileUpload.php&quot; , data: { type: &quot;md5Check&quot; , md5: val } , cache: false , timeout: 1000 //todo 超时的话，只能认为该文件不曾上传过 , dataType: &quot;json&quot; }).then(function(data, textStatus, jqXHR){ if(data.ifExist){ //若存在，这返回失败给WebUploader，表明该文件不需要上传 task.reject(); uploader.skipFile(file); file.path = data.path; UploadComlate(file); }else{ task.resolve(); //拿到上传文件的唯一名称，用于断点续传 uniqueFileName = md5(&apos;&apos;+userInfo.userId+file.name+file.type+file.lastModifiedDate+file.size); } }, function(jqXHR, textStatus, errorThrown){ //任何形式的验证失败，都触发重新上传 task.resolve(); //拿到上传文件的唯一名称，用于断点续传 uniqueFileName = md5(&apos;&apos;+userInfo.userId+file.name+file.type+file.lastModifiedDate+file.size); }); }); return $.when(task); } ...... 好啦，前端做完了，后端只需要拿到对应的md5签名，然后进行相关的检索，如果确实存在这个签名匹配的文件，则提示前端该文件已经存在。这部分通常会利用数据库来完成，当然也可以使用内存KV数据结构来做，我们不纠结这！ 好啦，截止到这里，我们大概已经完成了大文件的上传问题，完整的代码我会稍后在github上提供出来，有兴趣的童鞋可以去瞅瞅。 更正1： 尽管我们在例子中已经设置为单文件上传，并且在生成文件的唯一标识时加上了userId，但这并不能阻碍并发上传时造成的冲突，先来看下面这个场景，让我来手绘一张图，真的是纯手绘哟~~： 假设我们的用户A用浏览器打开了两个tab窗口（userId一致）：tab1，tab2。这两个窗口同时上传同一个文件（文件标识一致），这里暂不考虑浏览器对多tab上传同一文件是否会做优化，如果确实会做优化，那我们也可以假设用户是同时在两款浏览器上同时上传某一个文件。 如上图所示，假设tab1先上传了3个分片，在上传第四个分片之前阻塞了（原因不详）。这个时候tab2开始上传，由于生产的文件标识一致，所以tab2的分片校验请求会对前3个分片返回true，这样tab2就会直接从第4个分片开始上传，我们假设tab2上传完第4个分片且再第5个分片开始上传前，tab1结束了阻塞状态，它会得知第4个分片已经上传完毕，迅速开始第5个分片的上传工作。 到目前为止，一切都是那么的顺利和合理，仿佛生活多么的美好。好，这个时候我们假设这个文件一共就需要5个分片就可以上传完毕。从图上可以看到，此时tab1和tab2会同时进行第5个分片的上传任务（因为在它们发送的分片校验请求时，彼此都没有完成上传，所以它们都会得到一个目标分片不存在须上传的回复）。 如果最终结果是tab1先上传完成了，那么依照我上文的做法，那么tab1会触发合并动作，合并过程中会清理已合并分片。那么问题就来了，会有以下几种不好的情况发生： tab1合并完分片1时，但tab2正在合并分片1，tab1删除分片1失败 tab1合并完分片1后成功删除分片1，导致tab2上传完分片5后无法触发合并动作（分片总数不达标），从而导致前端WU很尴尬（因为分片5上传完毕后就会触发uploadSuccess事件，但拿不到合并后的文件地址） tab1合并完毕且删除了tmp文件，此时tab2上传完分片5，再次到回到情况2 好乱啊，总之上文中的方案不能很好的解决这个场景。那该怎么办好捏？ WU似乎应该是考虑过这种情况，所以提供了对应的Hook：after-send-file。 在所有分片都上传完毕后，且没有错误后request，用来做分片验证，此时如果promise被reject，当前文件上传会触发错误。 我们只需要在这个hook中发送一个ajax请求给后端来触发合并动作，听起来这并没有解决并发啊，毕竟这个请求也可能同时到达后端！ 是的，没错，但这么做有个好处，至少WU并不会尴尬，你可以告诉WU上传失败了，至少不会像之前的方案那样，明明触发了uploadSuccess事件，但却拿不到合并后的文件地址。 那么并发冲突怎么解决？我们就需要使用一种锁机制了，如果node平台，由于其执行主进程是单线程的，所以我们可以在全局变量中设置一个标志位即可。在java平台中，由于其依赖多线程实现并发，我们就要借助同步操作来解决这个问题。最头疼的就是php，老一点的手段就是让多个进程基于某个临时文件锁进行同步，当然也可以依赖第三方软件解决，比方说redis的原子操作，甚至再重一点引入zookeeper。 我会在上面提到的github地址中选择一种方案来解决这个问题的，敬请关注。","tags":[{"name":"webuploader","slug":"webuploader","permalink":"https://blog.kazaff.me/tags/webuploader/"},{"name":"秒传","slug":"秒传","permalink":"https://blog.kazaff.me/tags/秒传/"},{"name":"断点续传","slug":"断点续传","permalink":"https://blog.kazaff.me/tags/断点续传/"},{"name":"上传进度","slug":"上传进度","permalink":"https://blog.kazaff.me/tags/上传进度/"},{"name":"分块","slug":"分块","permalink":"https://blog.kazaff.me/tags/分块/"},{"name":"大文件","slug":"大文件","permalink":"https://blog.kazaff.me/tags/大文件/"}]},{"title":"数据库中间件的比较","date":"2014-11-10T09:37:12.000Z","path":"2014/11/10/数据库中间件的比较/","text":"这年头，但凡是搞开发的，都要和互联网挂上边儿，但凡是搞互联网的，都要和大数据挂上钩儿，我们也不例外~~哇哈哈哈哈！这里我们不谈什么高大上的数据挖掘分析（为什么看到“挖掘”就不自觉地联想到山东呢？），今次我们要聊的是应用中要如何处理海量数据的数据库存储和使用。说的更详细一些，就是如何保证面对应用所需的海量数据，我们如何确保mysql数据库高可用和高性能： 高可用，自然是做冗余备份，并能做到Failover； 高性能，一般就是数据库调优，读写分离，分库分表。 在此我们不考虑更复杂的方案，例如：引入nosql，缓存，等等。 其实现存的数据库高可用的开源解决方案中，已经有很多现成模块足够我们选择的了。 就我个人而言，是比较亲近阿里的。本来以为会毫不犹豫的选择cobar，不过出于复杂度的考虑，我目前可能更倾向于Atlas~~还是自己搭建一下运行环境测试测试再做最后的决定吧。 Atlas我比较欣赏的是它的简单，非常适合中小型项目或者刚起步的创业公司。正是由于它的简单，所以不需要什么耗时的学习和调优就可以快速投入使用。 我在虚拟机中测试安装，步骤很简单，我使用是rpm安装： rpm -ivh Atlas-2.2.el6.x86_64.rpm 一切是如此的顺利，接下来就是按照说明书修改配置文件。最后，执行： sudo ./mysql-proxyd test start 搞定。接下来就是在终端中以管理员的身份连接Atlas： mysql -h127.0.0.1 -P2345 -uadmin -p123456 注意，上面的参数是需要和配置文件保持一致的。然后在终端中执行： mysql&gt; select * from help; +----------------------------+---------------------------------------------------------+ | command | description | +----------------------------+---------------------------------------------------------+ | SELECT * FROM help | shows this help | | SELECT * FROM backends | lists the backends and their state | | SET OFFLINE $backend_id | offline backend server, $backend_id is backend_ndx&apos;s id | | SET ONLINE $backend_id | online backend server, ... | | ADD MASTER $backend | example: &quot;add master 127.0.0.1:3306&quot;, ... | | ADD SLAVE $backend | example: &quot;add slave 127.0.0.1:3306&quot;, ... | | REMOVE BACKEND $backend_id | example: &quot;remove backend 1&quot;, ... | | ADD CLIENT $client | example: &quot;add client 192.168.1.2&quot;, ... | | REMOVE CLIENT $client | example: &quot;remove client 192.168.1.2&quot;, ... | | SAVE CONFIG | save the backends to config file | +----------------------------+---------------------------------------------------------+ 10 rows in set (0.00 sec) 可以看到管理员可以使用的运维命令。 我们可以只使用它提供的读写分离，但是你的mysql主从配置还是需要自己在mysql中配置好的。 Atlas的这种中间代理的设计架构，不可避免的会损失掉一部分性能，主要是数据转发引起的，就看你是否接受了。 至于分库分表功能，Atlas并不是非常的强大，细节可以看这里。还是那句话，中小项目足矣！ 目前我能想到的Atlas的代码侵入性也就是在分库分表中如果想要指定sql执行的服务器，需要加入： /*master*/ 另外的侵入性体现在分库分表需要用户自己创建固定个数的数据表，而且表名也有硬性规则。别的应该木有啦~~ 对于我们目前的项目来说，完全足够了~ Cobar有人说，阿里开源出来的东西里，cobar算是一个非常成功的项目之一（还有LVS，Dubbo等）！这也是我一直比较亲睐阿里的原因之一，毕竟能够回馈开源，展示国人实力，是一件非常伟大的使命，阿里做到了。不说废话，先来学习一下Cobar的基础知识：传送门（万万没想到github上Cobar的wiki现在还在完善中，只能通过下载“其他”栏目中的原来的wiki资料学习了！） 简单的总结一下就是说，Cobar在分库分表和提高SQL查询上做了非常多的工作，所以不可避免的引入了复杂性！如果项目足够大，确实可以考虑使用，不过小项目还是算了。","tags":[{"name":"读写分离","slug":"读写分离","permalink":"https://blog.kazaff.me/tags/读写分离/"},{"name":"mysql","slug":"mysql","permalink":"https://blog.kazaff.me/tags/mysql/"},{"name":"负载均衡","slug":"负载均衡","permalink":"https://blog.kazaff.me/tags/负载均衡/"},{"name":"Atlas","slug":"Atlas","permalink":"https://blog.kazaff.me/tags/Atlas/"},{"name":"分库分表","slug":"分库分表","permalink":"https://blog.kazaff.me/tags/分库分表/"},{"name":"性能","slug":"性能","permalink":"https://blog.kazaff.me/tags/性能/"},{"name":"侵入性","slug":"侵入性","permalink":"https://blog.kazaff.me/tags/侵入性/"}]},{"title":"关于Redis集群和事务","date":"2014-11-01T09:37:12.000Z","path":"2014/11/01/关于redis集群和事务/","text":"最近为了核算项目的两个架构指标（可用性和伸缩性），需要对项目中使用的Redis数据库的集群部署进行一定程度的了解，当然顺便再学习一遍它的事务细节。既然我在上面把Redis称之为数据库，那么在我们目前的项目里，它自然就需要持久化相关的数据，而不仅仅充当缓存而已！ 在网上逛了一遍，看了不少关于redis集群搭建的文章，有一些把redis的主备当集群来讲的，也有一些讲的是以第三方代理方式搭建集群的，比较新的是讲的redis3.0beta提供的服务器端实现集群的~~ 比较有代表型的一款中间代理方式实现redis集群的开源产品是Twitter推出的twemproxy。而这种方式的优劣，redis的作者早已经写过一篇分析的文章了，相信大家读过以后就能了解其中的好坏。 在这里，我主要是贴一下关于twemproxy对redis命令的支持情况的细节，相信有了这个数据，我们在设计使用redis时可以起到指导的作用。 另外，twemproxy中不少命令的支持与否需要依赖Hash Tags，简单粗暴解释的话，其实就是说twemproxy支持指定key中的某一部分用来做hash散列，这样就有助于把相关的一些数据分布在同一台服务器上，从而避免一些指令导致数据在多服务器之间不必要的迁移而影响了性能！ 从这个表格中我们注意到，twemproxy不支持redis的事务操作，原因其实在上面给出的文章中已经解释了~这里我主要想来聊一下redis的事务到底是什么？记忆中看过一篇文章，模糊记得redis的事务和传统关系型数据库的事务是存在差异的。 先看一下这篇文章，写的已经非常之详细了，不是么？我们必须搞清楚： 原子性、一致性和回滚功能。这可能显得有一些过于纠结定义了，不过查了一下GG才发现，其实关于原子性和一致性的理解却是有很多种说法~~而我更偏向于下面这种解释： 一致性：如果事务启动时数据是一致的，那么当这个事务成功结束的时候数据库也应该是一致的; 原子性：事务的所有操作在数据库中要么全部正确反映，要么全部不反映。 分别举例子来说，一致性就是数据库的数据状态符合数据库所描述的业务逻辑和规则，比如最简单的一条一致性规则：银行账户存款余额不能是负值。 而原子性就是其字面解释：要么都执行，要么都取消！这时候就需要数据库事务有回滚能力。不难理解吧？ 接下来我们再说redis的事务，上面的资料中提到： 当事务失败时，Redis 也不会进行任何的重试或者回滚动作。 也就是说，redis不具备事务原子性（非事务下的redis命令具备原子性）。看一个代码例子： set name kazaff0 //首先我们为key为name的键设置了一个值：kazaff0 multi //开启事务 set name kazaff1 //事务里我们修改name的值为kazaff1 lpush name kazaff2 //故意造成一个执行错误 exec //提交事务 get name //？ 可以猜出最后一条指令的返回结果应该：kazaff1。为什么？因为redis不支持事务失败后回滚！ 但是需要注意的是，服务器在执行事务之前会先检查客户端的状态，如果发现不满足事务执行的条件的话服务器端会直接终止事务，也就是说任务队列中的指令一条都没有执行！ 为什么要注意这一点呢？也就是说只有执行错误才会需要回滚，而watch，discard，入队错误等都不需要回滚，因为执行队列中的指令压根一条都没有执行过！ 以前总是把redis的事务和pipe看成一个东西：打包执行指令~~但现在才发现，完全两码事儿嘛！！ 参考：理解事务的一致性和原子性","tags":[{"name":"redis","slug":"redis","permalink":"https://blog.kazaff.me/tags/redis/"},{"name":"集群","slug":"集群","permalink":"https://blog.kazaff.me/tags/集群/"},{"name":"tomcat","slug":"tomcat","permalink":"https://blog.kazaff.me/tags/tomcat/"},{"name":"twemproxy","slug":"twemproxy","permalink":"https://blog.kazaff.me/tags/twemproxy/"},{"name":"一致性","slug":"一致性","permalink":"https://blog.kazaff.me/tags/一致性/"},{"name":"原子性","slug":"原子性","permalink":"https://blog.kazaff.me/tags/原子性/"},{"name":"回滚","slug":"回滚","permalink":"https://blog.kazaff.me/tags/回滚/"},{"name":"pipe","slug":"pipe","permalink":"https://blog.kazaff.me/tags/pipe/"},{"name":"缓存","slug":"缓存","permalink":"https://blog.kazaff.me/tags/缓存/"}]},{"title":"Java Web中的session新手向","date":"2014-10-24T09:37:12.000Z","path":"2014/10/24/java web中的session新手向/","text":"前几天又一遍熟悉了一下常用的负载均衡软件，比方说Nginx，还挺顺利的。既然有了负载均衡，那就要求应用能具备良好的伸缩性，而要达到这一点，就要解决多个应用镜像之间的数据共享问题，说白了就是会话状态的共享！其实无会话状态才是服务追求的目标，搞过RESTful的童鞋应该知道这一点！之前玩PHP的时候，要做到Session共享其实不难，只需要把session存储到mysql中，这事儿就搞定一多半了，剩下的就是设置session的作用域等参数，能够保证客户端请求时携带任意一台服务器为它创建的session_id即可！~那，在javaEE下又该怎么做呢？ 先从基础知识讲起吧，看一下这些文章：传送门1，传送门2。其实和PHP里定义的Session差不多，这也很正常，本来这个概念就不是由语言提出来的，而是由HTTP引出的，所以语言相关性不大。而且类比apache+php，其实在java web中，session也是交给tomcat这种容器来管理的，而servlet只是提供了相关的接口定义而已，想了解这其中的内部细节的童鞋可以看一下这篇文章。 好了，到这里为止基本上已经算是熟悉java下的session了！接下来我们就可以直奔主题了，其实实作方式应该也和php的差不多，只不过需要写成tomcat的“插件”，具体细节其实已经有相关的扩展了，我这里直接找到一篇非常实战的文章供大家操作：传送门。 这篇文章就到这里吧，虽然感觉上并没有写什么，哇哈哈哈，毕竟作为新手能遇到的问题都应该被前辈大妞们解决了，我们只需要拼命的学习就行啦！","tags":[{"name":"session","slug":"session","permalink":"https://blog.kazaff.me/tags/session/"},{"name":"负载均衡","slug":"负载均衡","permalink":"https://blog.kazaff.me/tags/负载均衡/"},{"name":"会话亲和性","slug":"会话亲和性","permalink":"https://blog.kazaff.me/tags/会话亲和性/"}]},{"title":"搭建排查Tomcat内存溢出问题的调试环境","date":"2014-10-23T09:37:12.000Z","path":"2014/10/23/搭建排查tomcat内存溢出问题的调试环境/","text":"上个月赶工上线的门户网站，由于种种原因导致部署到线上服务器后每隔一段时间后就会导致tomcat内存溢出，今天我就要来直面这个棘手的问题。 要解决的问题对我来说还是有点难度的，原因有二： 代码不是我写的； 我对java并不熟悉。 废话不多说，就由我这个小白依靠GG带领大家来启程吧！ 凭借我多年的编程经验，我认为首先要找到趁手的工具，那么，问题就来了，挖掘机技术到底哪家强？…… 好吧，GG一下，可以很容易查到很多用来监控jvm实时状态的工具，我们以jconsole为第一款尝试的工具吧。 jconsole这里要说明的是，我们需要搭建的监控环境是在win桌面机上远程监控一台centos服务器。按照网上说的，搭建起这么一个环境没有多大难度，大家可以参考这里：传送门。 如果你像我一样碰到了timeout提示，那多半就是centos防火墙拦截导致的，可以暂时关闭防火墙再尝试一下： /etc/init.d/iptables stop 好的，终于有了一个监控界面了，是不是感觉心里敞亮了不少呢？不过我感觉还是太笼统了，只能大概知道jvm的状况，而对于我们要排查代码导致的内存泄露问题似乎并没有帮到太大的忙~~ 不过可以通过提供的一些信息来判断是否配置了比较合理的参数，比方说可以通过GC时间看出是否给tomcat分配了适当的内存大小，尽可能的设置一个合理的内存来减少gc的次数和耗时。 那我们接下来换哪个工具呢？ JProfiler好吧，上大杀器！无需我多讲，我相信没有人会对我的选择有质疑吧？哇哈哈哈哈哈~~不过JProfiler是个商用产品，钱花到位才能享受生活，这一点儿错都没有。 大家可以参考下面这些链接，相信你们很快就能搭建成功： 传送门1,传送门2,传送门3 按照上面提供的信息，我相信你很顺利就能安装部署完毕jprofiler（毕竟是商用产品嘛，肯定做的足够简单），但如果你和我一样是第一次用这个玩意儿，肯定会被它的默认界面震出翔！ 不要pia，先阅读一下这篇文章： 传送门。 这里要提到的一点是，可能是软件版本的原因，按照上面前辈说的方法我却死活查不到方法调用Tree，查了一下GG才发现是需要调整配置选项，如下图： 我这属于暴力解决吧，毕竟我把能开启的选项都选中了，不过想得到的问题也就是速度慢点，对于远程连接方式来说带宽占用多一些而已吧~~ 测试代码找到了趁手的兵器，下一步就是要挖的坑了！哦，no，是一段会造成内存溢出的测试代码，我简单地修改了一下tomcat提供的例子中的HelloWorldExample.java： import java.io.IOException; import java.io.PrintWriter; import java.util.ResourceBundle; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.util.ArrayList; public class HelloWorldExample extends HttpServlet { private static final long serialVersionUID = 1L; private static ArrayList list = new ArrayList(); @Override public void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException { ResourceBundle rb = ResourceBundle.getBundle(&quot;LocalStrings&quot;,request.getLocale()); response.setContentType(&quot;text/html&quot;); PrintWriter out = response.getWriter(); out.println(&quot;&lt;html&gt;&quot;); out.println(&quot;&lt;head&gt;&quot;); out.println(&quot;&lt;/head&gt;&quot;); out.println(&quot;&lt;body bgcolor=\\&quot;white\\&quot;&gt;&quot;); for(int i=0; i &lt; 1000; i++){ //Object o = new String(&quot;by kazaff, index is :&quot; + i); HelloWorldExample.list.add(new kazaffBean()); //o = null; } out.println(&quot;&lt;div&gt;kazaff in here!!!!!!!&lt;/div&gt;&quot;); out.println(&quot;&lt;/body&gt;&quot;); out.println(&quot;&lt;/html&gt;&quot;); } } class kazaffBean { String name= &quot;&quot;; } 然后我们还要手动编译修改后的java代码，进入到/usr/local/apache-tomcat-7.0.53/webapps/examples/WEB-INF/classes/HelloWorldExample.java所在的文件夹中，执行下面的命令： javac HelloWorldExample.java -cp /usr/local/apache-tomcat-7.0.53/lib/servlet-api.jar 当然，你的tomcat路径和我的很可能不同，酌情修改即可~~ 然后我们重启tomcat，对了，重启之前最好改一下分配给tomcat的内存上限，改小一些，有助于问题快速的暴露： -Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/www/java-OOM/ 后面的两个-XX参数是用来让jvm在出现OOM后自动保存内存堆栈快照信息用的，方便我们排查问题。 重启吧，然后为了加速内存溢出，我们可以使用apache自带的ab做压力测试： ab -c 100 -n 10000 http://192.168.153.128:81/examples/jsp/ 执行上面的这条命令后，基本上tomcat肯定就已经挂掉了，注意，我说的是tomcat挂掉了！也就是说你在终端中执行jps命令，不再会看到Bootstrap这个进程了！我之所以强调这一点，是因为我在测试中发现直接导致jconsole断开连接，并且再也无法建立连接。 而奇怪的是，jprofiler照常可以连接并获取到远程服务器上的jvm的监控数据，这种情况一度使我陷入深深的迷惘。因为我在tomcat的logs里死活找不到任何关于内存溢出或其他异常的记录，直到我的目光落在终端上： Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread &quot;http-bio-81-exec-13&quot; …… java.lang.OutOfMemoryError: GC overhead limit exceeded …… java.lang.OutOfMemoryError: Java heap space …… Exception in thread &quot;RMI TCP Connection(idle)&quot; java.lang.OutOfMemoryError: Java heap space 这才是乖孩子嘛，就应该是这样的才对嘛~~不过还有几个点想不明白： 虽然jps已经查不到tomcat的进程，但是从jprofiler的线程监控中还是可以看到相关的线程，如下图： 既然tomcat都已经挂了，为什么jprofiler没有像jconsole那样连接断开呢？ 我这种小白目前是搞不定这两个问题了，还是抛到社区给大牛们分析吧。我们继续往下走~ 关于内存溢出通过我上面列出的异常信息，已经是非常常见的了，对于一些java老鸟而言肯定是再熟悉不过的了！不过我还是找到了一篇排版不咋滴但是比较全面的文章： 传送门。 到此为止，就算做好了一切准备，可以去真正的项目上搞了！","tags":[{"name":"jvm","slug":"jvm","permalink":"https://blog.kazaff.me/tags/jvm/"},{"name":"jconsole","slug":"jconsole","permalink":"https://blog.kazaff.me/tags/jconsole/"},{"name":"jprofiler","slug":"jprofiler","permalink":"https://blog.kazaff.me/tags/jprofiler/"},{"name":"内存溢出","slug":"内存溢出","permalink":"https://blog.kazaff.me/tags/内存溢出/"}]},{"title":"javaEE部署项目新手向","date":"2014-10-23T09:37:12.000Z","path":"2014/10/23/javaEE部署项目新手向/","text":"今天主要说的是关于java的web项目部署时候的琐碎。公司之前的几个javaEE项目我都没有参与编码，更别提部署了！不过接下来的项目我就要参与到所有环节中拉，所以趁着有时间，搞一下预备工作，都是很基础的东西，只是作为总结记录下来而已~~以问题的方式来排版吧，这样便于阅读： 项目中配置文件里使用的classpath到底指向哪里？可以通过下面这行代码打印出来classpath的绝对路径： System.out.println(&quot;当前classpath的绝对路径：&quot; + Thread.currentThread().getContextClassLoader().getResource(&quot;&quot;)); 我用Idea13部署项目后可以看到终端中打印出： 当前classpath的绝对路径：file:/D:/Program%20Files/apache-tomcat-7.0.53/webapps/springMVCDemo/WEB-INF/classes/ 可以看到classpath指向的是部署后的/WEB-INF/classes/，而这个classes文件夹就是存放编译后的.class文件的地方。 而这个文件夹在部署之前是不存在的，那么IDE中配置文件里使用的classpath:又指向哪里呢？因为像Idea这样的吊炸天IDE都提供即时功能的，比方说我在web.xml中添加下面这个配置： ... &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:mvc-dispatcher-servlet.xml&lt;/param-value&gt; ... IDE是会帮我检查对应文件夹下是否存在指定文件的，那我们之前说过编译前classes文件夹是不存在的，那么IDE又是帮我去哪找的呢？看下图： 图中左上角中的箭头指向的那个位置就是IDE查找的位置，也就是说java文件夹对应着编译后的classes文件夹。 上图中的项目是基于springMVC模板的，那文件夹这种对应关系又是在哪里设置的呢？其实不同的IDE都有自己的一套项目文件组织结构，之前公司用的是Netbeans，也有人使用Eclipse的，默认情况下它们的项目结构都不太一样，但是不管你用什么IDE，向Tomcat中部署的时候，都要输出一致的，Tomcat理解的标准目录，在Idea的Artifacts窗口下可以定义编译部署的项目结构，上图中下方的箭头和红框标出来的就是我们为了测试而修改mvc-dispatcher-servlet.xml位置后需要对应做的修改，这样才能保证IDE部署项目时导出正确的部署结构。 如何在Controller中读取属性文件中的设置？这个问题网上有不少贴，可以看这里：传送门，已经是非常具体了。可以看出，Controller中靠注解直接把配置文件中的键值注入到类属性中，很是NB啊！ 实际测试中还是需要按照第一个问题里说的那样调整一下Artifacts的部署结构，这里我发现在Idea13中，我把属性文件直接放在WEB-INF目录下，IDE中竟然显示找不到文件，但是部署后是正常的，就是显示红色的提示让我看着很不爽，所以才扔到classes下的~ nginx代理项目中的静态资源文件公司前几个小项目都出现了不同程度的性能问题，其中比较早遇见的就是关于静态文件太多导致的响应卡顿的问题！ 其实并不能一定断定就是Tomcat处理静态文件不利导致的，只不过看网上大家都吐槽它，还是果断放弃为好~~我们只需要搭建一个反向代理服务即可，这里第一选择肯定就是Nginx。 我在自己的开发机上直接尝试搭建这个测试环境，大概环境如下： win7 64bit jdk1.7 tomcat7 nginx1.7.6 测试项目部署完毕后的目录结构为： springMVCDemo | |--images //这里就是我们的静态文件夹 | |-WEB-INF 好啦，现在直接修改Nginx的配置文件/conf/nginx.conf: server { listen 80; server_name localhost; root &quot;D:/Program Files/apache-tomcat-7.0.53/webapps/springMVCDemo&quot;; location / { proxy_pass http://localhost:8080/springMVCDemo/; } location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf)$ { expires 30d; } location ~ .*\\.(js|css)?$ { expires 1h; } 很简单吧，这样就搞定了。这里面有个小插曲我需要说一下：我是直接用IDE编译打包成一个war文件，然后拷贝到tomcat的webapps下，由于我的天真，我一直试图尝试在Nginx的配置中直接指向war文件内部的images目录~~哇哈哈哈，网上找了一大圈都没有发现，结果最后才发现一个问题：tomcat在启动之初，如果发现需要加载war项目，会直接把war文件解压出来。阿西吧~ 最后，还要叮嘱一件事儿，那就是关于nginx的反向代理配置不当造成的安全隐患，具体细节可以看这里： WooYun。 springMVC中静态文件的处理虽然在上一个问题里我们已经把静态文件交给Nginx来处理了，但是我觉得那是生产环境下的配置~~在开发环境下还是要尽可能的直观一些，这样可以让配置较低的开发机更快一些，而且也可以让调试变得简单高效。 其实实现的方法网上说了好多，但我还是偏向于使用Spring提供的方式： &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;me.kazaff.springmvc&quot;/&gt; &lt;mvc:annotation-driven /&gt; &lt;mvc:resources mapping=&quot;/images/**&quot; location=&quot;/images/&quot; /&gt; .... &lt;/beans&gt; 这样就可以啦，注意在头部要加上mvc这个命名空间的定义，一共要修改两个位置哟： 在beans节点上增加： xmlns:mvc=”http://www.springframework.org/schema/mvc 在xsi:schemaLocation属性上增加： http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd 其实在测试中还遇到了一个问题，就是一旦加上mvc:resources定义后就会造成无法访问页面，错误是： 警告: No mapping found for HTTP request with URI [/springMVCDemo/] in DispatcherServlet with name &apos;mvc-dispatcher&apos; 所以一定要记得加上： &lt;mvc:annotation-driven /&gt; 理由可以查看这里： 传送门。 javaEE负载均衡环境搭建接下来我们简单聊一下高可用和可伸缩的解决方案，我们可以在架构设计上做一些工作，从而使得我们的业务应用服务可以搬到集群中进行部署，至于要达到可伸缩性而需要解决的技术点我们暂且不表，毕竟这篇文章是新手向，我们主要聊一下集群环境下的负载均衡话题~~ 其实负载均衡不仅仅能做到平摊负载，牛逼一些的负载均衡软件还可以负责自动容灾处理。还拿Nginx来说，非常简单，看这里和那里。能感觉到还是非常的简单明了吧？这也是Nginx能脱颖而出的原因：简单粗暴！ 但这个世界没有银弹，我们来看一下常用的负载均衡软件的优缺点：传送门。 看前辈总结了那么多，心里应该有谱了吧？！上面那一篇文章最大的价值在于描述了一个项目从小到大所处的不同阶段应该做什么样的选择，哥甚是喜欢！ 我还找到了一个碉堡的视频共大家观看：传送门。 如何在一台物理机运行多个tomcat进程？最后这个问题是非常有意义的，不过你可能会很好奇谁会这么做？首先，部署在同一台物理机上，就谈不上什么高可用，毕竟如果这台机器宕机了，所有tomcat实例都会挂掉。而且由于进程间切换也会导致性能上的折损，那谁会这么做呢？ 并非只有在开发环境或测试环境下为了模拟集群才会这么做，其实生产环境下这么做也是有目的的。 我们先聊聊操作系统和JDK，一般服务器现在都会装64bit的版本，这样可以使内存突破4G的上限。而这个时候可能你就会理所应当的安装64bit版的jdk。那么，问题就来了：目前来说，64位的jdk版本整体性能不如32位版！这个现状会随着时间飞逝而逐渐消失。 那么除了jdk本身的性能因素，还有其它问题么？ 我们简单的说一下关于JVM内存管理的内容。在我们启动tomcat时是可以设置几个和内存相关的参数的，包括：堆大小（新生代），永久代，虚拟机栈，垃圾回收器等。这些参数都是jvm性能调优的关键，我们暂不深究，只是知道它们很重要即可！ 刚才提到了，64位的系统和jdk版本可以使内存突破上限，这样我们的jvm相关设置就可以调的更大，理应拿到更好的性能，对吧？但其实不然，来看一下主要的问题点： 太大的堆栈会直接导致垃圾回收时间增加，出现明显的卡顿； 相同的程序下，64位比32位更吃内存（指针膨胀等）； 由于内存分配过大，导致调试工具（jmap等）无法使用（dump一次会产生非常大的文件，而且分析这个文件也变得不再可行），这就要求程序必须有足够高的稳定性，确保不会出现内存泄露 综上所述，目前我们可以选择64位的操作系统+32位的jdk作为搭配，但是这样的话服务器上大量的内存资源就浪费了，浪费是可耻的！ 所以才需要我们在一台物理机上运行多个tomcat实例，这样可以达到最佳效果。而且由于我们的目的是为了换取更大的内存使用率，所以多个tomcat进程上的多个项目镜像不需要做特殊处理（会话粘性），直接可以用nginx做个ip_hash负载均衡即可。 好啦，扯了那么多，到底如何在同一台物理机上运行多个tomcat实例呢？点我 最后目前大概也就这些，够开发用了，我会继续努力的！ PS：做新人的感觉真TM爽，每天都那么充实~~ PS2：推荐一款加速网页显示的大神器，你们懂的。","tags":[{"name":"nginx","slug":"nginx","permalink":"https://blog.kazaff.me/tags/nginx/"},{"name":"负载均衡","slug":"负载均衡","permalink":"https://blog.kazaff.me/tags/负载均衡/"},{"name":"classpath","slug":"classpath","permalink":"https://blog.kazaff.me/tags/classpath/"},{"name":"springMVC","slug":"springMVC","permalink":"https://blog.kazaff.me/tags/springMVC/"},{"name":"Idea13","slug":"Idea13","permalink":"https://blog.kazaff.me/tags/Idea13/"},{"name":"tomcat","slug":"tomcat","permalink":"https://blog.kazaff.me/tags/tomcat/"},{"name":"jvm","slug":"jvm","permalink":"https://blog.kazaff.me/tags/jvm/"}]},{"title":"到底积累什么才是最佳选择？","date":"2014-10-13T17:25:30.000Z","path":"2014/10/13/到底积累什么才是最佳选择/","text":"其实一直以来都在困惑，像我这种草根出身的程序员，随着工作时间的增长，价值到底体现在什么地方？ 如果说是体现在编程能力上，那其实任何一个上心的普通正常人都可以在2-3年的编程岗位上达到一样的高度，这样来说，5年的程序员和3年的程序员差别又是什么呢？ 而且技术迭代的速度着实快，编程语言更是每年都有黑马，你曾经积累的某种语言的编程经验很可能没过几年就变成了垃圾。相比之下，可能编程思想才是真正有价值的遗产。 那么编程思想又都包含什么呢？ 这个问题就涉及到软件开发领域中的一些术语，像设计模式、项目把控、需求分析，等等。那是不是搞定这些就能保证你不会被公司外聘来的“和尚”所代替呢？我看也不尽然。 我觉得应该是你对业务理解的程度，才是你保值的最有利武器。这个道理并不难理解，但恰恰是很多程序员所做不到的！我们往往由于折服于技术的强大，而忽略了真正的驱动力：业务。 为什么多数VC们总是盯着商业价值？其实商业价值跟技术关系并不大，跟市场，跟业务却是密不可分的。当然不能否认的是：只有过硬的技术才能支撑哪些天花乱坠的商业模式。不过，天外有天，你技术再好，也会被更nb的咖秒杀。 我想，这就是为什么销售人员比开发人员的吃香的本质吧？ PS：上面说的都是建立在很多假定上的，例如你所在的环境确实有前景，例如你得到了老板的重视和认可，例如你确实遇到了瓶颈等。 PS2：最近一段时间，组织了团队内的一次较大的头脑风暴会议，感触颇多，我想这是非常难得的经历，而目前公司的这个项目也是一个非常好的历练机会，真心希望可以通过半年的时间，能让自己更上一层楼。","tags":[{"name":"迷茫","slug":"迷茫","permalink":"https://blog.kazaff.me/tags/迷茫/"}]},{"title":"所谓协议相对URL","date":"2014-09-26T18:15:30.000Z","path":"2014/09/26/所谓协议相对URL/","text":"之前在搞前端的时候，看到过很多国外的插件或library都好使用没有指定协议的url，查过原因，但忘记了～～这次又看到，决定记录下来。 &lt;script src=&quot;//cdnjscn.b0.upaiyun.com/libs/jquery/2.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; 这种写法还有一个较为学术的名字：协议相对URL（The Protocol-relative URL）。 具体作用其实很简单，也就是根据页面使用的协议来适配。例如上面的那行代码，如果页面使用的是http，则会加载： http://cdnjscn.b0.upaiyun.com/libs/jquery/2.1.1/jquery.min.js 如果是https，则加载： https://cdnjscn.b0.upaiyun.com/libs/jquery/2.1.1/jquery.min.js 就是这么简单明了！！ 参考：协议相对URL","tags":[{"name":"url","slug":"url","permalink":"https://blog.kazaff.me/tags/url/"}]},{"title":"Mac配置Apache的权限问题","date":"2014-09-22T10:54:30.000Z","path":"2014/09/22/Mac配置Apache的权限问题/","text":"说来不怕你笑话，搞了好些年的php，可还是会在配置环境的时候出现各种个样的逗比情况。今天用自己的笔记本调试一个php网站，需要搞一个vhost。配置好后发现提示我403错误： You don’t have permission to access on this server. 按照个人的条件反射，第一件事儿就是把指定的web目录设置为777权限。可是发现还是不行，查了一下网上的解决方案，竟然发现很多人都说是： &lt;Directory /&gt; Options Indexes FollowSymLinks MultiViews AllowOverride None Order deny,allow Allow from all #这里是重点 &lt;/Directory&gt; 但是对我来说，肯定没用～～这个时候我就有点抓瞎了，后来总算知道问题了，原来不仅仅要对当前web目录设置正确的权限，还要把其父目录设置正确的权限。 参考：Mac配置Apache的Vhosts权限问题","tags":[{"name":"apache","slug":"apache","permalink":"https://blog.kazaff.me/tags/apache/"},{"name":"权限","slug":"权限","permalink":"https://blog.kazaff.me/tags/权限/"},{"name":"mac","slug":"mac","permalink":"https://blog.kazaff.me/tags/mac/"},{"name":"vhost","slug":"vhost","permalink":"https://blog.kazaff.me/tags/vhost/"}]},{"title":"Dubbo协议下的单一长连接与多线程并发如何协同工作","date":"2014-09-20T15:54:30.000Z","path":"2014/09/20/dubbo协议下的单一长连接与多线程并发如何协同工作/","text":"上班的路上突然就冒出了这么个问题：既然在dubbo中描述消费者和提供者之间采用的是单一长连接，那么如果消费者端是高并发多线程模型的web应用，单一长连接如何解决多线程并发请求问题呢？其实如果不太了解socket或者多线程编程的相关知识，不太容易理解这个问题。传统的最简单的RPC方式，应该是为每次远程调用请求创建一个对应的线程，我们先不说这种方式的缺点。至少优点很明显，就是简单。简单体现在哪儿？ 通信双方一对一（相比NIO来说）。 通俗点来说，socket通信的双方发送和接受数据不会被其它（线程）干扰，这种干扰不同于数数据包的“粘包问题”。其实说白了就相当于电话线路的场景： 试想一下如果多个人同时对着同一个话筒大喊，对方接受到的声音就会是重叠且杂乱的。 对于单一的socket通道来说，如果发送方多线程的话，不加控制就会导致通道中的数据乱七八糟，接收端无法区分数据的单位，也就无法正确的处理请求。 乍一看，似乎dubbo协议所说的单一长连接与客户端多线程并发请求之间，是水火不容的。但其实稍加设计，就可以让它们和谐相处。 socket中的粘包问题是怎么解决的？用的最多的其实是定义一个定长的数据包头，其中包含了完整数据包的长度，以此来完成服务器端拆包工作。 那么解决多线程使用单一长连接并发请求时包干扰的方法也有点雷同，就是给包头中添加一个标识id，服务器端响应请求时也要携带这个id，供客户端多线程领取对应的响应数据提供线索。 其实如果不考虑性能的话，dubbo完全也可以为每个客户端线程创建一个对应的服务器端线程，但这是海量高并发场景所不能接受的~~ 那么脑补一张图： 下面咱们试图从代码中找到痕迹。 一路追踪，我们来到这个类：com.alibaba.dubbo.remoting.exchange.support.header.HeaderExchangeChannel.java，先来看看其中的request方法，大概在第101行左右： public ResponseFuture request(Object request, int timeout) throws RemotingException { if (closed) { throw new RemotingException(this.getLocalAddress(), null, &quot;Failed to send request &quot; + request + &quot;, cause: The channel &quot; + this + &quot; is closed!&quot;); } // create request. Request req = new Request(); req.setVersion(&quot;2.0.0&quot;); req.setTwoWay(true); req.setData(request); //这个future就是前面我们提到的：客户端并发请求线程阻塞的对象 DefaultFuture future = new DefaultFuture(channel, req, timeout); try{ channel.send(req); //非阻塞调用 }catch (RemotingException e) { future.cancel(); throw e; } return future; } 注意这个方法返回的ResponseFuture对象，当前处理客户端请求的线程在经过一系列调用后，会拿到ResponseFuture对象，最终该线程会阻塞在这个对象的下面这个方法调用上，如下： public Object get(int timeout) throws RemotingException { if (timeout &lt;= 0) { timeout = Constants.DEFAULT_TIMEOUT; } if (! isDone()) { long start = System.currentTimeMillis(); lock.lock(); try { while (! isDone()) { //无限连 done.await(timeout, TimeUnit.MILLISECONDS); if (isDone() || System.currentTimeMillis() - start &gt; timeout) { break; } } } catch (InterruptedException e) { throw new RuntimeException(e); } finally { lock.unlock(); } if (! isDone()) { throw new TimeoutException(sent &gt; 0, channel, getTimeoutMessage(false)); } } return returnFromResponse(); } 上面我已经看到请求线程已经阻塞，那么又是如何被唤醒的呢？再看一下com.alibaba.dubbo.remoting.exchange.support.header.HeaderExchangeHandler.java，其实所有实现了ChannelHandler接口的类都被设计为装饰器模式，所以你可以看到类似这样的代码： protected ChannelHandler wrapInternal(ChannelHandler handler, URL url) { return new MultiMessageHandler( new HeartbeatHandler( ExtensionLoader.getExtensionLoader(Dispather.class).getAdaptiveExtension().dispath(handler, url) )); } 现在来仔细看一下HeaderExchangeHandler类的定义，先看一下它定义的received方法，下面是代码片段： public void received(Channel channel, Object message) throws RemotingException { channel.setAttribute(KEY_READ_TIMESTAMP, System.currentTimeMillis()); ExchangeChannel exchangeChannel = HeaderExchangeChannel.getOrAddChannel(channel); try { if (message instanceof Request) { ..... } else if (message instanceof Response) { //这里就是作为消费者的dubbo客户端在接收到响应后，触发通知对应等待线程的起点 handleResponse(channel, (Response) message); } else if (message instanceof String) { ..... } else { handler.received(exchangeChannel, message); } } finally { HeaderExchangeChannel.removeChannelIfDisconnected(channel); } } 我们主要看中间的那个条件分支，它是用来处理响应消息的，也就是说当dubbo客户端接收到来自服务端的响应后会执行到这个分支，它简单的调用了handleResponse方法，我们追过去看看： static void handleResponse(Channel channel, Response response) throws RemotingException { if (response != null &amp;&amp; !response.isHeartbeat()) { //排除心跳类型的响应 DefaultFuture.received(channel, response); } } 熟悉的身影：DefaultFuture，它是实现了我们上面说的ResponseFuture接口类型，实际上细心的童鞋应该可以看到，上面request方法中其实实例化的就是这个DefaultFutrue对象： DefaultFuture future = new DefaultFuture(channel, req, timeout); 那么我们可以继续来看一下DefaultFuture.received方法的实现细节： public static void received(Channel channel, Response response) { try { DefaultFuture future = FUTURES.remove(response.getId()); if (future != null) { future.doReceived(response); } else { logger.warn(&quot;The timeout response finally returned at &quot; + (new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;).format(new Date())) + &quot;, response &quot; + response + (channel == null ? &quot;&quot; : &quot;, channel: &quot; + channel.getLocalAddress() + &quot; -&gt; &quot; + channel.getRemoteAddress())); } } finally { CHANNELS.remove(response.getId()); } } 留一下我们之前提到的id的作用，这里可以看到它已经开始发挥作用了。通过id，DefaultFuture.FUTURES可以拿到具体的那个DefaultFuture对象，它就是上面我们提到的，阻塞请求线程的那个对象。好，找到目标后，调用它的doReceived方法，这就是标准的java多线程编程知识了： private void doReceived(Response res) { lock.lock(); try { response = res; if (done != null) { done.signal(); } } finally { lock.unlock(); } if (callback != null) { invokeCallback(callback); } } 这样我们就可以证实上图中左边的绿色箭头所标注的两点。 接下来我们再来看看右边绿色箭头提到的两点是如何实现的？其实dubbo在NIO的实现上默认依赖的是netty，也就是说真正在长连接两端发包和接包的苦力是netty。由于哥们我对netty不是很熟悉，所以暂时我们就直接把netty当做黑箱，只需要知道它可以很好的完成NIO通信即可。 参考： Dubbo基本原理机制 ALIBABA DUBBO框架同步调用原理分析","tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://blog.kazaff.me/tags/dubbo/"},{"name":"NIO","slug":"NIO","permalink":"https://blog.kazaff.me/tags/NIO/"},{"name":"多线程","slug":"多线程","permalink":"https://blog.kazaff.me/tags/多线程/"},{"name":"Netty","slug":"Netty","permalink":"https://blog.kazaff.me/tags/Netty/"}]},{"title":"又一次新气象","date":"2014-09-05T08:13:12.000Z","path":"2014/09/05/又一次新气象/","text":"作为一个懒惰的，只会指手画脚，自己设计不来的程序猿，换一次博客皮肤是多么奢侈的一件事儿啊~~想想都头疼！不过，还是在昨晚陪同组员加班到凌晨1点多期间忙（yi）里（xin）偷（yi）闲（yi）的把node版本的hexo博客系统重新整了整，当然，由于是突发奇想的行为，所以并没有太多时间用来筹备，直接在github上找到一款个人比较喜欢的皮肤，稍加改动就成了你现在看到的样子~ 至于wp版，已经决定不在更新了，也就是说现在哥只会在node版本的这个网址下添加新的内容，有兴趣查阅老的内容的童鞋请点击顶部导航的Deprecated链接。 好吧，再扯点别的，说点什么好呢？哦，对了，我要结婚了马上~~当然，是和一个女人！婚期定在冬天，一个众人鸭绒我西装独显我瘦的季节。顺便征求一下婚礼现场的音乐清单，目前我喜欢的有： 大家还有什么推荐的吗？逗比就算了，毕竟是一个各族人民欢聚一堂的正经场合，单曲循环小苹果什么的有点太神经了~~ PS：忘记说了，发现一个非常牛叉的chrome插件：Point，实现了我多年以来一直计划开发但却迟迟下不了手的构想，哎，又一次和百万富翁擦肩而过啊~~","tags":[]},{"title":"Redis中坑爹的pattern参数","date":"2014-08-29T17:22:12.000Z","path":"2014/08/29/redis中坑爹的pattern参数/","text":"很久没有更新博客了，久的都有点忘记自己还有个博客~~ 今天和同事讨论一个问题：如何从redis的多个zset中汇总并过滤数据给客户端。你听起来可能有点费劲儿，我再来稍微描述一下场景： 假设现在我们的redis中有2个zset，分别如下： one =&gt; a,80 b,75 c,60 two =&gt; x,99 y,66 z,100 不熟悉redis或者zset用法的童鞋，请自己脑补。 我们假设 one 和 two 这两个集合中存放的是两个班级学生的名字和总分（不考虑学生重名的问题）。那么，现在我们要在系统中显示一个列表，该列表要按照分数排列显示这两个班级的学生名单，考虑到例子里的数据并不多，那我们只能假设每一页只显示3个人。 场景描述到这里，应该就足够清晰了~~ 解决方案很多，我们以最佳性能为基准，来评论方案的好坏。当然，最简单的是从分别从2个集合中取出所有数据，然后在应用系统中进行排序和截断。但不用多想，这种方法是最吃力不讨好的，为什么这么说？首先要从redis中拿出全部数据，这就已经不太合理了，还要自己实现一个排序算法~~我不往下说了！ 还可以使用zset提供的ZUNIONSTORE，调用后它会在redis中创建一个新的zset集合，不过却可以帮我们提供排序和截断方法，从另一个角度想，新建的这个zset集合其实充当了“缓存”的作用，只要在应用逻辑中先检查是否存在这个key，就避免了每次都做合并操作。但，数据更新后，如何即时的更新这个“缓存”就成了问题，这涉及到定时任务的话题，就不展开了，总之知道这种方法的利弊即可。当然你也可以说每次都做合并操作，但考虑到这个合并操作的时间复杂度是 O(N)+O(M log(M))，再加上并发请求，我可以认为，这是在玩火，但至于redis是内部如何处理并发创建集合的，是否有多线程保护等，我就不得而知了。总之，不推荐这么做。 步入正题，现在我们延伸一下场景，如果我们给出一个学生名单{a,c,x,z}，我要求你在上述方案2中拿到的汇总集合上做一次子集的排序获取，并按照前端页面的要求，根据分数顺序只取前三个用于显示。 这个名单中的学生，由于分数是错乱的，所以你该怎么办？分别获取每一个学生在集合中的score，再在应用系统中做排序和截断？这又回到了之前的那个方案1…… 好吧，我们还可以构建一个zset，如下： list =&gt; a,0 c,0 x,0 z,0 然后执行： //total代表学生总集合 //target表示我们临时创建的目标集合 zinterstore target 2 list total 这样就拿到了我们学生名单中的所有学生的zset集合，然后可以根据显示需求做分页显示了。这么做的问题我在方案2中已经提过了，就不多说了。总之觉得这么做很不尽人意！ 其实说到这里，也就没什么好办法了，不过翻了翻文档，发现redis2.8以后，提供了一个新的操作：ZSCAN。 我擦，以为找到了希望，眼睛瞬间冒了绿光。尝试这么做： zscan total a|c|x|z 3 我觉得这么做太爽了，要是真的管用，那就更好了T_T 返回的结果里，毛都没有，哇哈哈哈~为什么呢？难道是我的正则写错了？换了好几种写法，还是不行。 最后发现，原来，redis命令中所谓的pattern模式参数，并不支持正则这么强大的规则，我们可以在这里看到redis的pattern所支持的模式。 额，貌似有点悲剧的味道~~~","tags":[{"name":"redis","slug":"redis","permalink":"https://blog.kazaff.me/tags/redis/"},{"name":"pattern","slug":"pattern","permalink":"https://blog.kazaff.me/tags/pattern/"},{"name":"zinterstore","slug":"zinterstore","permalink":"https://blog.kazaff.me/tags/zinterstore/"},{"name":"zscan","slug":"zscan","permalink":"https://blog.kazaff.me/tags/zscan/"},{"name":"zset","slug":"zset","permalink":"https://blog.kazaff.me/tags/zset/"},{"name":"zunionstore","slug":"zunionstore","permalink":"https://blog.kazaff.me/tags/zunionstore/"}]},{"title":"Php页面白屏","date":"2014-08-08T11:37:12.000Z","path":"2014/08/08/php页面白屏/","text":"昨天给家正惬意着，突然朋友打电话说让我帮着看看他们的服务器，貌似出问题了~电话里描述的问题是： 访问所有的php页面都是白屏，但是静态页面却能打开老实说，我之前还真没印象碰见过这种问题，第一印象以为是apache配置的php参数有问题，但那也不会白屏啊，应该是下载php文件才对啊~好吧，我确实抓瞎了！ 先简单描述一下服务器的配置环境，其实很简单，用的是VPS，操作系统是CentOS，web环境装的是WDCP，这应该是一个很常见的产品环境下的lnamp集成套件了，提供了强大的界面管理后台，我很喜欢~~不多说了，再说就成了广告贴了！ 通过ssh登录到服务器上，简单的看了看相关的配置，确实没什么思路，相关的log也没发现什么眉目！只能求助于wdcp论坛了，按照前辈们的解决方案，开始排查，结果发现果然是由于磁盘满了造成的php页面白屏！ 非常的好奇，为什么磁盘满了，php就会白屏呢？这尼玛是不死逗比？ 既然知道原因，那么解决就不是问题了，找到造成磁盘写满的主要原因，是因为mysql的数据库文件被指定到了根目录挂载点，而vps的这个挂载点分配的很小，所以只需要把数据库文件指定到最大的挂载点即可。注意修改新路径下的文件夹权限，否则mysql可能无法正常启动哟~","tags":[{"name":"白屏","slug":"白屏","permalink":"https://blog.kazaff.me/tags/白屏/"},{"name":"wdcp","slug":"wdcp","permalink":"https://blog.kazaff.me/tags/wdcp/"},{"name":"磁盘写满","slug":"磁盘写满","permalink":"https://blog.kazaff.me/tags/磁盘写满/"}]},{"title":"Java命令行参数","date":"2014-07-28T14:53:12.000Z","path":"2014/07/28/java命令行运行参数/","text":"妈蛋，实在是顶不住部分文章的排版，看着太让人心焦了~~所以，对一篇文章重新排版一下，希望能帮助到其他人~ 先发出原文链接，供眼神好的童鞋~~ 下面是小弟排版后的内容： Java在运行已编译完成的类时，是通过java虚拟机来装载和执行的，java虚拟机通过操作系统命令JAVA_HOME&quot;bin/&quot;java -option来启动，option为虚拟机参数， JAVA_HOME为JDK安装路径，通过虚拟机参数可对虚拟机的运行状态进行调整，掌握参数的含义可对虚拟机的运行模式有更深入的理解。 如何查看参数列表虚拟机参数分为基本和扩展两类，在命令行中输入JAVA_HOME&quot;bin/&quot;java就可得到基本的参数列表，在命令行中输入JAVA_HOME&quot;bin/&quot;java -X就可以得到扩展参数列表。 基本参数说明-client/-server 这两个参数用于设置虚拟机使用何种运行模式，client模式启动比较快，但运行时性能和内存管理效率不如server模式，通常用于客户端应用程序。相反，server模式启动稍慢，但可获得更高的运行性能。 在win上，缺省的虚拟机类型为client模式，如果要使用server模式就需要在启动虚拟机时加上-server参数，以获得更高性能。对服务器应用，推荐采用server模式，尤其是多个cpu的系统。 在Linux，Solaris上缺省采用的是server模式。 -hotspot 含义与client相同，jdk1.4以前使用的参数，现在已经不再使用，取而代之的是client。 -classpath/-cp 虚拟机在运行一个类时需要将其装入内存，搜索类的方式和顺序如下： Bootstrap classes Extension Classes User Classes Bootstrap中的路径是虚拟机自带的jar或zip文件，虚拟机首选搜索这些包文件，用下面这种方式可得到虚拟机搜索的路径： System.getProperty(&quot;sun.boot.class.path&quot;) Extension位于jre的lib/ext目录下的jar文件，虚拟机在搜索完Bootstrap后就搜索该目录下的jar文件，用下面这种方式可得到虚拟机使用的Extension搜索路径： System.getProperty(&quot;java.ext.dirs&quot;) User类搜索顺序为： -classpath指定的路径 环境变量CLASSPATH 当前目录 在使用-classpath/-cp时，多个目录之间用分号分隔。推荐使用该命令来指定虚拟机要搜索的类路径，而不是依赖环境变量，以减少多个项目同时使用环境变量时存在的潜在冲突（多版本库）。 可在运行时通过下面的代码获取虚拟机查找类的路径： System.getProperty(&quot;java.class.path&quot;) -D=value 在虚拟机的系统属性中设置属性名/值对，运行在此虚拟机上的应用程序可用： System.getProperty(&quot;属性名&quot;) 得到value的值。 如果value中有空格，则需要用双引号将该值括起来，如：-Dname=&quot;kazaf f&quot;。 该参数通常用于设置系统级全局变量值，如配置文件路径，保证该属性在程序中任何地方都可访问。 -verbose[:class|gc|jni] 在输出设备上显示虚拟机运行信息。 -verbose和-verbose:class含义相同，表示输出虚拟机装入的类的信息，格式如下： [Loaded java.io.FilePermission$1 from shared objects file] 当虚拟机报告类找不到或类冲突时，用此参数来查看虚拟机装入类的情况。 -verbose:gc用于在虚拟机发生内存回收时在输出设备上显示信息，格式如下： [Full GC 268K-&gt;168K(1984K), 0.0187390 secs] -verbose:jni用于在虚拟机调用native方法时在设备上输出显示信息，格式如下： [Dynamic-linking native method HelloNative.sum ... JNI] 该参数用于监视虚拟机调用本地方法的情况，在发生jni错误时可以为诊断提供便利。 -ea[:…|:]/-enableassertions[:…|:] 从jdk1.4开始，java可支持断言机制，用于诊断运行时问题。通常在测试阶段使断言有效，在线上运行时不需要运行断言。断言后的表达式的值是一个逻辑值，为true时断言不运行，为false时断言抛出java.lang.AssertionError错误。 这个参数就是用来设置虚拟机是否启动断言机制，缺省时虚拟机关闭断言机制，用-ea可打开断言机制，不加packagename和classname表示运行所有包和类中的断言，如果希望只是运行某些包或类中的断言，可将包名或类名加到-ea之后，例如：-ea:com.foo.util。 -da[:…|:]/-disableassertions[:…|:]用来设置虚拟机关闭断言处理，用法与-ea类似。 -esa/-enablessystemassertions 设置虚拟机开启系统类的断言。 -dsa/-disablesystemassertions 设置虚拟机关闭系统类的断言。 -agentlib:[=] 该参数是jdk5新引入的，用于虚拟机装载本地代理库。其中libname为本地代理库文件名，虚拟机的搜索路径为环境变量path中的路径，options为传给本地库启动时的参数，多个参数之间用逗号分隔。 在win平台中，虚拟机搜索本地库后缀名名为.dll，在Unix上则为.so文件。例如可以用-agentlib:hprof来获取虚拟机的运行情况。可用-agentlib:hprof=help来得到使用帮助列表（确保在win平台下的jre的lib目录下存在hprof.dll文件）。 -agentpath:[=] 设置虚拟机按全路径装载本地库，不再搜索PATH中的路径，其他功能同-agentlib。 -javaagent:[=] 虚拟机启动时装入java语言设备代理。jarpath文件中的mainfest文件必须有Agent-Class属性。代理类要实现 public static void premain(String agentArgs, Instrumentation inst) 方法。当虚拟机初始化时，将按照代理类的说明顺序调用premain方法。 扩展参数说明-Xmixed 设置-client模式虚拟机对使用频率高的方法进行Just-In-Time编译和执行，对其他方法使用解释方式执行，该方式是虚拟机缺省模式。 -Xint 设置-client模式下运行的虚拟机以解释方式执行类的字节码，不将字节码编译为本机码。 -Xbootclasspath[/a|/p]:path 改变虚拟机装载缺省系统运行包rt.jar的路径，从-Xbootclasspath中设定的搜索路径中装载运行类。除非你自己能写一个运行时，否则不会用到这个参数。 其中/a将在缺省搜索路径后加上path中的搜索路径，而/p在缺省路径前先搜索path中的路径。 -Xnoclassgc 关闭虚拟机对class的垃圾回收功能。 -Xincgc 启动增量垃圾收集器，缺省是关闭的。增量垃圾收集器能减少偶尔发生的长时间的垃圾回收造成的暂停时间。但增量垃圾收集器和应用程序并发执行，因此会占用部分CPU在应用程序上的功能。 -Xloggc: 将虚拟机每次垃圾回收的信息写到日志文件中，文件名由file指定，内容和-verbose:gc输出内容相同。 -Xbatch 虚拟机的缺省运行方式是在后台编译类代码，然后在前台执行代码，使用该参数将关闭虚拟机后台编译，在前台编译完成后再执行。 -Xms 设置虚拟机可用内存堆的初始大小，缺省单位为字节。该大小为1024的整数倍并且要大于1MB，可用k(K)或m(M)为单位来设置较大的内存数。初始堆的大小为2MB。例如：-Xms6400K、-Xms256M -Xmx 设置虚拟机内存堆的最大可用大小，缺省单位为字节，缺省堆最大值为64MB。该值必须为1024整数倍并且要大于2MB，可用k(K)或m(M)为单位来设置较大的内存数。 当应用程序申请了大内存，运行时虚拟机抛出java.lang.OutOfMemoryError: Java heap space错误，就需要用改参数设置更大的内存数。 -Xss 设置线程栈的大小，缺省单位为字节，通常操作系统分配给线程栈的缺省大小为1MB。另外，也可以在java中创建线程对象时设置栈的大小，构造函数原型为： Thread(ThreadGroup group, Runnable target, String name, long stackSzie) -Xprof 输出CPU运行时的诊断信息。 -Xfuture 对类文件进行严格格式检查，以保证类代码符合类代码规范。为保证向后兼容，虚拟机缺省不进行严格的格式检查。 -Xrs 减少虚拟机中操作系统的信号的使用。该参数通常在虚拟机以后台服务方式运行时使用（如Serlet）。 -Xcheck:jni 对jni函数执行检查。 -Xshare:off 不尝试使用共享类的数据 -Xshare:auto 在可能的情况下使用共享类的数据，默认值。 -Xshare:on 要求使用共享类的数据，否则运行失败。","tags":[{"name":"jvm","slug":"jvm","permalink":"https://blog.kazaff.me/tags/jvm/"}]},{"title":"是什么系列之RMI","date":"2014-07-18T09:18:12.000Z","path":"2014/07/18/是什么系列之RMI/","text":"转型Java阵营没多久，就面对企业级的需求，所以大量的调研内容，“是什么系列”几乎都是针对javaEE领域的姿势，可见我最近是多么的充实啊~这次记录一下关于RMI的科普知识，其实它比起之前的Thrift和Avro来说，已经非常的轻量级了~~ RMI（Remote Method Invocation）是jdk1.1就已经存在的分布式应用解决方案，轻量简单是它最大的特点，广泛的应用在EJB中。如果我们的系统比较小，服务很少且很轻，并且不需要面向跨语言，那么RMI是一个比较理想的选择。 在另外的一些场景，例如跨语言平台的分布式应用，RMI就显得过于简陋了，这个时候可能就需要其它的同类技术了~ RMI是基于TCP协议的传递可序列化java对象字节数据的库，因此，在同等业务数据量的前提下，RMI的效率要高于基于SOAP规范的WebService。因此，RMI可以用在业务结构简单，要求实时性高的分布式应用中。 设计角度上，java采用三层结构来实现RMI：客户端、服务端、注册表，这是很常见的基于服务的架构。具体细节可以从下图看出： 客户端一方包含： 桩（Stub）：远程对象在客户端上的代理； 远程引用层（Remote Reference Layer）：解析并执行远程引用协议，完成了调用的方法与服务对应地址的转换； 传输层（Transport）：发送调用、传递远程方法参数、接受远程方法执行结果。 服务端一方包含： 骨架（Skeleton）：读取客户端传递的方法参数，调用服务器端的实际对象方法，并接受方法执行后的返回值； 远程引用层（Remote Reference Layer）：处理远程引用语法之后向骨架发送远程方法调用； 传输层（Transport）：监听客户端的入站连接，接受并转发调用到远程引用层。 注册表（Registry）：以URL形式注册远程对象，并向客户端回复远程对象的引用。 在实际的应用中，客户端并没有真正的和服务端直接对话来进行远程调用，而是通过本地JVM的桩对象来进行的。 客户端从远程服务器的注册表中查询并获得远程对象引用，当进行远程调用时，客户端首先会与桩对象进行对话，而这个桩对象将远程方法所需的参数序列化后，传递给它下层的远程引用层。 桩对象与远程对象具有相同的接口和方法列表，当客户端调用远程对象时，实际是由相对应的桩对象代理完成的。远程引用层在将桩的本地引用转换为服务器上对象的远程引用后，再将调用传递给传输层，有传输层通过tcp协议发送调用。 在服务器端，传输层监听入站连接，它一旦接受到客户端远程调用后，就将这个引用转发给其上层的远程引用。 服务端的远程引用层将客户端发送的远程引用转化为本地虚拟机的引用后，再将请求传递给骨架。 骨架读取参数，将请求传递给服务器，最后由服务器进行实际的方法调用。 如果远程方法调用后有返回值，则服务器将这些结果又沿着骨架-&gt;远程引用层-&gt;传输层向下传递。 客户端的传输层接受到返回值后，又沿着传输层-&gt;远程引用层-&gt;桩向上传递，最后由桩来反序列化这些返回值，并将最终结果传递给客户端程序。 从这个流程上可以看出，java为我们隐藏了很多处理细节，开发者可以只关注业务细节。 关于RMI的例子，网上非常的多，包括上面的内容，出自这里。","tags":[{"name":"分布式","slug":"分布式","permalink":"https://blog.kazaff.me/tags/分布式/"},{"name":"是什么系列","slug":"是什么系列","permalink":"https://blog.kazaff.me/tags/是什么系列/"},{"name":"tcp","slug":"tcp","permalink":"https://blog.kazaff.me/tags/tcp/"}]},{"title":"是什么系列之Thrift","date":"2014-07-07T16:18:12.000Z","path":"2014/07/07/是什么系列之Thrift/","text":"这次我们要科普的，是Thrift，它是Facebook的一个开源项目，定位为一个跨语言的远程服务调用框架。目前流行的服务调用方式有很多种，例如基于SOAP消息格式的Web Service，基于JSON消息格式的RESTful服务等，其中所用到的数据传输格式包括XML，JSON等，然后XML相对体积太大，传输效率底，JSON体积小，但还不够完善。针对高并发，大数据量和多语言的需求，传输二进制格式的Thrift更有优势。 Thrift有一个代码生成器来对它所定义的IDL文件自动生成服务代码框架，用户只需要在其之上进行二次开发即可，对底层的RPC通信等都是透明的。目前它支持的语言有C++，Java，Python，PHP，Ruby，Erlang，Perl，Haskell，C#，Cocoa，Smalltalk，等。（妈蛋，真强悍啊！） 现在看一个Thrift的IDL定义，Hello.thrift文件： namespace java service.demo service Hello{ string helloString(1:string para) i32 helloInt(1:i32 para) bool helloBoolean(1:bool para) void helloVoid() string helloNull() } 上面的这段代码定义了了服务Hello的5个方法，每个方法包含方法名，参数表和返回类型，每个参数包含参数序号，参数类型及参数名。使用Thrift工具编译Hello.thrift，就会生成响应的Hello.java文件，该文件包含了在Hello.thrift文件中描述的服务Hello的接口定义：Hello.Iface接口，以及服务调用的底层通信细节，包括客户端的调用逻辑Hello.Client以及服务器端的处理逻辑Hello.Processor，用于构建客户端和服务器端的功能。 如图所示，图中黄色部分是用户实现的业务逻辑，褐色部分是根据Thrift定义的服务接口描述文件生成的客户端和服务器端代码框架，红色部分是根据Thrift文件生成代码实现数据的读写操作。红色部分以下是Thrift的传输体系、协议以及底层I/O通信，使用Thrift可以很方便的定义一个服务并且选择不同的传输协议和传输层而不用重新生成代码。 Thrift服务器包含用于绑定协议和传输层的基础架构，它提供阻塞、非阻塞、单线程和多线程的模式运行在服务器上，可以配合服务器/容器一起运行，可以和现有的J2EE服务器/Web容器无缝的结合。 该图所示是HelloServiceServer启动的过程以及服务被客户端调用时，服务器的响应过程。从图中我们可以看到，程序调用了TThreadPoolServer的serve方法后，server进入阻塞监听状态，其阻塞在TServerSocket的accept方法上。当接收到来自客户端的消息后，服务器发起一个新线程处理这个消息请求，原线程再次进入阻塞状态。在新线程中，服务器通过TBinaryProtocol协议读取消息内容，调用HelloServiceImpl的helloVoid方法，并将结果写入helloVoid_result中传回客户端。 该图所示是HelloServiceClient调用服务的过程以及接收到服务器端的返回值后处理结果的过程。从图中我们可以看到，程序调用了Hello.Client的helloVoid方法，在helloVoid方法中，通过send_helloVoid方法发送对服务的调用请求，通过recv_helloVoid方法接收服务处理请求后返回的结果。 上述过程对应的代码如下： 服务器端： package service.server; import org.apache.thrift.TProcessor; import org.apache.thrift.protocol.TBinaryProtocol; import org.apache.thrift.protocol.TBinaryProtocol.Factory; import org.apache.thrift.server.TServer; import org.apache.thrift.server.TThreadPoolServer; import org.apache.thrift.transport.TServerSocket; import org.apache.thrift.transport.TTransportException; import service.demo.Hello; import service.demo.HelloServiceImpl; public class HelloServiceServer { /** * 启动 Thrift 服务器 * @param args */ public static void main(String[] args) { try { // 设置服务端口为 7911 TServerSocket serverTransport = new TServerSocket(7911); // 设置协议工厂为 TBinaryProtocol.Factory Factory proFactory = new TBinaryProtocol.Factory(); // 关联处理器与 Hello 服务的实现 TProcessor processor = new Hello.Processor(new HelloServiceImpl()); TServer server = new TThreadPoolServer(processor, serverTransport, proFactory); System.out.println(&quot;Start server on port 7911...&quot;); server.serve(); } catch (TTransportException e) { e.printStackTrace(); } } } 客户端： package service.client; import org.apache.thrift.TException; import org.apache.thrift.protocol.TBinaryProtocol; import org.apache.thrift.protocol.TProtocol; import org.apache.thrift.transport.TSocket; import org.apache.thrift.transport.TTransport; import org.apache.thrift.transport.TTransportException; import service.demo.Hello; public class HelloServiceClient { /** * 调用 Hello 服务 * @param args */ public static void main(String[] args) { try { // 设置调用的服务地址为本地，端口为 7911 TTransport transport = new TSocket(&quot;localhost&quot;, 7911); transport.open(); // 设置传输协议为 TBinaryProtocol TProtocol protocol = new TBinaryProtocol(transport); Hello.Client client = new Hello.Client(protocol); // 调用服务的 helloVoid 方法 client.helloVoid(); transport.close(); } catch (TTransportException e) { e.printStackTrace(); } catch (TException e) { e.printStackTrace(); } } } HelloServiceImpl.java： package service.demo; import org.apache.thrift.TException; public class HelloServiceImpl implements Hello.Iface { @Override public boolean helloBoolean(boolean para) throws TException { return para; } @Override public int helloInt(int para) throws TException { try { Thread.sleep(20000); } catch (InterruptedException e) { e.printStackTrace(); } return para; } @Override public String helloNull() throws TException { return null; } @Override public String helloString(String para) throws TException { return para; } @Override public void helloVoid() throws TException { System.out.println(&quot;Hello World&quot;); } } 数据类型Thrift脚本可定义的数据类型包括以下几种： 基本类型 bool：对应java的boolean，true或false byte：对应java的byte，8位有符号整数 i16：对应java的short，16位有符号整数 i32：对应java的int，32位有符号整数 i64：对应java的long，64位有符号整数 double：对应java的double，64位浮点数 string：对应java的String，未知编码文本或二进制字符串 结构体类型 struct：定义公共的对象，对应Java的一个JavaBean，类似C语言中的结构体定义 容器类型 list：对应java的ArrayList set：对应java的HashSet map：对应java的HashMap 异常类型 exception：对应java的Exception 服务类型 service：对应服务的类 协议Thrift可以让用户选择客户端与服务端之间传输通信协议的类别，在传输协议上总体划分为文本（text）和二进制（binary）传输协议，为节约带宽，提高传输效率，一般情况下使用二进制类型的传输协议为主。常用的协议如下： TBinaryProtocol：二进制编码格式传输协议 TCompactProtocol：高效率的，密集的二进制编码格式传输协议 TJSONProtocol：使用JSON的数据编码传输协议 TSimpleJSONProtocol：只提供JSON只写的协议，适用于通过脚本语言解析 传输层常用的传输层有： TSocket：使用阻塞式I/O进行传输 TFramedTransport：使用非阻塞方式，按照块的大小进行传输，类似JAVA的NIO TNonblockingTransport：使用非阻塞方式，用于构建异步客户端 部署 从图中可以看出，客户端和服务端部署时，都需要用到公共的jar包和java文件（图中绿色区域）。 上文内容抄于这里。 可以看出，Thrift比Avro要简单，至少从例子的角度上来看，RPC的调用方式更直观一些~","tags":[{"name":"序列化","slug":"序列化","permalink":"https://blog.kazaff.me/tags/序列化/"},{"name":"json","slug":"json","permalink":"https://blog.kazaff.me/tags/json/"},{"name":"schema","slug":"schema","permalink":"https://blog.kazaff.me/tags/schema/"},{"name":"编码","slug":"编码","permalink":"https://blog.kazaff.me/tags/编码/"},{"name":"rpc","slug":"rpc","permalink":"https://blog.kazaff.me/tags/rpc/"},{"name":"是什么系列","slug":"是什么系列","permalink":"https://blog.kazaff.me/tags/是什么系列/"}]},{"title":"是什么系列之Avro","date":"2014-07-07T11:13:12.000Z","path":"2014/07/07/是什么系列之Avro/","text":"今天来关注一下Avro，目的是想接触一下跨端RPC中间件中关于数据编解码及传输的相关技术，这和我目前负责的项目很有关系！那么先从网上找一些相关的文献来给自己科普一下~Avro是Hadoop的一个子项目，也是Apache中的一个独立项目，它是一个基于二进制数据传输高性能的中间件。在Hadoop的其他项目中（Hbase，Hive）的客户端与服务端的数据传输中被大量采用。听上去很给力啊？！ Avro是一个数据序列化的系统，它可以将数据结构或对象转化成便于存储或传输的格式，Avro设计之初就定位为用来支持数据密集型应用，适用于远程或本地大规模数据的存储于交换，Avro支持的编程语言包括：C/C++，JAVA，Python，Ruby，PHP，C#，它的特点有： 丰富的数据结构类型 快速可压缩的二进制数据形式（对数据二进制序列化后可节约数据存储空间和网络传输带宽） 存储持久数据的文件容器 可实现远程过程调用(RPC) 简单的动态语言结合功能 Avro和动态语言结合后，读写数据文件和使用RPC协议都不需要生成代码，而代码生成作为一种可选的优化只需要在静态类型语言中实现。 Avro依赖于模式（Schema），通过模式定义各种数据结构，只有确定了模式才能对数据进行解释，所以在数据的序列化和反序列化之前，必须先确定模式的结构。同时可动态加载相关数据的模式，数据的读写都使用模式，这使得数据之间不存在任何其他标识（类比Thrift），这样就减少了开销，使得序列化快速又轻巧，同时这种数据及模式的自我描述也方便了动态脚本语言的使用。 当数据存储到文件中时，它的模式也随之存储，这样任何程序都可以对文件进行处理。如果读取数据时使用的模式与写入时使用的模式不同，也容易解决，因为读取和写入的模式都是已知的。看下面这个规则： 新模式WriterReader规则增加了字段采用旧的模式（新增前）采用新的模式Reader对新字段会使用其默认值（Writer不会为新增的字段赋值）采用新的模式（新增后）采用旧的模式Reader会忽略掉新曾的字段，Writer会为新增字段赋值减少了字段采用旧的模式（减少前）采用新的模式Reader会忽略已经被删除的字段采用新的模式（减少后）采用旧的模式如果旧模式里被删除的那个字段有默认值，那么Reader会采用，否则，Reader会报错 类型数据类型标准化的意义在于： 使不同系统对相同的数据能够正确解析 有利于数据序列化/反序列化 Avro定义了几种简单数据类型，包括：null，boolean，int，long，float，double，bytes，string。简单数据类型有类型名称定义，不包含属性信息，如： {“type”: “string”} Avro定义了六种复杂数据类型，每一种复杂数据类型都具有独特的属性，如下： Records： { &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;名称json字符串[必填]&quot;, &quot;namespace&quot;: &quot;命名空间json字符串[选填]&quot;, &quot;doc&quot;: &quot;文档json字符串[选填]&quot; &quot;aliases&quot;: &quot;别名json字符串数组[选填]&quot;, &quot;fields&quot;: [ //json数组[必填] { &quot;name&quot;: &quot;字段名&quot;, &quot;type&quot;: &quot;类型&quot;, &quot;default&quot;: &quot;默认值&quot;, &quot;order&quot;: &quot;字段顺序&quot; }, .... ] } 例如： { &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;test&quot;, &quot;fields&quot;: [ {&quot;name&quot;: &quot;a&quot;, &quot;type&quot;: &quot;long&quot;}, {&quot;name&quot;: &quot;b&quot;, &quot;type&quot;: &quot;string&quot;} ] } Enums： { &quot;type&quot;: &quot;enum&quot;, &quot;name&quot;: &quot;名称json字符串[必填]&quot;, &quot;namespace&quot;: &quot;命名空间json字符串[选填]&quot;, &quot;doc&quot;: &quot;文档json字符串[选填]&quot; &quot;aliases&quot;: &quot;别名json字符串数组[选填]&quot;, &quot;symbols&quot;: [ //json数组[必填] &quot;值json字符串[必填]，所有值必须是唯一的&quot; ] } 例如： { &quot;type&quot;: &quot;enum&quot;, &quot;name&quot;: &quot;test&quot;, &quot;symbols&quot;: [&quot;k&quot;, &quot;a&quot;, &quot;z&quot;, &quot;a&quot;, &quot;ff&quot;] } Arrays： { &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;子元素模式&quot; } 例如： { &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;long&quot; } Maps： { &quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;值元素模式&quot; } Fixed： { &quot;type&quot;: &quot;fixed&quot;, &quot;name&quot;: &quot;名称json字符串[必填]&quot;, &quot;namespace&quot;: &quot;命名空间json字符串[选填]&quot;, &quot;aliases&quot;: &quot;别名json字符串数组[选填]&quot;, &quot;size&quot; : &quot;整型，指定每个值的字节数[必填]&quot; } Unions： json数组 序列化/反序列化Avro指定两种数据序列化编码方式：binary和json。使用二进制编码会高效序列化，并且序列化后得到的结果会比较小，而json一般用于调试系统或基于web应用。 简单数据类型： 类型编码例子null0字节Nullboolean1字节{true: 1, false: 0}int/longvariable-length zig-zag codingfloat4字节Java’s floatToIntBitsdouble8字节Java’s floatToIntBitsbytes一个表示长度的long值，后跟指定长度的字节序列string一个表示长度的long值，后跟UTF-8字符集的指定长度的字节序列“foo”:{3,f,o,o} 复杂数据类型： records类型会按字段声明的顺序串连编码值，例如下面这个record schema： { &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;test&quot;, &quot;fields&quot;: [ {&quot;name&quot;: &quot;a&quot;, &quot;type&quot;: &quot;long&quot;}, {&quot;name&quot;: &quot;b&quot;, &quot;type&quot;: &quot;string&quot;} ] } 实例化这个record，假设给a字段赋值27(编码为0x36)，给b字段赋值“foo”(06 66 6f 6f，注意第一个06表示字符串长度3的编码值)，那么这个record编码结果为： 36 06 66 6f 6f enum被编码为一个int，比如： { &quot;type&quot;: &quot;enum&quot;, &quot;name&quot;: &quot;test&quot;, &quot;symbols&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;] } 这将被编码为一个取值范围为[0，3]的int，0表示A，3表示D。 arrays编码为block序列，每个block包含一个long的count值，紧跟着的是array items，一个block的count为0表示该block是array的结尾。 maps编码为block序列，每个block包含一个long的count值，紧跟着的是key/value对，一个block的count为0表示该block是map的结尾。 union编码以一个long值开始，表示后面的数据时union中的哪种数据类型。 fixed编码为指定数目的字节。 上图表示的是Avro本地序列化和反序列化流程，Avro将用户定义的模式和具体的数据编码成二进制序列存储在对象容器文件中，例如用户定义了包含学号，姓名，院系和电话的学生模式，而Avro对其进行编码后存储在student.db文件中，其中存储数据的模式放在文件头的元数据中，这样读取的模式即使与写入的模式不同，也可以迅速的读取数据。假如另一个程序需要获取学生的姓名和电话，只需要定义包含姓名和电话的学生模式，然后用此模式去读取容器文件中的数据即可。 排序Avro为数据定义了一个标准的排列顺序，“比较”在很多时候都是经常被使用的对象之间的炒作，标准定义可以进行方便有效的比较和排序，同时标准的定义可以方便对Avro的二进制编码数据直接进行排序而不需要反序列化。 只有当数据项包含相同的Schema时，数据之间的比较才有意义，比较按照Schema深度优先，从左至右的顺序递归进行，找到第一个不匹配即可终止比较。 两个拥有相同模式的项的比较按照以下规则进行： null：总是相等 int,long,float：按照数值大小 boolean：flase在true之前 string：按照字典顺序 bytes，fixed：按照byte的字典顺序 array：按照元素的字典顺序 enum：按照符号在枚举中的位置 record：按照域的字典顺序，如果指定了以下属性： ascending：域值的顺序不变 descending：域值的顺序颠倒 ignore：排序时忽略域值 map：不可进行排序比较 对象容器文件Avro定义了一个简单的对象容器文件格式，一个文件对应一个模式，所有存储在文件中的对象都是根据模式写入的，对象按照块进行存数，块可以采用压缩的方式存储。为了在进行MapReduce处理的时候有效的切分文件，在块之间采用了同步记号。 一个文件有两部分组成：文件头(Header)和一个或多个文件的数据块(Data Block)。而头信息由由三部分构成：四个字节的前缀，文件Meta-data信息和随机生成的16字节同步标记符。目前Avro支持的Meta-data有两种：schema和codec。 codec表示对后面的文件数据块采用何种压缩方式。Avro的实现都需要支持下面两种压缩方式：null（不压缩）和deflate（使用Deflate算法压缩数据块）。除了文档中认定的两种Meta-data，用户也可以自定义适用于自己的Meta-data，这里用long型来表示有多少个Meta-data数据对，也是让用户在实际应用中可以定义足够的Meta-data信息。 对于每对Meta-data信息，都有一个string型的key（要以“avro.”为前缀）和二进制编码后的value。由于对象可以组织成不同的块，使用时就可以不经过反序列化而对某个数据块进行操作。还可以由数据块数，对象数和同步标记符来定位损坏的块以确保数据完整性。 RPC当在RPC中使用Avro时，服务器和客户端可以在握手连接时交换模式，服务器和客户端有彼此全部的模式，因此相同命名字段，缺失字段和多余字段等信息之间通信中需要处理的一致性问题就可以解决。 客户端希望同服务器端交互时，就需要交换双方通信的协议，它类似于模式，需要双方来协商定义，在Avro中被称为消息。 上图所示，协议中定义了用于传输的消息，消息使用框架后放入缓冲区中进行传输，由于传输的开始就交换了各自的协议定义，因此数据就能够正确解析。所谓传输的开始，也就是很重要的握手阶段。 握手的过程是确保Server和Client获得对方的Schema定义，从而使Server能够正确反序列化请求信息，Client能够正确反序列化响应信息。通常，Server/Client会缓存最后使用到的一些协议格式，所以大多数情况下，握手过程不需要交换整个Schema文本。 所有的RPC请求和响应处理都建立在已经完成握手的基础上，对于无状态的连接，所有的请求响应之前都附有一次握手过程，而对于有状态的连接，一次握手完成，整个连接的生命期内都有效。 下面看一下具体握手过程： Client发起HandshakeRequest，其中包含Client本身SchemaHash值和对应Server端的SchemaHash值，例如：（clientHash!=null,clientProtocol=null,serverHash!=null），如果本地缓存有ServerHash值则直接填充，如果没有则通过猜测填充； Server用如下之一的HandshakeResponse响应Client请求： （match=BOTH,serverProtocol=null,serverHash=null）：表示当Client发送正确的ServerHash值且Server缓存中存在相应的ClientHash，则握手过程完成； （match=CLIENT,serverProtocol!=null,serverHash!=null）：表示当Server缓存有Client的Schema，但Client请求中的ServHash值不正确。Server发送Server端的Schema数据和相应的Hash值，此次握手完成。 （match=NONE）：表示当Client发送的ServerHash不正确且Server端没有Client的Schema缓存。这种情况下Client需要重新提交请求信息（clientHash!=null,clientProtocol!=null,serverHash!=null），Server根据实际情况给予正确的响应，握手完成。 握手过程使用的Schema结构如下： { &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;HandshakeRequest&quot;, &quot;namespace&quot;: &quot;org.apache.avro.ipc&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;clientHash&quot;, &quot;type&quot;: { &quot;type&quot;: &quot;fixed&quot;, &quot;name&quot;: &quot;MD5&quot;, &quot;size&quot;: 16 } }, {&quot;name&quot;: &quot;clientProtocol&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;]}, {&quot;name&quot;: &quot;serverHash&quot;, &quot;type&quot;: &quot;MD5&quot;}, { &quot;name&quot;: &quot;meta&quot;, &quot;type&quot;: [ &quot;null&quot;, { &quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;bytes&quot; } ] } ] } { &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;HandshakeResponse&quot;, &quot;namespace&quot;: &quot;org.apache.avro.ipc&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;match&quot;, &quot;type&quot;: { &quot;type&quot;: &quot;enum&quot;, &quot;name&quot;: &quot;HandshakeMatch&quot;, &quot;symbols&quot;: [&quot;BOTH&quot;, &quot;CLIENT&quot;, &quot;NONE&quot;] } }, {&quot;name&quot;: &quot;serverProtocol&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;]}, { &quot;name&quot;: &quot;serverHash&quot;, &quot;type&quot;: { &quot;type&quot;: &quot;fixed&quot;, &quot;name&quot;: &quot;MD5&quot;, &quot;size&quot;: 16 } }, { &quot;name&quot;: &quot;meta&quot;, &quot;type&quot;: [ &quot;null&quot;, { &quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;bytes&quot; } ] } ] } 消息从客户端发送到服务器端需要经过传输层（Transport Layer），它发送消息并接受服务器端响应。到达传输层的数据就已经是二进制数据了，通常以HTTP作为传输模型，数据以POST方式发送给对方，在Avro中，消息被封装成一组Buffer，如下图所示： 每个Buffer以四个字节开头，中间是多个字节的数据，最后以一个空缓冲区结尾。 协议定义Avro协议描述了RPC接口，和Schema一样，用JSON定义，有以下属性： protocol，必需，协议名称 namespace，可选，协议的命名空间 doc，可选，描述这个协议的字符串 types，可选，定义协议类型名称列表 messages，可选，一个json对象，key是message名称，value是json对象 doc，可选，对消息的说明 request，参数列表，其中参数拥有名字和类型 response，响应数据的schema error，可选，用一个declared union来描述，error类型的定义和record一样，除了它使用error，而不是record one-way，可选，布尔类型，当response 类型是null，并且没有列出error时，one-way parameter只能是true 实例可以参考这篇文章，写得非常详细：这里","tags":[{"name":"序列化","slug":"序列化","permalink":"https://blog.kazaff.me/tags/序列化/"},{"name":"json","slug":"json","permalink":"https://blog.kazaff.me/tags/json/"},{"name":"schema","slug":"schema","permalink":"https://blog.kazaff.me/tags/schema/"},{"name":"编码","slug":"编码","permalink":"https://blog.kazaff.me/tags/编码/"},{"name":"rpc","slug":"rpc","permalink":"https://blog.kazaff.me/tags/rpc/"},{"name":"是什么系列","slug":"是什么系列","permalink":"https://blog.kazaff.me/tags/是什么系列/"}]},{"title":"是什么系列之Servlet","date":"2014-07-03T11:37:12.000Z","path":"2014/07/03/是什么系列之Servlet/","text":"公司已经开始大面积转型j2ee，正在使用的是整套java web的技术解决方案：Tomcat+springMVC+jdbc。作为一个从php转型过来的程序猿，还是需要好好消化一下java web开发的一些基础概念的。实不相瞒，我就一直闹不懂什么是Servlet，总是无法正确的理解它的作用及位置~刚好今天有时间，就找了篇不错的技术贴好好科普一下，下面记录一些相关重点。 首先，先看一下我们要解决的疑惑： 以Tomcat为例了解Servlet容器是如何工作的？ 一个Web工程在Servlet容器中是如何启动的？ Servlet容器如何解析你在web.xml中定义的Servlet？ 用户的请求是如何被分配给指定的Servlet的？ Servlet容器如何管理Servlet生命周期？ 先从Servlet容器聊起Servlet和Servlet容器，从名字上来猜，就想水和器皿，之所以独立出两个东西，完全是为了满足工业化生产，也就是说是为了解耦，通过标准接口来让这两个东西协作从而完成实际需求。 目前成熟的Servlet容器产品很多，像Jetty，Tomcat都是java web开发人员耳熟能详的。我们下面就以Tomcat为例来讲一下Servlet容器如何管理Servlet的。 从上图可以看出，Tomcat的容器分为四个等级，真正管理Servlet的是Context容器，可以看到，一个Context对应一个web工程，这在Tomcat配置文件里也可以很容易的发现这一点： &lt;Context path=&quot;/projectOne &quot; docBase=&quot;D:\\projects\\projectOne&quot; reloadable=&quot;true&quot; /&gt; Servlet容器的启动过程我们已经知道，一个Web项目对应一个Context容器，也就是Servlet运行时的Servlet容器，添加一个Web应用时将会创建一个StandardContext容器，并且给这个Context容器设置别要的参数（url，path等）。 下面看一张时序图，来分析一下Tomcat的启动过程： 上图描述的主要是类之间的时序关系，我们主要关注StandardContext容器的启动过程。 当Context容器初始化状态设为init时，添加在Context容器上的Listener将会被调用，我们主要看一下ContextConfig做了什么，这个类会负责整个Web应用的配置文件的解析工作： 创建用于解析xml配置文件的contextDigester对象 读取默认context.xml配置文件，如果存在则解析它 读取默认Host配置文件，如果存在则解析它 读取默认Context自身的配置文件，如果存在则解析它 设置Context的DocBase ContextConfig的init方法完成后，Context容器会执行startInternal方法，主要做的是： 创建读取资源文件的对象 创建ClassLoader对象 设置应用的工作目录 启动相关的辅助类，如：logger，realm，resources等 修改启动状态，通知感兴趣的观察者（web应用的配置） 子容器的初始化 获取ServletContext并设置必要的参数 初始化“load on startup”的Servlet Web应用的初始化工作实在ContextConfig的configureStart方法中实现的，应用的初始化主要是要解析web.xml文件，这个文件描述了一个web应用的关键信息，也是一个web应用的入口。 Tomcat首先会创建globalWebXml对象，接着是hostWebXml对象，再然后是对应应用的WebXml对象。Tomcat会将ContextConfig从web.xml文件中解析出的各项属性保存在WebXml对象中。再然后会将WebXml对象中的属性设置到Context容器中，这里面包括创建的Servlet对象，filter，listener等等，这个工作在WebXml的configureContext方法中完成，在该方法中会将Servlet包装成Context容器中的StandardWrapper，为什么要将Servlet包装成StandardWrapper而不直接创建Servlet对象呢？这是因为StandardWrapper是Tomcat容器的一部分，它具有容器的特征，而Servlet是一个独立的Web开发标准，不应该强耦合在Tomcat中。 除了将Servlet包装成StandardWrapper并作为子容器添加到Context中，其它的所有web.xml属性都被解析到Context中，所以说Context容器才是真正运行Servlet的Servlet容器。一个Web应用对应一个Context容器，容器的配置属性由应用的web.xml指定，这样我们就能理解web.xml到底起到什么作用了。 创建Servlet实例我们已经完成了Servlet的解析工作，并且被包装成StandardWrapper添加在Context容器中，但它仍然不能工作，因为我们还没有实例化它。 如果Servlet的load-on-startup配置项大于0，那么在Context容器启动的时候就会实例化它，之前提到在解析配置文件时会读取在Tomcat目录下的Conf文件夹下的Web.xml文件的默认配置项，用来创建globalWebXml，其定义了两个Servlet，分别是：org.apache.catalina.servlets.DefaultServlet 和 org.apache.jasper.servlet.JspServlet 它们的 load-on-startup 分别是 1 和 3，也就是当Tomcat启动时这两个Servlet就会被实例化。 创建Servlet实例的方法是从Wrapper.loadServlet开始的，该方法要完成的就是获取servletClass，然后把它交给InstanceManager去创建一个基于servletClass.class的对象，并且如果这个Servlet配置了jsp-file，那么这个servletClass就是conf/web.xml中定义的org.apache.jasper.servlet.JspServlet了！ 初始化Servlet初始化Servlet这个工作是在StandardWrapper的initServlet方法中完成的，这个方法很简单，就是调用Servlet的init方法，同时把包装了StandarWrapper对象的StandardWrappFacade作为ServletConfig传递给Servlet，原因待会在说~ 如果该Servlet关联的是一个jsp文件，那么在前面初始化的就是JspServlet，接下去会模拟一次简单的请求，请求调用这个jsp文件，以便编译这个jsp文件为class，并初始化这个class。 这样Servlet对象就初始化完成了，事实上Servlet从被 web.xml 中解析到完成初始化，这个过程非常复杂，中间有很多过程，包括各种容器状态的转化引起的监听事件的触发、各种访问权限的控制和一些不可预料的错误发生的判断行为等等。我们这里只抓了一些关键环节进行阐述，试图让大家有个总体脉络。 下面看一下时序图： Servlet体系结构 上图可以看出，Servlet规范就是基于这几个类运转的，与Servlet主动关联的是三个类：ServletConfig，ServletRequest和ServletResponse。这三个类都是通过容器传递给Servlet的，其中ServletConfig是在Servlet初始化时就传递给Servlet的（前面我们提到了）。而后面这两个是在请求到达时调用Servlet时传参过来的。 那么，ServletConfig和ServletContex对Servlet有何作用呢？从ServletConfig的接口中声明的方法不难看出，这些方法都是为了获取这个Servlet的一些配置属性，而这些配置属性可能在Servlet运行时会被用到。 而ServletContext的作用呢？这里提到了Servlet的运行模式，Servlet的运行模式是一个典型的“握手型的交互式”运行模式。所谓“握手型的交互式”就是两个模块为了交换数据通常都会准备一个交易场景，这个场景一直跟随个这个交易过程直到这个交易完成为止。这个交易场景的初始化是根据这次交易对象指定的参数来定制的，这些指定参数通常就会是一个配置类。所以对号入座，交易场景就由ServletContext来描述，而定制的参数集合就由ServletConfig来描述。而ServletRequest和ServletResponse就是要交互的具体对象了，它们通常都是作为运输工具来传递交互结果。 下图是ServletConfig和ServletContex在Tomcat容器中的类关系图： 上图可以看出StandardWrapper和StandardWrapperFacade都实现了ServletConfig接口，而StandardWrapperFacade是 StandardWrapper门面类。所以传给Servlet的是StandardWrapperFacade对象，这个类能够保证从StandardWrapper中拿到 ServletConfig所规定的数据，而又不把ServletConfig不关心的数据暴露给Servlet。 同样ServletContext也与ServletConfig有类似的结构，Servlet中能拿到的ServletContext的实际对象也是 ApplicationContextFacade对象。ApplicationContextFacade同样保证ServletContext只能从容器中拿到它该拿的数据，它们都起到对数据的封装作用，它们使用的都是门面设计模式。 通过ServletContext可以拿到Context容器中一些必要信息，比如应用的工作路径，容器支持的Servlet最小版本等。 Tomcat一接受到请求首先将会创建org.apache.coyote.Request和org.apache.coyote.Response，这两个类是Tomcat内部使用的描述一次请求和响应的信息类，它们是一个轻量级的类，作用就是在服务器接收到请求后，经过简单解析将这个请求快速的分配给后续线程去处理，所以它们的对象很小，很容易被JVM回收。 接下去当交给一个用户线程去处理这个请求时又创建org.apache.catalina.connector.Request和org.apache.catalina.connector.Response对象。这两个对象一直穿越整个Servlet容器直到要传给Servlet，传给Servlet的是Request和Response的门面类RequestFacade和RequestFacade，这里使用门面模式与前面一样都是基于同样的目的——封装容器中的数据。一次请求对应的Request和Response的类转化如下图所示： Servlet如何工作我们已经清楚了Servlet是如何被加载的，Servlet是如何被初始化的，以及Servlet的体系结构，现在的问题是它如何被调用？ 当用户从浏览器向服务器发起一个请求，通常包含如下信息：http://hostname:port/contextpath/servetpath，hostname和port是用来与服务器建立tcp连接的，而后面的URL才是用来选择服务器中哪个子容器服务用户的请求。 根据URL来映射正确的Servlet容器的工作有专门的一个类来完成：org.apache.tomcat.util.http.mapper，这个类保存了Tomcat的Container容器中的所有子容器的信息，当org.apache.catalina.connector.Request类在进入Container容器之前，mapper将会根据这次请求的hostname和contextpath将host和context容器设置到Request的mappingData属性中。所以当Request进入Container容器之前就已经确定了它要访问哪个子容器了！看下图： 上图描述了一次Request请求是如何达到最终的Wrapper容器的，我们现正知道了请求是如何达到正确的Wrapper容器，但是请求到达最终的Servlet还要完成一些步骤，必须要执行Filter链，以及要通知你在web.xml中定义的listener。 接下去就要执行Servlet的service方法了，通常情况下，我们自己定义的servlet并不是直接去实现javax.servlet.servlet接口，而是去继承更简单的HttpServlet类或者GenericServlet类，我们可以有选择的覆盖相应方法去实现我们要完成的工作。 Servlet的确已经能够帮我们完成所有的工作了，但是现在的web应用很少有直接将交互全部页面都用servlet来实现，而是采用更加高效的MVC框架来实现。这些MVC框架基本的原理都是将所有的请求都映射到一个Servlet，然后去实现service方法，这个方法也就是MVC框架的入口。 当Servlet从Servlet容器中移除时，也就表明该Servlet的生命周期结束了，这时Servlet的destroy方法将被调用，做一些扫尾工作。 Session与Cookie 从上图可以看到Session工作的时序图，有了Session ID服务器端就可以创建HttpSession对象了，第一次触发是通过request.getSession()方法，如果当前的Session ID还没有对应的HttpSession对象那么就创建一个新的，并将这个对象加到 org.apache.catalina.Manager的sessions容器中保存，Manager类将管理所有Session的生命周期，Session过期将被回收，服务器关闭，Session将被序列化到磁盘等。只要这个HttpSession对象存在，用户就可以根据Session ID来获取到这个对象，也就达到了状态的保持。 引用原文","tags":[{"name":"是什么系列","slug":"是什么系列","permalink":"https://blog.kazaff.me/tags/是什么系列/"},{"name":"Servlet","slug":"Servlet","permalink":"https://blog.kazaff.me/tags/Servlet/"},{"name":"Tomcat","slug":"Tomcat","permalink":"https://blog.kazaff.me/tags/Tomcat/"}]},{"title":"Nodejs操作redis","date":"2014-06-06T14:53:12.000Z","path":"2014/06/06/nodejs操作redis/","text":"前前后后已经写过好多个语言与redis进行结合了，有php，c++，java，今天总算要用nodejs来操作redis了，不知为何还有点儿小兴奋~~ 虽然限于nodejs的异步编程模型的影响，直观上和其它语言非常不同，但基本上花一些时间都可以很快的适应，操作redis的方法依然如此，与操作mysql的api没什么两样~ 我还是准备着重介绍关于redis的认证，事务和批处理相关的内容，毕竟redis的数据结构本身非常的直观，并没有什么好多讲的~哦，忘记说了，我使用的库是：redis，从官网上可以了解很全面信息，这里面有个小插曲，我的桌面OS是win7 64bit，但是死活安装不好vs环境，导致无法成功编译hiredis，所以只能使用redis提供的javascript接口，这在性能上会有所降低，不过目前这并不是我的关注点。 javascript是基于事件驱动的，nodejs理所当然的也拥有事件驱动引擎，所以先看一下这个redis库提供了哪些重要的事件： ready与redis服务创建连接成功后，会触发“ready”事件，这表明服务器端已经准备好接受命令了，但这并不表明必须在该事件发生后才能使用client执行命令，那是因为在“ready”事件触发之前，用client执行的命令会存放在队列中，等到就绪后会根据顺序依次调用命令。 end当连接关闭后，会触发“end”事件。 drain当连接缓冲区可写后，会触发“drain”事件，这有助于你控制发送频率。 下面来介绍一些重要的接口： redis.createClient(port, host, options)默认port是6379，默认的host当然是127.0.0.1，options为一个对象，可以包含下列的属性： parser： redis协议的解析器，默认是hiredis，如果像我一样悲剧的装不上，那该项会被设置为javascript； no_ready_check： 默认为false，当连接建立后，服务器端可能还处在从磁盘加载数据的loading阶段，这个时候服务器端是不会响应任何命令的，这个时候node_redis会发送INFO命令来检查服务器状态，一旦INFO命令收到响应了，则表明服务器端已经可以提供服务了，此时node_redis会触发“ready”事件，这就是为什么会有“connect”和“ready”事件之分，如果你关闭了这个功能，我觉得会出现一些貌似灵异的问题； enable_offline_queue： 默认为true，前面提到过，当连接ready之前，client会把收到的命令放入队列中等待执行，如果关闭该项，所有ready前的调用都将会立刻得到一个error callback； retry_max_delay： 默认为null，默认情况下client连接失败后会重试，每次重试的时间间隔会翻倍，直到永远，而设置这个值会增加一个阀值，单位为毫秒； connect_timeout: 默认为false，默认情况下客户端将会一直尝试连接，设置该参数可以限制尝试连接的总时间，单位为毫秒； max_attempts: 默认为null，可以设置该参数来限制尝试的总次数； auth_pass： 默认为null，该参数用来认证身份。 client.auth(password, callback)这个接口用来认证身份的，如果你要连接的redis服务器是需要认证身份的，那么你必须确保这个方法是创建连接后第一个被调用的。需要注意的是，为了让使重连足够的简单，client.auth()保存了密码，用于每次重连后的认证，而回调只有会执行一次哦~~ 另外你需要确保的是，千万别自作聪明的把client.auth()放在“ready”或“connect”事件的回调中，你将会得到一行神秘的报错： Error: Ready check failed: ERR operation not permitted 只需要在redis.createClient()代码后直接调用clietn.auth()即可，具体原因我没有深究~ client.end()这个方法会强行关闭连接，并不会等待所有的响应。如果不想如此暴力，推荐使用client.quit()，该方法会在收到所有响应后发送QUIT命令。 client.unref()这个方法作用于底层socket连接，可以在程序没有其他任务后自动退出，可以类比nodejs自己的unref()。这个方法目前是个试验特性，只支持一部分redis协议。 client.multi([commands])multi命令可以理解为打包，它会把命令都存放在队列里，直到调用exec方法，redis服务器端会一次性原子性的执行所有发来的命令，node_redis接口最终会返回一个Multi对象。 Multi.exec(callback)client.multi()会返回一个Multi对象，它包含所有的命令，直到调用Multi.exec()。我们来主要说一下这个方法的回调函数中的两个参数： err: null或者Array，没有出错当然就会返回null，如果出错则返回命令队列链中对应的发生错误的命令的错误信息，数组中最后一个元素exec()方法自身的错误信息，这里我要说的是，可以看出这种方式所谓的原子性主要是指的命令链中的命令一定会保证一起在服务器端执行，而不是指的像关系型数据库那样的回滚功能； results： null或者Array，返回命令链中每个命令的返回信息。 var redis = require(&quot;./index&quot;), client = redis.createClient(), set_size = 20; client.sadd(&quot;bigset&quot;, &quot;a member&quot;); client.sadd(&quot;bigset&quot;, &quot;another member&quot;); while (set_size &gt; 0) { client.sadd(&quot;bigset&quot;, &quot;member &quot; + set_size); set_size -= 1; } // multi chain with an individual callback client.multi() .scard(&quot;bigset&quot;) .smembers(&quot;bigset&quot;) .keys(&quot;*&quot;, function (err, replies) { // NOTE: code in this callback is NOT atomic // this only happens after the the .exec call finishes. client.mget(replies, redis.print); }) .dbsize() .exec(function (err, replies) { console.log(&quot;MULTI got &quot; + replies.length + &quot; replies&quot;); replies.forEach(function (reply, index) { console.log(&quot;Reply &quot; + index + &quot;: &quot; + reply.toString()); }); });","tags":[{"name":"redis","slug":"redis","permalink":"https://blog.kazaff.me/tags/redis/"}]},{"title":"用Nodejs实现一个小小爬虫","date":"2014-06-05T11:37:12.000Z","path":"2014/06/05/用nodejs实现一个小小爬虫/","text":"呵呵，不要被题目给忽悠了啊，我并不具备开发一个足够智能的网络爬虫程序的能力，那我到底要做什么呢？ 其实是这样的，目前公司开发了一个门户类型的网站客户端，但是由于数据流比较复杂，性能非常不理想。要解决这个问题其实也不难，那就是加缓存层（计算机领域提速的万能方案就是缓存，不论是硬件还是软件）。当然，你可能还会提到：全站静态化（其实也是缓存的理念）。后端开发增加缓存层，其实成本不高，很容易就可以把memcache或redis等内存性KV系统引入到当前项目中。一般情况下会设计为：当用户第一次访问某个页面时，把相关数据存储到缓存中，以加速下一次请求的响应速度。 这里需要注意的是第一次访问，你大概知道我想说什么了吧？第一次访问的用户就不是亲娘养的了？，谁来保证他们的体验？ 我们姑且把这种方案叫做：被动缓存，那么如何使缓存更加的主动呢？最好是数据一旦发生CURD，就立刻映射到缓存层。这里面涉及到缓存颗粒度，缓存的键规则等细节问题，而且对于目前我们公司的项目而言，难度会更大，因为数据的CURD是发生在其他系统中，而这个门户网站客户端只有在访问时才需要与其他系统通信。 那么如何快速的实现主动缓存呢？再回过头来看看文章标题，你应该猜到最终的解决方案了，对，用机器模拟请求，充当这“第一个用户”，姑且称为伪主动缓存吧~~ 好吧，说了这么多，总算到今天的主题了：写一个爬虫程序。我觉得还是有必要再多说一下场景，这里面涉及到具体需求，有助于思考和权衡！ 我们的这个爬虫需要从给定的入口页面开始出发，扫描该页面下所有的链接，并继续进行请求，就这样不停的爬下去。当然，如果检测到的链接和当前入口页面的域不同，则丢弃，毕竟我们是为了创建本站缓存而工作的。另外，我们也不需要爬完全部的链接，按照业界的统计，只需要为三级以内的页面创建缓存即可，什么叫三级以内？简单的说，就是从主页出发，三次跳转以内能达到的页面都算三级以内！这样既能最大程度保证用户体验，又可以尽可能的降低内存占用率，而且我们的爬虫程序也会更快的完成任务！何乐而不为？ 最后还要说的是，我们采用广度优先规则进行爬行（而不是搜索引擎用的深度优先），原因很简单，可以简化编程模型，并且个人认为更符合伪主动缓存的需求。 开发语言我选择使用nodejs，当然从文章题目上，你早已知道答案了！nodejs的编程模型可以有效降低该项目的开发难度（避免多线程模型和锁冲突），又最大程度的确保并发性能（毕竟这个项目是网络IO阻塞型），另外，它有很丰满的工具库供我们使用，这样会让开发成本降至最低。 依赖库 request： 用于发起http请求，简化了操作api htmlparser2: 用于解析取得的html内容，方便抽取url async： 用于管理代码执行流程 redis： 用于清空redis中的旧缓存（并不是爬虫程序所必须的） 示意图 看，很简单吧，加上nodejs的强大社区，该程序应该三两天就能做完~我会把相关代码放到github上，有兴趣的朋友可去看看。","tags":[{"name":"缓存","slug":"缓存","permalink":"https://blog.kazaff.me/tags/缓存/"},{"name":"爬虫","slug":"爬虫","permalink":"https://blog.kazaff.me/tags/爬虫/"}]},{"title":"RabbitMQ的认证","date":"2014-05-15T16:42:58.000Z","path":"2014/05/15/rabbitmq的认证/","text":"一旦要把模块上线，那么安全就成了一个关注点。这在互联网领域更是焦点话题！ 我在本地开启RabbitMQ Server后，用 localhost 去连接其默认vhost（“/”）,代码一切正常，无需提供用户名密码。 但是一旦把localhost换成真实ip，RabbitMQ就会立刻提示你无权限操作（或者你尝试用localhost去连接非默认vhost也会报错）！那么我们只需要为RabbitMQ创建用户即可： rabbitmqctl add_user kazaff 123456 如果需要创建新的vhos： rabbitmqctl add_vhost foo 然后还要把新创建的账户绑定到指定的vhost上： rabbitmqctl set_permissions -p foo kazaff &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 后面3个”.*”分别代表kazaff用户拥有对foo虚拟机的配置，写，读等权限。举个例子： “.*”：表示匹配任何Exchange和queue； “ “：表示不匹配任何Exchange和queue； “kazaff-*”：表示匹配任何以kazaff-开头的Exchange和queue。 其他相关的命令如下： rabbitmqctl list_permissions -p vhost //查看vhost上用户权限列表 rabbitmqctl list_user_permissions user //查看user用户在所有vhost上的配置权限列表 rabbitmqctl clear_permissions -p vhost user //删除vhost上user的权限","tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"https://blog.kazaff.me/tags/rabbitmq/"}]},{"title":"RabbitMQ的一些概念","date":"2014-05-15T11:35:39.000Z","path":"2014/05/15/rabbitmq的一些概念/","text":"早在一年多以前，我就已经开始试图在项目中异步化一些业务，例如系统的行为日志。当时选择的就是大名鼎鼎的RabbitMQ，这也是调查过不少同类产品后最终的选择，直到今天也无怨无悔~最喜欢的一点并不是它的业务模型丰富，而是它支持的语言很全面，从php到java，c/c++，甚至nodejs，都可以很方便的使用（虽然c/c++下的库文档真的很少~）！ 虽然我一直记着它的好，但悲剧的是早先调研它时学习的很多概念，时至今日已经忘不少了~所以感觉还是要写一篇博文记录下来，以备后用！ 那就一个一个来吧： ##Message acknowledgment当队列中的任务被你的消费者进程取走后，如果消费者处理中挂掉了，那这个任务也就丢失了（虽然可能只做了一半）！很多情况下这并不是我们可以接受的，所以 Ack 机制出现了，它给了我们一个很优的解决方案： 当消费者连接断开后，如果RabbitMQ没有收到消费者针对该任务的Ack，那么RabbitMQ就会认为该消费者挂掉了，同时会把该任务分给其他消费者。 这里还要注意的是：任务是没有超时限制的，也就是说只要消费者的连接有效，RabbitMQ就不会把任务再发送给其他消费者，这样可以保证某些需要耗时很久的任务正常执行。 尤其注意的是，千万不要忘记发送Ack，否则RabbitMQ会不停的把任务重复发送并且一直积累，直到崩溃~可以通过下面这个命令来查看当前没有收到Ack的消息个数： $ sudo rabbitmqctl list_queues name messages_ready messages_unacknowledged Listing queues ... hello 0 0 ...done. ##Message durability一般情况下当RabbitMQ退出或崩溃，那么队列和任务将会丢失，这当然是不能容忍的。RabbitMQ提供了持久化方案，只需要把队列声明成持久的即可（注意，RabbitMQ并不允许修改当前已存在队列的持久性）。 此外我们还需要把消息也设置成持久化的，这些都有对应的属性参数让我们来设置。 但注意，RabbitMQ并不能百分之百保证消息一定不会丢失，因为为了提升性能，RabbitMQ会把消息暂存在内存缓存中，直到达到阀值才会批量持久化到磁盘，也就是说如果在持久化到磁盘之前RabbitMQ崩溃了，那么就会丢失一小部分数据，这对于大多数场景来说并不是不可接受的，如果确实需要保证任务绝对不丢失，那么应该使用事务机制。 ##Round-robin dispatching这个机制是RabbitMQ最常用业务模型中的。一般我们选择异步任务，除了降低模块间的依赖外，还有一个理由就是有效规避大并发负载，尤其是针对http。 举个场景，网站上的找回密码功能，系统会向对应用户的邮箱发送修改密码的连接。如果是同步流程的话，大量用户同时请求该功能，由于发送邮件比较耗时，那么你的web服务器会持续等待，这个时候就可能会被大量涌入的请求搞死，即便是没死，也会大大影响其响应速度。 那么如果使用异步的话，我们可以把找回密码的请求都存到队列里，然后由后台进程逐步完成邮件发送的任务，web服务器就可以快速响应用户。 好，说了这么多，那么到底 Round-robin 是做什么的？看图： 上图中我们有两个消费者（C1，C2），它们同时从队列中领取任务并执行，默认情况下RabbitMQ会按照顺序依次把消息发送给C1和C2，这样可以保证每个消费者领到的任务个数都是相同的，这种分配任务的方式就是Round-robin。 任务耗时不均匀的情况下，这种方式可能并不是最佳的。 ##Fair dispatch上面说到了，由于默认情况下RabbitMQ不会去管任务到底是什么类型的（特指其耗时情况），它只会一味的按照 Round-robin 的算法把队列中的消息平均分配给所有消费者。还是上面的那个图，我们假设队列中的任务很奇葩，奇数任务是耗时久的，偶数任务是耗时低的，那么C1可能一直很忙，而C2则几乎没事儿可做！ 听上去很不公平是吧？这就是因为 Round-robin 机制并不考虑每个消费者当前正在处理的任务数（换句话说，就是当前该消费者仍没有Ack的任务数）。 我们可以设置 prefetch 来避免上述情况，该设置可以告诉RabbitMQ：直到该消费者处理完当前指定数目的任务之前，不要再给消费者分配新任务（这是依赖统计该消费者的Ack情况来实现的，可见两者必须欧同时开启哦）。 如果当前所有消费者都在忙，那么任务将会阻塞在队列中，你可能需要增加消费者数量来避免大量任务被阻塞。 ##Exchanges上图中的那种架构并不是RabbitMQ推荐的，为什么这么说呢？RabbitMQ核心思想是生产者绝对不应该直接将任务投递到目标队列中，换句话说，生产者根本不需要知道任务最终应该会投递到哪里。 取而代之的，生产者只需要把任务发送到一个 Exchange 中即可，如下图： Exchange 非常容易理解，它负责根据映射关系和投递模型把任务投递到队列中，有效的投递模型有：direct，topic，headers，fanout。官方提供的例子中就已经把这些模型讲的很清楚了。 剩下要做的就是把Exchange和Queue绑定到一起了，如果向一个没有绑定任何队列的Exchange发送任务，则任务都会被丢弃。 你可以通过下面的命令来查看绑定关系： $ sudo rabbitmqctl list_bindings Listing bindings ... logs exchange amq.gen-JzTY20BRgKO-HjmUJj0wLg queue [] logs exchange amq.gen-vso0PVvyiRIL2WoV3i48Yg queue [] ...done. ##Routing其实这个机制是建立在 Exchanges 上的，有了Route，我们就可以实现根据类别，让Exchange来选择性的分发任务给匹配的队列。 要做到这点，只需要在为Exchange绑定queue时设置一个 routingKey 即可。注意，fanout类型的exchanges会忽略这个值，毕竟这种类型的exchange要实现的是广播机制。 如上图，我们这次使用的是 direct 投递模型的Exchage，这种模型下的路由逻辑非常简单：根据绑定时声明的routingKey来分发任务。 另外值得一提的是，绑定非常灵活，不仅可以像上图那样为一个队列绑定多个不同的routingKey，也可以为Exchage绑定多个队列同时监听相同的routingKey（这等同于fanout模型）。 ##Topic exchange我承认，可能排版上有点乱，因为按道理说这个概念应该合并到 Exchanges 中，但是由于它依赖 Routing ，所以我决定采用官方提供的学习步骤。 我们已经了解过direct，fanout两种投递模型。那么topic到底又是什么呢？ 简单来说，topic只是为routingKey设置了一个规则（任意单词来描述主题，以”.”分割为不同层级，长度不能超过255位），有点命名空间的味道，这里称之为主题可能更加合适一些。 在规则中还提供了两个关键字： ***：可以匹配任意1个单词； #：可以匹配任意0个或多个单词。 有点正则的味道，不过确实在direct模型的基础上进一步提升了灵活性。举个例子，如果我们用这么一个routingKey：*.love.*，那么投递任务时，任何这种模式主题的任务都会投递到对应队列，例如：everyone.love.kazaff。再如：kazaff.#，这意味着会匹配kazaff.me.is.cool，也会匹配kazaff.me，等等。 如果发送任务用的routingKey不能匹配声明的模式，那么任务就会被丢弃。 你可以把模式只定义为#来模仿fanout模型，也可以把模式定义为不包含*和#的具体字符串来模仿direct模型。 ##Message properties在AMQP协议中为每个任务预定义了14个属性，大多数属性都非常少用，常用的4个如下： deliveryMode：标识任务的持久性； contentType：用于描述任务数据的MIME-TYPE，例如application/json； replyTo：用于命名一个callback队列； correlationId：用于标识RPC任务的请求与响应的配对编号。 可想到为每一次RPC请求都创建一个回调队列是非常低效的。更高效的做法是为每一个发出RPC请求的客户端创建一个回调队列，但这又产生了一个新问题：如何知道回调队列中的响应是对应哪一个请求的呢？ 这就是 correlationId 属性存在的意义，我们只要为每个请求设置一个唯一的值，我们就能在从回调队列中取到响应数据后根据correlationId找到对应的请求。 如果我们收到一个未知的correlationId响应，只需要忽略它既可。你可能会想，怎么可能收到未知的响应？确实，这种情况发生的概率不高，如下图： 假设S从rpc_queue中取得RPC任务后，进行处理，然后把响应发送给reply_to队列，此时S挂了，它还没来得及向rpc_queue发送ack！但是你知道的，其实整个RPC已经可以算完成了！ 这个时候S重启完毕，它会再次取到刚才的那个RPC任务，再次处理，再次把结果发送给reply_to队列，这次它挺了下来没死机，并把ack发送到rpc_queue。但是，C端早已经在S第一次死机之前就拿到结果了，第二次发来的响应任务C自然找不到对应的correlationId。我这么说，你懂了么？ 当目前为止，我已经把我认为重要的概念都提到了，有啥问题，私下讨论吧！","tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"https://blog.kazaff.me/tags/rabbitmq/"}]},{"title":"Hello World","date":"2014-05-08T10:54:30.000Z","path":"2014/05/08/hello-world/","text":"其实一直想把博客好好整理一下，不过总是拖延~ 今天偶然间看到另一个基于nodejs的静态博客系统 hexo ，感觉很喜欢~趁着热情就部署了一下~ 现在就是头疼老文章迁移问题，还有就是博客皮肤！一点一点来吧！","tags":[{"name":"hello","slug":"hello","permalink":"https://blog.kazaff.me/tags/hello/"}]}]